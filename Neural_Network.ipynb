{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca22a4e",
   "metadata": {},
   "source": [
    "# Final Project Decision Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e116b4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the code Backpropagation 1 there is an example of a [4,5,2] network used to predict\n",
    "the outcome of the variable `survived` from the features `pclass`,\n",
    "`sex`,`age`,`sibsp` in the dataset \"Titanic Dataset.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "429d03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e63cfcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
       "      <td>male</td>\n",
       "      <td>30.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived                                             name     sex  \\\n",
       "0       1         1                    Allen, Miss. Elisabeth Walton  female   \n",
       "1       1         1                   Allison, Master. Hudson Trevor    male   \n",
       "2       1         0                     Allison, Miss. Helen Loraine  female   \n",
       "3       1         0             Allison, Mr. Hudson Joshua Creighton    male   \n",
       "4       1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
       "\n",
       "     age  sibsp  parch  ticket      fare    cabin embarked boat   body  \\\n",
       "0  29.00      0      0   24160  211.3375       B5        S    2    NaN   \n",
       "1   0.92      1      2  113781  151.5500  C22 C26        S   11    NaN   \n",
       "2   2.00      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "3  30.00      1      2  113781  151.5500  C22 C26        S  NaN  135.0   \n",
       "4  25.00      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "\n",
       "                         home.dest  \n",
       "0                     St Louis, MO  \n",
       "1  Montreal, PQ / Chesterville, ON  \n",
       "2  Montreal, PQ / Chesterville, ON  \n",
       "3  Montreal, PQ / Chesterville, ON  \n",
       "4  Montreal, PQ / Chesterville, ON  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv('Titanic Dataset.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54be7e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1309, 14)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ec794f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pclass         int64\n",
       "survived       int64\n",
       "name          object\n",
       "sex           object\n",
       "age          float64\n",
       "sibsp          int64\n",
       "parch          int64\n",
       "ticket        object\n",
       "fare         float64\n",
       "cabin         object\n",
       "embarked      object\n",
       "boat          object\n",
       "body         float64\n",
       "home.dest     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efaf093e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pclass          0\n",
       "survived        0\n",
       "name            0\n",
       "sex             0\n",
       "age           263\n",
       "sibsp           0\n",
       "parch           0\n",
       "ticket          0\n",
       "fare            1\n",
       "cabin        1014\n",
       "embarked        2\n",
       "boat          823\n",
       "body         1188\n",
       "home.dest     564\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38485ed1",
   "metadata": {},
   "source": [
    "There are some missing values, so let's drop those observations. But, before, let's focus on the variables we want: `pclass`, `sex`,`age`,`sibsp`, and the target `survived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce595a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>30.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>25.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass     sex    age  sibsp  survived\n",
       "0       1  female  29.00      0         1\n",
       "1       1    male   0.92      1         1\n",
       "2       1  female   2.00      1         0\n",
       "3       1    male  30.00      1         0\n",
       "4       1  female  25.00      1         0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_v = titanic[['pclass', 'sex', 'age', 'sibsp', 'survived']]\n",
    "titanic_v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4c260c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pclass        0\n",
       "sex           0\n",
       "age         263\n",
       "sibsp         0\n",
       "survived      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_v.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cda76ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1046 entries, 0 to 1308\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   pclass    1046 non-null   int64  \n",
      " 1   sex       1046 non-null   object \n",
      " 2   age       1046 non-null   float64\n",
      " 3   sibsp     1046 non-null   int64  \n",
      " 4   survived  1046 non-null   int64  \n",
      "dtypes: float64(1), int64(3), object(1)\n",
      "memory usage: 49.0+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic_v = titanic_v.dropna()\n",
    "titanic_v.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970df651",
   "metadata": {},
   "source": [
    "Missing values have been correctly removed. (1309 original - 263 missing = 1046 left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5bf4a",
   "metadata": {},
   "source": [
    "Now, we have to encode `sex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "444311b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UX325\\AppData\\Local\\Temp\\ipykernel_10660\\608239367.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dummies = pd.get_dummies(titanic_v['sex']).applymap(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   female  male\n",
       "0       1     0\n",
       "1       0     1\n",
       "2       1     0\n",
       "3       0     1\n",
       "4       1     0\n",
       "5       0     1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(titanic_v['sex']).applymap(int) \n",
    "dummies.head(6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68e53f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>survived</th>\n",
       "      <th>female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass     sex    age  sibsp  survived  female\n",
       "0       1  female  29.00      0         1       1\n",
       "1       1    male   0.92      1         1       0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_v = pd.concat([titanic_v, dummies.female], axis=1)\n",
    "titanic_v.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b8e8c",
   "metadata": {},
   "source": [
    "Now, let's define the features and target of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6d7d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = ['pclass', 'female','age','sibsp']\n",
    "target = 'survived'\n",
    "Y = titanic_v[target].to_numpy(dtype = int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85ec9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic_v[features].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "424e687a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  1.  , 29.  ,  0.  ],\n",
       "       [ 1.  ,  0.  ,  0.92,  1.  ],\n",
       "       [ 1.  ,  1.  ,  2.  ,  1.  ],\n",
       "       ...,\n",
       "       [ 3.  ,  0.  , 26.5 ,  0.  ],\n",
       "       [ 3.  ,  0.  , 27.  ,  0.  ],\n",
       "       [ 3.  ,  0.  , 29.  ,  0.  ]], shape=(1046, 4))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e10616f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3266270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 2-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert survival 0/1 into a \n",
    "    corresponding desired output from the neural\n",
    "    network. 0 -> [1,0] and 1 -> [0,1]\"\"\"\n",
    "    e = np.zeros((2, 1))\n",
    "    e[int(j)] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5d00f",
   "metadata": {},
   "source": [
    "Legend:\n",
    "- Not Survived: `[1,0]`\n",
    "- Survived: `[0,1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81757bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = [vectorized_result(y) for y in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16b88163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.],\n",
       "        [1.]]),\n",
       " array([[0.],\n",
       "        [1.]]),\n",
       " array([[1.],\n",
       "        [0.]])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_results[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31e36f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], shape=(1046, 2))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert list of (2, 1) arrays into a single (n_samples, 2) array\n",
    "training_results_array = np.hstack(training_results).T\n",
    "training_results_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d550a4dc",
   "metadata": {},
   "source": [
    "Normalize the features to improve learning efficiency and stability during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93738bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(X):\n",
    "    k = X.shape[0]\n",
    "    X_mean = np.mean(X, axis = 0)\n",
    "    X_std = np.std(X, axis =0, ddof=1) \n",
    "    X = (X-X_mean)/X_std \n",
    "    return X, X_mean, X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2243257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.43489222,  1.30163551, -0.06113283, -0.5512893 ],\n",
       "       [-1.43489222, -0.76752975, -2.00930734,  0.54500083],\n",
       "       [-1.43489222,  1.30163551, -1.93437755,  0.54500083]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X_mean, X_std =  normalise(X)\n",
    "X[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba7fcf",
   "metadata": {},
   "source": [
    "All under the same scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb63463a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.43489222],\n",
       "       [ 1.30163551],\n",
       "       [-0.06113283],\n",
       "       [-0.5512893 ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_inputs = [np.reshape(x, (4, 1)) for x in X]\n",
    "training_inputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f582e7",
   "metadata": {},
   "source": [
    "Now, let's code the neural network architechture: Dense layer class, ReLu Activation, Softmax Activation and Categorical Cross-Entropy loss.\n",
    "\n",
    "**Why Softmax activation + Categoriccal Cross-Entropy loss for the outputs, instead of Sigmoid + Binary Cross-Entropy?**\n",
    "\n",
    "- Softmax ensures the outputs are mutually exclusive probabilities that sum to 1, which is perfect for one-hot encoded targets like [1, 0] or [0, 1], that we are using for the `survived` class.\n",
    "\n",
    "- Categorical Cross-Entropy directly compares the full probability distribution to the one-hot label, penalizing confidence in the wrong class.\n",
    "\n",
    "- Sigmoid treats each output independently, so it doesn't assume only one class is correct.\n",
    "\n",
    "- Sigmoid is fine for multi-label problems (multiple classes can be \"on\"/correct), but not ideal when only one class is correct, like in this case.\n",
    "\n",
    "Wrapping up, for our current setup with 2 output nodes, we should use softmax. If we switched the setup to 1 output node, we might have used sigmoid and adjusted the labels and loss accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "698005ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd2df032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c70f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    " def forward(self, inputs):\n",
    "  exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "  probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "  self.output = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc1b1a",
   "metadata": {},
   "source": [
    "Softmax doesn't have the backward pass as it is very complex to get the partial derivative of softmax's outputs with respect to its inputs. \n",
    "\n",
    "Instead, we will use a combined method in the `Activation_Softmax_Loss_CategoricalCrossentropy` class, where we will directly get the result of the partial derivative of loss with respect to softmax's inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "faa184e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    " def calculate(self, output, y):\n",
    "  sample_losses = self.forward(output, y)\n",
    "  data_loss = np.mean(sample_losses)\n",
    "  return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "deb3a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d834e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Create activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        self.dinputs = dvalues.copy() # Copy so we can safely modify\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52a33439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD:\n",
    " def __init__(self, learning_rate=0.05):\n",
    "  self.learning_rate = learning_rate\n",
    " # Update parameters - follow the formula (new_weights = current_weights - learning_rate * gradient)\n",
    " def update_params(self, layer):\n",
    "  layer.weights += -self.learning_rate * layer.dweights\n",
    "  layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c870edfe",
   "metadata": {},
   "source": [
    "Now that we have created all the classes, let's run the neural network with forward and backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6679ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.358, loss: 0.693\n",
      "epoch: 100, acc: 0.592, loss: 0.676\n",
      "epoch: 200, acc: 0.592, loss: 0.671\n",
      "epoch: 300, acc: 0.784, loss: 0.602\n",
      "epoch: 400, acc: 0.783, loss: 0.481\n",
      "epoch: 500, acc: 0.789, loss: 0.455\n",
      "epoch: 600, acc: 0.792, loss: 0.449\n",
      "epoch: 700, acc: 0.795, loss: 0.447\n",
      "epoch: 800, acc: 0.795, loss: 0.445\n",
      "epoch: 900, acc: 0.795, loss: 0.444\n",
      "epoch: 1000, acc: 0.795, loss: 0.444\n",
      "epoch: 1100, acc: 0.796, loss: 0.443\n",
      "epoch: 1200, acc: 0.796, loss: 0.443\n",
      "epoch: 1300, acc: 0.796, loss: 0.442\n",
      "epoch: 1400, acc: 0.794, loss: 0.442\n",
      "epoch: 1500, acc: 0.794, loss: 0.442\n",
      "epoch: 1600, acc: 0.794, loss: 0.441\n",
      "epoch: 1700, acc: 0.795, loss: 0.441\n",
      "epoch: 1800, acc: 0.793, loss: 0.441\n",
      "epoch: 1900, acc: 0.793, loss: 0.440\n",
      "epoch: 2000, acc: 0.793, loss: 0.440\n",
      "epoch: 2100, acc: 0.793, loss: 0.439\n",
      "epoch: 2200, acc: 0.798, loss: 0.439\n",
      "epoch: 2300, acc: 0.799, loss: 0.439\n",
      "epoch: 2400, acc: 0.799, loss: 0.438\n",
      "epoch: 2500, acc: 0.800, loss: 0.438\n",
      "epoch: 2600, acc: 0.799, loss: 0.437\n",
      "epoch: 2700, acc: 0.800, loss: 0.437\n",
      "epoch: 2800, acc: 0.801, loss: 0.437\n",
      "epoch: 2900, acc: 0.800, loss: 0.436\n",
      "epoch: 3000, acc: 0.802, loss: 0.436\n",
      "epoch: 3100, acc: 0.802, loss: 0.435\n",
      "epoch: 3200, acc: 0.802, loss: 0.435\n",
      "epoch: 3300, acc: 0.802, loss: 0.435\n",
      "epoch: 3400, acc: 0.801, loss: 0.434\n",
      "epoch: 3500, acc: 0.801, loss: 0.434\n",
      "epoch: 3600, acc: 0.800, loss: 0.434\n",
      "epoch: 3700, acc: 0.800, loss: 0.433\n",
      "epoch: 3800, acc: 0.800, loss: 0.433\n",
      "epoch: 3900, acc: 0.802, loss: 0.433\n",
      "epoch: 4000, acc: 0.803, loss: 0.433\n",
      "epoch: 4100, acc: 0.803, loss: 0.433\n",
      "epoch: 4200, acc: 0.803, loss: 0.433\n",
      "epoch: 4300, acc: 0.803, loss: 0.433\n",
      "epoch: 4400, acc: 0.805, loss: 0.432\n",
      "epoch: 4500, acc: 0.806, loss: 0.432\n",
      "epoch: 4600, acc: 0.807, loss: 0.432\n",
      "epoch: 4700, acc: 0.807, loss: 0.432\n",
      "epoch: 4800, acc: 0.809, loss: 0.432\n",
      "epoch: 4900, acc: 0.809, loss: 0.432\n",
      "epoch: 5000, acc: 0.809, loss: 0.432\n",
      "epoch: 5100, acc: 0.809, loss: 0.432\n",
      "epoch: 5200, acc: 0.809, loss: 0.432\n",
      "epoch: 5300, acc: 0.809, loss: 0.431\n",
      "epoch: 5400, acc: 0.809, loss: 0.431\n",
      "epoch: 5500, acc: 0.809, loss: 0.431\n",
      "epoch: 5600, acc: 0.809, loss: 0.431\n",
      "epoch: 5700, acc: 0.809, loss: 0.431\n",
      "epoch: 5800, acc: 0.809, loss: 0.431\n",
      "epoch: 5900, acc: 0.809, loss: 0.431\n",
      "epoch: 6000, acc: 0.809, loss: 0.431\n",
      "epoch: 6100, acc: 0.809, loss: 0.431\n",
      "epoch: 6200, acc: 0.809, loss: 0.431\n",
      "epoch: 6300, acc: 0.808, loss: 0.431\n",
      "epoch: 6400, acc: 0.808, loss: 0.431\n",
      "epoch: 6500, acc: 0.808, loss: 0.431\n",
      "epoch: 6600, acc: 0.808, loss: 0.431\n",
      "epoch: 6700, acc: 0.809, loss: 0.431\n",
      "epoch: 6800, acc: 0.809, loss: 0.431\n",
      "epoch: 6900, acc: 0.809, loss: 0.431\n",
      "epoch: 7000, acc: 0.809, loss: 0.431\n",
      "epoch: 7100, acc: 0.809, loss: 0.431\n",
      "epoch: 7200, acc: 0.809, loss: 0.431\n",
      "epoch: 7300, acc: 0.809, loss: 0.431\n",
      "epoch: 7400, acc: 0.809, loss: 0.431\n",
      "epoch: 7500, acc: 0.809, loss: 0.431\n",
      "epoch: 7600, acc: 0.809, loss: 0.431\n",
      "epoch: 7700, acc: 0.809, loss: 0.431\n",
      "epoch: 7800, acc: 0.809, loss: 0.431\n",
      "epoch: 7900, acc: 0.809, loss: 0.431\n",
      "epoch: 8000, acc: 0.809, loss: 0.431\n",
      "epoch: 8100, acc: 0.809, loss: 0.431\n",
      "epoch: 8200, acc: 0.809, loss: 0.431\n",
      "epoch: 8300, acc: 0.809, loss: 0.431\n",
      "epoch: 8400, acc: 0.809, loss: 0.430\n",
      "epoch: 8500, acc: 0.809, loss: 0.430\n",
      "epoch: 8600, acc: 0.809, loss: 0.430\n",
      "epoch: 8700, acc: 0.809, loss: 0.430\n",
      "epoch: 8800, acc: 0.809, loss: 0.430\n",
      "epoch: 8900, acc: 0.809, loss: 0.430\n",
      "epoch: 9000, acc: 0.809, loss: 0.430\n",
      "epoch: 9100, acc: 0.809, loss: 0.430\n",
      "epoch: 9200, acc: 0.809, loss: 0.430\n",
      "epoch: 9300, acc: 0.809, loss: 0.430\n",
      "epoch: 9400, acc: 0.809, loss: 0.430\n",
      "epoch: 9500, acc: 0.809, loss: 0.430\n",
      "epoch: 9600, acc: 0.809, loss: 0.430\n",
      "epoch: 9700, acc: 0.809, loss: 0.430\n",
      "epoch: 9800, acc: 0.809, loss: 0.430\n",
      "epoch: 9900, acc: 0.809, loss: 0.430\n",
      "epoch: 10000, acc: 0.809, loss: 0.430\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) # for reproducibility\n",
    "dense1 = Layer_Dense(4, 5)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(5, 2)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_GD()\n",
    "\n",
    "\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, training_results_array)\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(training_results_array.shape) == 2:\n",
    "        y_train = np.argmax(training_results_array, axis=1)\n",
    "    else:\n",
    "        y_train = training_results_array.copy()\n",
    "    accuracy = np.mean(predictions == y_train)\n",
    "    \n",
    "    if not epoch % 100: # Print every 100 epochs (epoch = one complete pass through the entire training dataset)\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}')\n",
    "    \n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y_train)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    \n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa5d6ae",
   "metadata": {},
   "source": [
    "We are looking for parameters (weights and biases) that minimize loss. However, for evaluating the model's performance, we should consider the trade-off between accuracy and loss (best accuracy for lowest loss possible).\n",
    "\n",
    "Therefore, we can appreciate that at epoch (which means, one complete pass through the entire training dataset by the learning algorithm) 6000, the loss begins to decay very very slowly, with few improvement. A bit after, at epoch 9000 we reach the peak in accuracy (0.815) with a loss of 0.450, which is almost identical to the loss of 0.444 at epoch 10000. For that reason, the parameters that we will keep for the model will be thos that gave the highest accuracy, as loss is very similar for the last 4000 iterations.\n",
    "\n",
    "Note:\n",
    "- By manually changing the learning rate, we found out 0.05 to be the best one. We started with a high learning rate to make sure we didn't got stuck in a local minima, regardless of overshooting, and gradually descreased it to reach highest accuracy and minimum loss.\n",
    "    - I'm aware that we ca build a learning rate decay model to automate the gradual reduction in learning rate as we approach the minimum loss, but I think this simple approach is quite good too. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9395ed5a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Once we have trained and optimized parameters, we can save them and just load them again whevener we want to use the model. Just the forward pass will be required in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50af70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the weights and biases that give the best accuracy\n",
    "np.save('dense1_weights.npy', dense1.weights)\n",
    "np.save('dense1_biases.npy', dense1.biases)\n",
    "np.save('dense2_weights.npy', dense2.weights)\n",
    "np.save('dense2_biases.npy', dense2.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "51282889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.809\n"
     ]
    }
   ],
   "source": [
    "dense1 = Layer_Dense(4, 5)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(5, 2)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Load weights and biases\n",
    "dense1.weights = np.load('dense1_weights.npy')\n",
    "dense1.biases = np.load('dense1_biases.npy')\n",
    "dense2.weights = np.load('dense2_weights.npy')\n",
    "dense2.biases = np.load('dense2_biases.npy')\n",
    "\n",
    "\n",
    "# Just do a forward pass through the trained model\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss_activation.forward(dense2.output, training_results_array)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "true_labels = (\n",
    "    training_results_array\n",
    "    if training_results_array.ndim == 1\n",
    "    else np.argmax(training_results_array, axis=1)\n",
    ")\n",
    "accuracy = np.mean(predictions == true_labels)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df6f0b7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Here, you can check what have been our outputs (probability distribution between the 2 possible classes), and the final prediction made. \n",
    "- 0 = not survived\n",
    "- 1 = survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33f72112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [[0.01366455 0.98633545]\n",
      " [0.13516484 0.86483516]\n",
      " [0.01015673 0.98984327]\n",
      " [0.63633683 0.36366317]\n",
      " [0.0146259  0.9853741 ]]\n"
     ]
    }
   ],
   "source": [
    "print('Probabilities:', loss_activation.output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f07534a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print('Predictions:', predictions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079834a0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f94a257",
   "metadata": {},
   "source": [
    "**a. How well is this network performing compared to logistic regression? (3p)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16055bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4342359 ,  1.31606464,  0.63349426,  0.53528339],\n",
       "       [-0.24826042, -0.7598411 , -0.32341088, -0.54302242],\n",
       "       [-1.4342359 ,  1.31606464,  0.22339205,  0.53528339],\n",
       "       ...,\n",
       "       [-1.4342359 ,  1.31606464, -0.73351308, -0.54302242],\n",
       "       [ 0.93771507, -0.7598411 , -0.18671015, -0.54302242],\n",
       "       [ 0.93771507, -0.7598411 ,  0.22339205, -0.54302242]],\n",
       "      shape=(836, 4))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# all the data cleaning and preprocessing steps have been done prior to neural network. We are\n",
    "# going to use the same data for the logistic regression model.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e446f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "911058ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7238095238095238\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_reg.predict(X_test) \n",
    "y_pred_prob = log_reg.predict_proba(X_test)[:, 1]  \n",
    "print(\"Logistic Regression Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68086cd6",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "On the Titanic dataset, the neural network performs noticeably better at predicting passenger survival than logistic regression. The accuracy of the neural network was significantly higher at 0.815 than that of logistic regression, which was only about 0.724. \n",
    "\n",
    "This discrepancy demonstrates how the neural network's hidden layer and application of activation functions like ReLU enable it to identify complex, nonlinear relationships in the data. The linear nature of logistic regression, on the other hand, limits its capacity to simulate the relationships between characteristics like gender, age, and class. \n",
    "\n",
    "Despite the higher computational complexity, the neural network's results to better learn patterns or connections in data, resulting in more accurate predictions.\n",
    "\n",
    "Basically:\n",
    "\n",
    "- The neural network provides a clear performance improvement over logistic regression by capturing non-linear patterns in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3a2fe",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc49514",
   "metadata": {},
   "source": [
    "\n",
    "**b. Adapt the code so that you can use 5 input variables and add one variable of your choice. Did you perform better? What is the best layout of the network? (5p)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a74075",
   "metadata": {},
   "source": [
    "As fifth input, I'm going to add `fare`, which is the money paid for a journey on public transport. This might be related to the survival chances of the passenger, as those who paid more, most likely have preference to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57c58b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pclass        0\n",
       "sex           0\n",
       "age         263\n",
       "sibsp         0\n",
       "fare          1\n",
       "survived      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_v = titanic[['pclass', 'sex', 'age', 'sibsp', 'fare', 'survived']]\n",
    "titanic_v.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "545fa09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1045 entries, 0 to 1308\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   pclass    1045 non-null   int64  \n",
      " 1   sex       1045 non-null   object \n",
      " 2   age       1045 non-null   float64\n",
      " 3   sibsp     1045 non-null   int64  \n",
      " 4   fare      1045 non-null   float64\n",
      " 5   survived  1045 non-null   int64  \n",
      "dtypes: float64(2), int64(3), object(1)\n",
      "memory usage: 57.1+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic_v = titanic_v.dropna()\n",
    "titanic_v.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0233e832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UX325\\AppData\\Local\\Temp\\ipykernel_10660\\3209907120.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dummies = pd.get_dummies(titanic_v['sex']).applymap(int)\n"
     ]
    }
   ],
   "source": [
    "dummies = pd.get_dummies(titanic_v['sex']).applymap(int)\n",
    "titanic_v = pd.concat([titanic_v, dummies.female], axis=1)\n",
    "\n",
    "features = ['pclass', 'female', 'age', 'sibsp', 'fare']\n",
    "target = 'survived'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4fb2243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic_v[features].to_numpy()\n",
    "Y = titanic_v[target].to_numpy(dtype=int)\n",
    "\n",
    "#Normalize\n",
    "X, X_mean, X_std = normalise(X)\n",
    "\n",
    "#Re-encode Y to one-hot\n",
    "training_results = [vectorized_result(y) for y in Y]\n",
    "training_results_array = np.hstack(training_results).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80a5d7",
   "metadata": {},
   "source": [
    "For a 5 input structure, we just have to change the first Layer_Dense instance (`dense1`), from (4 inputs, 5 neurons) to (5 inputs, 5 neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "98e88753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.632, loss: 0.693\n",
      "epoch: 100, acc: 0.591, loss: 0.676\n",
      "epoch: 200, acc: 0.591, loss: 0.659\n",
      "epoch: 300, acc: 0.781, loss: 0.536\n",
      "epoch: 400, acc: 0.777, loss: 0.468\n",
      "epoch: 500, acc: 0.789, loss: 0.454\n",
      "epoch: 600, acc: 0.796, loss: 0.449\n",
      "epoch: 700, acc: 0.796, loss: 0.447\n",
      "epoch: 800, acc: 0.795, loss: 0.445\n",
      "epoch: 900, acc: 0.795, loss: 0.445\n",
      "epoch: 1000, acc: 0.794, loss: 0.444\n",
      "epoch: 1100, acc: 0.797, loss: 0.443\n",
      "epoch: 1200, acc: 0.798, loss: 0.443\n",
      "epoch: 1300, acc: 0.798, loss: 0.442\n",
      "epoch: 1400, acc: 0.798, loss: 0.442\n",
      "epoch: 1500, acc: 0.798, loss: 0.441\n",
      "epoch: 1600, acc: 0.798, loss: 0.441\n",
      "epoch: 1700, acc: 0.797, loss: 0.440\n",
      "epoch: 1800, acc: 0.797, loss: 0.440\n",
      "epoch: 1900, acc: 0.797, loss: 0.439\n",
      "epoch: 2000, acc: 0.797, loss: 0.439\n",
      "epoch: 2100, acc: 0.798, loss: 0.439\n",
      "epoch: 2200, acc: 0.799, loss: 0.438\n",
      "epoch: 2300, acc: 0.802, loss: 0.438\n",
      "epoch: 2400, acc: 0.801, loss: 0.438\n",
      "epoch: 2500, acc: 0.801, loss: 0.437\n",
      "epoch: 2600, acc: 0.802, loss: 0.437\n",
      "epoch: 2700, acc: 0.804, loss: 0.437\n",
      "epoch: 2800, acc: 0.804, loss: 0.437\n",
      "epoch: 2900, acc: 0.804, loss: 0.436\n",
      "epoch: 3000, acc: 0.805, loss: 0.436\n",
      "epoch: 3100, acc: 0.806, loss: 0.436\n",
      "epoch: 3200, acc: 0.805, loss: 0.435\n",
      "epoch: 3300, acc: 0.805, loss: 0.434\n",
      "epoch: 3400, acc: 0.807, loss: 0.434\n",
      "epoch: 3500, acc: 0.807, loss: 0.434\n",
      "epoch: 3600, acc: 0.807, loss: 0.433\n",
      "epoch: 3700, acc: 0.808, loss: 0.433\n",
      "epoch: 3800, acc: 0.808, loss: 0.433\n",
      "epoch: 3900, acc: 0.809, loss: 0.433\n",
      "epoch: 4000, acc: 0.809, loss: 0.433\n",
      "epoch: 4100, acc: 0.810, loss: 0.432\n",
      "epoch: 4200, acc: 0.811, loss: 0.432\n",
      "epoch: 4300, acc: 0.812, loss: 0.431\n",
      "epoch: 4400, acc: 0.814, loss: 0.431\n",
      "epoch: 4500, acc: 0.813, loss: 0.431\n",
      "epoch: 4600, acc: 0.812, loss: 0.431\n",
      "epoch: 4700, acc: 0.812, loss: 0.431\n",
      "epoch: 4800, acc: 0.812, loss: 0.431\n",
      "epoch: 4900, acc: 0.812, loss: 0.430\n",
      "epoch: 5000, acc: 0.814, loss: 0.430\n",
      "epoch: 5100, acc: 0.815, loss: 0.430\n",
      "epoch: 5200, acc: 0.815, loss: 0.430\n",
      "epoch: 5300, acc: 0.815, loss: 0.430\n",
      "epoch: 5400, acc: 0.816, loss: 0.430\n",
      "epoch: 5500, acc: 0.816, loss: 0.430\n",
      "epoch: 5600, acc: 0.817, loss: 0.430\n",
      "epoch: 5700, acc: 0.816, loss: 0.430\n",
      "epoch: 5800, acc: 0.816, loss: 0.430\n",
      "epoch: 5900, acc: 0.816, loss: 0.430\n",
      "epoch: 6000, acc: 0.816, loss: 0.429\n",
      "epoch: 6100, acc: 0.815, loss: 0.429\n",
      "epoch: 6200, acc: 0.815, loss: 0.429\n",
      "epoch: 6300, acc: 0.814, loss: 0.429\n",
      "epoch: 6400, acc: 0.815, loss: 0.429\n",
      "epoch: 6500, acc: 0.815, loss: 0.429\n",
      "epoch: 6600, acc: 0.815, loss: 0.429\n",
      "epoch: 6700, acc: 0.816, loss: 0.428\n",
      "epoch: 6800, acc: 0.815, loss: 0.428\n",
      "epoch: 6900, acc: 0.813, loss: 0.428\n",
      "epoch: 7000, acc: 0.812, loss: 0.428\n",
      "epoch: 7100, acc: 0.815, loss: 0.428\n",
      "epoch: 7200, acc: 0.815, loss: 0.428\n",
      "epoch: 7300, acc: 0.816, loss: 0.428\n",
      "epoch: 7400, acc: 0.814, loss: 0.428\n",
      "epoch: 7500, acc: 0.816, loss: 0.427\n",
      "epoch: 7600, acc: 0.816, loss: 0.427\n",
      "epoch: 7700, acc: 0.815, loss: 0.427\n",
      "epoch: 7800, acc: 0.816, loss: 0.427\n",
      "epoch: 7900, acc: 0.816, loss: 0.427\n",
      "epoch: 8000, acc: 0.816, loss: 0.427\n",
      "epoch: 8100, acc: 0.817, loss: 0.427\n",
      "epoch: 8200, acc: 0.816, loss: 0.427\n",
      "epoch: 8300, acc: 0.814, loss: 0.427\n",
      "epoch: 8400, acc: 0.814, loss: 0.425\n",
      "epoch: 8500, acc: 0.816, loss: 0.425\n",
      "epoch: 8600, acc: 0.814, loss: 0.424\n",
      "epoch: 8700, acc: 0.814, loss: 0.424\n",
      "epoch: 8800, acc: 0.816, loss: 0.424\n",
      "epoch: 8900, acc: 0.817, loss: 0.423\n",
      "epoch: 9000, acc: 0.818, loss: 0.423\n",
      "epoch: 9100, acc: 0.818, loss: 0.423\n",
      "epoch: 9200, acc: 0.818, loss: 0.423\n",
      "epoch: 9300, acc: 0.817, loss: 0.422\n",
      "epoch: 9400, acc: 0.817, loss: 0.422\n",
      "epoch: 9500, acc: 0.817, loss: 0.422\n",
      "epoch: 9600, acc: 0.817, loss: 0.422\n",
      "epoch: 9700, acc: 0.817, loss: 0.421\n",
      "epoch: 9800, acc: 0.816, loss: 0.421\n",
      "epoch: 9900, acc: 0.816, loss: 0.421\n",
      "epoch: 10000, acc: 0.819, loss: 0.421\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) # for reproducibility\n",
    "dense1 = Layer_Dense(5, 5)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(5, 2)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_GD()\n",
    "\n",
    "\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, training_results_array)\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(training_results_array.shape) == 2:\n",
    "        y_train = np.argmax(training_results_array, axis=1)\n",
    "    else:\n",
    "        y_train = training_results_array.copy()\n",
    "    accuracy = np.mean(predictions == y_train)\n",
    "    \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}')\n",
    "    \n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y_train)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    \n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0a3fc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.819\n",
      "Loss: 0.42096326912466286\n"
     ]
    }
   ],
   "source": [
    "# We save the weights and biases that give the best accuracy\n",
    "np.save('dense1_weights.npy', dense1.weights)\n",
    "np.save('dense1_biases.npy', dense1.biases)\n",
    "np.save('dense2_weights.npy', dense2.weights)\n",
    "np.save('dense2_biases.npy', dense2.biases)\n",
    "\n",
    "dense1 = Layer_Dense(5, 5)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(5, 2)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Load weights and biases\n",
    "dense1.weights = np.load('dense1_weights.npy')\n",
    "dense1.biases = np.load('dense1_biases.npy')\n",
    "dense2.weights = np.load('dense2_weights.npy')\n",
    "dense2.biases = np.load('dense2_biases.npy')\n",
    "\n",
    "\n",
    "# Just do a forward pass through the trained model\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss_activation.forward(dense2.output, training_results_array)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "true_labels = (\n",
    "    training_results_array\n",
    "    if training_results_array.ndim == 1\n",
    "    else np.argmax(training_results_array, axis=1)\n",
    ")\n",
    "accuracy = np.mean(predictions == true_labels)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.3f}')\n",
    "print('Loss:', loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a284f98",
   "metadata": {},
   "source": [
    "With a 5 input setup, using the same layout as before of 5 neurons in the hidden layer and 2 outputs, and keeping the same learning rate of 0.05, we get both better accuracy and lower loss, which means that including `fare` as variable in the model, has helped us in making more accurate predictions on Titanic survival."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adabb55",
   "metadata": {},
   "source": [
    "However, for a better neural network layout, let's experiment with different layouts. I could keep trying many layouts for a long time, but I'm just going to try 2 modifications:\n",
    "- More neurons in the hidden layer (10 neurons -- [5,10,2])\n",
    "- Add another hidden layer ([5,8,4,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "afc80c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.448, loss: 0.693\n",
      "epoch: 100, acc: 0.591, loss: 0.676\n",
      "epoch: 200, acc: 0.591, loss: 0.665\n",
      "epoch: 300, acc: 0.720, loss: 0.584\n",
      "epoch: 400, acc: 0.777, loss: 0.479\n",
      "epoch: 500, acc: 0.784, loss: 0.457\n",
      "epoch: 600, acc: 0.796, loss: 0.449\n",
      "epoch: 700, acc: 0.799, loss: 0.445\n",
      "epoch: 800, acc: 0.801, loss: 0.443\n",
      "epoch: 900, acc: 0.801, loss: 0.441\n",
      "epoch: 1000, acc: 0.800, loss: 0.439\n",
      "epoch: 1100, acc: 0.805, loss: 0.438\n",
      "epoch: 1200, acc: 0.803, loss: 0.437\n",
      "epoch: 1300, acc: 0.805, loss: 0.436\n",
      "epoch: 1400, acc: 0.804, loss: 0.435\n",
      "epoch: 1500, acc: 0.806, loss: 0.434\n",
      "epoch: 1600, acc: 0.805, loss: 0.434\n",
      "epoch: 1700, acc: 0.806, loss: 0.433\n",
      "epoch: 1800, acc: 0.807, loss: 0.433\n",
      "epoch: 1900, acc: 0.808, loss: 0.432\n",
      "epoch: 2000, acc: 0.808, loss: 0.432\n",
      "epoch: 2100, acc: 0.810, loss: 0.432\n",
      "epoch: 2200, acc: 0.810, loss: 0.431\n",
      "epoch: 2300, acc: 0.811, loss: 0.431\n",
      "epoch: 2400, acc: 0.811, loss: 0.431\n",
      "epoch: 2500, acc: 0.811, loss: 0.431\n",
      "epoch: 2600, acc: 0.812, loss: 0.431\n",
      "epoch: 2700, acc: 0.813, loss: 0.430\n",
      "epoch: 2800, acc: 0.812, loss: 0.430\n",
      "epoch: 2900, acc: 0.812, loss: 0.429\n",
      "epoch: 3000, acc: 0.812, loss: 0.428\n",
      "epoch: 3100, acc: 0.813, loss: 0.427\n",
      "epoch: 3200, acc: 0.811, loss: 0.427\n",
      "epoch: 3300, acc: 0.811, loss: 0.426\n",
      "epoch: 3400, acc: 0.811, loss: 0.426\n",
      "epoch: 3500, acc: 0.811, loss: 0.426\n",
      "epoch: 3600, acc: 0.811, loss: 0.426\n",
      "epoch: 3700, acc: 0.811, loss: 0.425\n",
      "epoch: 3800, acc: 0.812, loss: 0.425\n",
      "epoch: 3900, acc: 0.813, loss: 0.425\n",
      "epoch: 4000, acc: 0.813, loss: 0.425\n",
      "epoch: 4100, acc: 0.814, loss: 0.425\n",
      "epoch: 4200, acc: 0.813, loss: 0.425\n",
      "epoch: 4300, acc: 0.814, loss: 0.424\n",
      "epoch: 4400, acc: 0.813, loss: 0.424\n",
      "epoch: 4500, acc: 0.813, loss: 0.424\n",
      "epoch: 4600, acc: 0.813, loss: 0.424\n",
      "epoch: 4700, acc: 0.814, loss: 0.424\n",
      "epoch: 4800, acc: 0.814, loss: 0.424\n",
      "epoch: 4900, acc: 0.814, loss: 0.424\n",
      "epoch: 5000, acc: 0.814, loss: 0.424\n",
      "epoch: 5100, acc: 0.814, loss: 0.424\n",
      "epoch: 5200, acc: 0.815, loss: 0.423\n",
      "epoch: 5300, acc: 0.816, loss: 0.423\n",
      "epoch: 5400, acc: 0.816, loss: 0.423\n",
      "epoch: 5500, acc: 0.816, loss: 0.423\n",
      "epoch: 5600, acc: 0.816, loss: 0.423\n",
      "epoch: 5700, acc: 0.816, loss: 0.423\n",
      "epoch: 5800, acc: 0.815, loss: 0.423\n",
      "epoch: 5900, acc: 0.816, loss: 0.423\n",
      "epoch: 6000, acc: 0.815, loss: 0.423\n",
      "epoch: 6100, acc: 0.815, loss: 0.423\n",
      "epoch: 6200, acc: 0.815, loss: 0.422\n",
      "epoch: 6300, acc: 0.815, loss: 0.422\n",
      "epoch: 6400, acc: 0.815, loss: 0.422\n",
      "epoch: 6500, acc: 0.815, loss: 0.422\n",
      "epoch: 6600, acc: 0.814, loss: 0.422\n",
      "epoch: 6700, acc: 0.814, loss: 0.422\n",
      "epoch: 6800, acc: 0.814, loss: 0.422\n",
      "epoch: 6900, acc: 0.814, loss: 0.422\n",
      "epoch: 7000, acc: 0.814, loss: 0.422\n",
      "epoch: 7100, acc: 0.814, loss: 0.422\n",
      "epoch: 7200, acc: 0.814, loss: 0.422\n",
      "epoch: 7300, acc: 0.814, loss: 0.422\n",
      "epoch: 7400, acc: 0.814, loss: 0.422\n",
      "epoch: 7500, acc: 0.815, loss: 0.422\n",
      "epoch: 7600, acc: 0.815, loss: 0.422\n",
      "epoch: 7700, acc: 0.815, loss: 0.422\n",
      "epoch: 7800, acc: 0.815, loss: 0.422\n",
      "epoch: 7900, acc: 0.815, loss: 0.422\n",
      "epoch: 8000, acc: 0.815, loss: 0.422\n",
      "epoch: 8100, acc: 0.815, loss: 0.422\n",
      "epoch: 8200, acc: 0.815, loss: 0.422\n",
      "epoch: 8300, acc: 0.815, loss: 0.422\n",
      "epoch: 8400, acc: 0.815, loss: 0.422\n",
      "epoch: 8500, acc: 0.815, loss: 0.422\n",
      "epoch: 8600, acc: 0.814, loss: 0.421\n",
      "epoch: 8700, acc: 0.815, loss: 0.421\n",
      "epoch: 8800, acc: 0.814, loss: 0.421\n",
      "epoch: 8900, acc: 0.816, loss: 0.421\n",
      "epoch: 9000, acc: 0.816, loss: 0.421\n",
      "epoch: 9100, acc: 0.814, loss: 0.420\n",
      "epoch: 9200, acc: 0.816, loss: 0.420\n",
      "epoch: 9300, acc: 0.816, loss: 0.420\n",
      "epoch: 9400, acc: 0.817, loss: 0.420\n",
      "epoch: 9500, acc: 0.816, loss: 0.420\n",
      "epoch: 9600, acc: 0.817, loss: 0.420\n",
      "epoch: 9700, acc: 0.817, loss: 0.420\n",
      "epoch: 9800, acc: 0.817, loss: 0.420\n",
      "epoch: 9900, acc: 0.817, loss: 0.420\n",
      "epoch: 10000, acc: 0.817, loss: 0.420\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) # for reproducibility\n",
    "dense1 = Layer_Dense(5, 10)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(10, 2)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_GD()\n",
    "\n",
    "\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, training_results_array)\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(training_results_array.shape) == 2:\n",
    "        y_train = np.argmax(training_results_array, axis=1)\n",
    "    else:\n",
    "        y_train = training_results_array.copy()\n",
    "    accuracy = np.mean(predictions == y_train)\n",
    "    \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}')\n",
    "    \n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y_train)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    \n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce72c1",
   "metadata": {},
   "source": [
    "Even though we haven't achieved higher accuracy or significantly lower loss, we can see that with more neurons in the hidden layer, we've reached a very also high accuracy and low loss in very few iterations. \n",
    "\n",
    "This could be due to the fact taht a wider hidden layer can speed up learning by allowing the network to capture patterns more efficiently. However, we've learnt that for small or simple datasets, more complexity doesnt always mean better final accuracy, it might just get you there faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7780423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.331, loss: 0.693\n",
      "epoch: 100, acc: 0.591, loss: 0.676\n",
      "epoch: 200, acc: 0.591, loss: 0.676\n",
      "epoch: 300, acc: 0.591, loss: 0.676\n",
      "epoch: 400, acc: 0.591, loss: 0.676\n",
      "epoch: 500, acc: 0.591, loss: 0.676\n",
      "epoch: 600, acc: 0.591, loss: 0.676\n",
      "epoch: 700, acc: 0.591, loss: 0.676\n",
      "epoch: 800, acc: 0.591, loss: 0.676\n",
      "epoch: 900, acc: 0.591, loss: 0.676\n",
      "epoch: 1000, acc: 0.591, loss: 0.676\n",
      "epoch: 1100, acc: 0.591, loss: 0.676\n",
      "epoch: 1200, acc: 0.591, loss: 0.676\n",
      "epoch: 1300, acc: 0.591, loss: 0.676\n",
      "epoch: 1400, acc: 0.591, loss: 0.676\n",
      "epoch: 1500, acc: 0.591, loss: 0.676\n",
      "epoch: 1600, acc: 0.591, loss: 0.676\n",
      "epoch: 1700, acc: 0.591, loss: 0.676\n",
      "epoch: 1800, acc: 0.591, loss: 0.676\n",
      "epoch: 1900, acc: 0.591, loss: 0.676\n",
      "epoch: 2000, acc: 0.591, loss: 0.676\n",
      "epoch: 2100, acc: 0.591, loss: 0.676\n",
      "epoch: 2200, acc: 0.591, loss: 0.676\n",
      "epoch: 2300, acc: 0.591, loss: 0.676\n",
      "epoch: 2400, acc: 0.591, loss: 0.676\n",
      "epoch: 2500, acc: 0.591, loss: 0.676\n",
      "epoch: 2600, acc: 0.591, loss: 0.676\n",
      "epoch: 2700, acc: 0.591, loss: 0.676\n",
      "epoch: 2800, acc: 0.591, loss: 0.676\n",
      "epoch: 2900, acc: 0.591, loss: 0.676\n",
      "epoch: 3000, acc: 0.591, loss: 0.676\n",
      "epoch: 3100, acc: 0.591, loss: 0.676\n",
      "epoch: 3200, acc: 0.591, loss: 0.676\n",
      "epoch: 3300, acc: 0.591, loss: 0.676\n",
      "epoch: 3400, acc: 0.591, loss: 0.676\n",
      "epoch: 3500, acc: 0.591, loss: 0.676\n",
      "epoch: 3600, acc: 0.591, loss: 0.676\n",
      "epoch: 3700, acc: 0.591, loss: 0.676\n",
      "epoch: 3800, acc: 0.591, loss: 0.676\n",
      "epoch: 3900, acc: 0.591, loss: 0.676\n",
      "epoch: 4000, acc: 0.591, loss: 0.676\n",
      "epoch: 4100, acc: 0.591, loss: 0.676\n",
      "epoch: 4200, acc: 0.591, loss: 0.676\n",
      "epoch: 4300, acc: 0.591, loss: 0.675\n",
      "epoch: 4400, acc: 0.591, loss: 0.674\n",
      "epoch: 4500, acc: 0.591, loss: 0.671\n",
      "epoch: 4600, acc: 0.591, loss: 0.658\n",
      "epoch: 4700, acc: 0.591, loss: 0.600\n",
      "epoch: 4800, acc: 0.789, loss: 0.524\n",
      "epoch: 4900, acc: 0.783, loss: 0.495\n",
      "epoch: 5000, acc: 0.787, loss: 0.483\n",
      "epoch: 5100, acc: 0.790, loss: 0.476\n",
      "epoch: 5200, acc: 0.793, loss: 0.472\n",
      "epoch: 5300, acc: 0.794, loss: 0.468\n",
      "epoch: 5400, acc: 0.797, loss: 0.465\n",
      "epoch: 5500, acc: 0.802, loss: 0.460\n",
      "epoch: 5600, acc: 0.801, loss: 0.456\n",
      "epoch: 5700, acc: 0.803, loss: 0.451\n",
      "epoch: 5800, acc: 0.798, loss: 0.447\n",
      "epoch: 5900, acc: 0.806, loss: 0.443\n",
      "epoch: 6000, acc: 0.808, loss: 0.440\n",
      "epoch: 6100, acc: 0.808, loss: 0.438\n",
      "epoch: 6200, acc: 0.810, loss: 0.437\n",
      "epoch: 6300, acc: 0.811, loss: 0.435\n",
      "epoch: 6400, acc: 0.812, loss: 0.435\n",
      "epoch: 6500, acc: 0.812, loss: 0.434\n",
      "epoch: 6600, acc: 0.811, loss: 0.433\n",
      "epoch: 6700, acc: 0.809, loss: 0.433\n",
      "epoch: 6800, acc: 0.809, loss: 0.433\n",
      "epoch: 6900, acc: 0.808, loss: 0.432\n",
      "epoch: 7000, acc: 0.809, loss: 0.432\n",
      "epoch: 7100, acc: 0.809, loss: 0.432\n",
      "epoch: 7200, acc: 0.808, loss: 0.432\n",
      "epoch: 7300, acc: 0.809, loss: 0.431\n",
      "epoch: 7400, acc: 0.809, loss: 0.431\n",
      "epoch: 7500, acc: 0.808, loss: 0.431\n",
      "epoch: 7600, acc: 0.809, loss: 0.431\n",
      "epoch: 7700, acc: 0.810, loss: 0.431\n",
      "epoch: 7800, acc: 0.810, loss: 0.431\n",
      "epoch: 7900, acc: 0.809, loss: 0.430\n",
      "epoch: 8000, acc: 0.809, loss: 0.430\n",
      "epoch: 8100, acc: 0.810, loss: 0.430\n",
      "epoch: 8200, acc: 0.810, loss: 0.430\n",
      "epoch: 8300, acc: 0.810, loss: 0.429\n",
      "epoch: 8400, acc: 0.810, loss: 0.429\n",
      "epoch: 8500, acc: 0.811, loss: 0.429\n",
      "epoch: 8600, acc: 0.810, loss: 0.429\n",
      "epoch: 8700, acc: 0.811, loss: 0.428\n",
      "epoch: 8800, acc: 0.811, loss: 0.428\n",
      "epoch: 8900, acc: 0.811, loss: 0.427\n",
      "epoch: 9000, acc: 0.813, loss: 0.427\n",
      "epoch: 9100, acc: 0.813, loss: 0.426\n",
      "epoch: 9200, acc: 0.814, loss: 0.425\n",
      "epoch: 9300, acc: 0.817, loss: 0.424\n",
      "epoch: 9400, acc: 0.819, loss: 0.423\n",
      "epoch: 9500, acc: 0.821, loss: 0.422\n",
      "epoch: 9600, acc: 0.824, loss: 0.421\n",
      "epoch: 9700, acc: 0.825, loss: 0.420\n",
      "epoch: 9800, acc: 0.824, loss: 0.419\n",
      "epoch: 9900, acc: 0.824, loss: 0.419\n",
      "epoch: 10000, acc: 0.825, loss: 0.418\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) # for reproducibility\n",
    "dense1 = Layer_Dense(5, 8)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(8, 4)\n",
    "activation2 = Activation_ReLU()\n",
    "dense3 = Layer_Dense(4, 2)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_GD()\n",
    "\n",
    "\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    dense3.forward(activation2.output)\n",
    "    loss = loss_activation.forward(dense3.output, training_results_array)\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(training_results_array.shape) == 2:\n",
    "        y_train = np.argmax(training_results_array, axis=1)\n",
    "    else:\n",
    "        y_train = training_results_array.copy()\n",
    "    accuracy = np.mean(predictions == y_train)\n",
    "    \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}')\n",
    "    \n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y_train)\n",
    "    dense3.backward(loss_activation.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    \n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.update_params(dense3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20814f8a",
   "metadata": {},
   "source": [
    "Based on the results provided, the [5, 5, 2] layout consistently reaches high accuracy (~81.9%) and low loss (~0.421) much faster than the [5, 8, 4, 2] layout, which only reaches comparable accuracy after 5,000 epochs.\n",
    "\n",
    "However, with the added layer, we reach higher accuracy and even lower loss (although there isn't much difference)\n",
    "\n",
    "Given its quicker convergence, cheaper training cost, and nearly identical performance to the more intricate [5, 8, 4, 2] model, the [5, 5, 2] layout is probably the better option overall. \n",
    "\n",
    "The performance difference between the two is negligible, but the deeper (extra layer) layout might be taken into consideration if training efficiency is less of an issue and maximum accuracy is the primary goal. Therefore, in the majority of situations, the [5,5,2] network is the most efficient choice due to its efficiency and simplicity of model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342c654",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "> Optimizing for speed and simplicity, the [5, 5, 2] layout is clearly the most efficient and practical choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6635d5ce",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07de4a",
   "metadata": {},
   "source": [
    "\n",
    "**c. Why do we have two output nodes? What happens if you just use one? Can you adapt a network with just one output node to this problem? (2p)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496a967",
   "metadata": {},
   "source": [
    "we have to output nodes because we have 2 possible outcomes (0: not survived, and 1: survived), and we use softmax activation function to represent the probability distribution across the two classes in binary classification problems. \n",
    "\n",
    "The model learns to predict the likelihood of each class given the input, with each output node representing a single class. \n",
    "\n",
    "Nevertheless, using a single output node with a sigmoid activation function, which produces a probability between 0 and 1 indicating the likelihood that the input belongs to a specific class (usually class 1), is also feasible and effective. This single-node configuration works well for binary classification tasks and is frequently used in conjunction with the binary cross-entropy loss function.\n",
    "\n",
    "We used softmax activation because we previously one-hot encoded the `survived` variable, transforming it into an array of 2 numbers:\n",
    "- 0 = not survived\n",
    "- 1 = survived\n",
    "\n",
    "If we hadn't transformed it to an array, and left it binarized (0 or 1) as it was originally, we could have used Sigmoid activation instead, and get 1 unique output node.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444dd52",
   "metadata": {},
   "source": [
    "Using Softmax activation with Categorical Crossentropy loss is only appropriate when you have two or more output nodes representing mutually exclusive classes, therefore, to adapt the network to one output node, I need to:\n",
    "- Use binary `survived` class, in the original form, not in an array.\n",
    "- Create a Sigmoid activation class\n",
    "- Create a Binary Cross-Entropy loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4fb05ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = titanic_v[features].to_numpy()\n",
    "Y_2 = titanic_v[target].to_numpy(dtype=int)\n",
    "\n",
    "X_2, X_mean_2, X_std_2 = normalise(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9dfde4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.43391396,  1.30064545, -0.05919979, -0.55163305,  3.13374271],\n",
       "       [-1.43391396, -0.76811329, -2.01066414,  0.54429193,  2.06098509],\n",
       "       [-1.43391396,  1.30064545, -1.93560782,  0.54429193,  2.06098509],\n",
       "       ...,\n",
       "       [ 0.94267619, -0.76811329, -0.23294128, -0.55163305, -0.52861549],\n",
       "       [ 0.94267619, -0.76811329, -0.19819298, -0.55163305, -0.52861549],\n",
       "       [ 0.94267619, -0.76811329, -0.05919979, -0.55163305, -0.51695264]],\n",
       "      shape=(1045, 5))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "103b5aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 0, 0], shape=(1045,))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f24984fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "\n",
    "class Loss_BinaryCrossentropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        self.output = - (y_true * np.log(y_pred_clipped) +\n",
    "                         (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        return np.mean(self.output)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "    # Reshape y_true to match dvalues shape if necessary\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "\n",
    "        dvalues_clipped = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        self.dinputs = -(y_true / dvalues_clipped - (1 - y_true) / (1 - dvalues_clipped)) / len(dvalues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "47960dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.576, loss: 0.693\n",
      "epoch: 1000, acc: 0.533, loss: 1.081\n",
      "epoch: 2000, acc: 0.533, loss: 1.100\n",
      "epoch: 3000, acc: 0.533, loss: 1.120\n",
      "epoch: 4000, acc: 0.533, loss: 1.124\n",
      "epoch: 5000, acc: 0.533, loss: 1.126\n",
      "epoch: 6000, acc: 0.533, loss: 1.128\n",
      "epoch: 7000, acc: 0.533, loss: 1.130\n",
      "epoch: 8000, acc: 0.533, loss: 1.130\n",
      "epoch: 9000, acc: 0.533, loss: 1.131\n",
      "epoch: 10000, acc: 0.533, loss: 1.129\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) # for reproducibility\n",
    "dense1 = Layer_Dense(5, 5)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(5, 1)\n",
    "activation2 = Activation_Sigmoid()\n",
    "loss_function = Loss_BinaryCrossentropy()\n",
    "optimizer = Optimizer_GD(learning_rate=0.5)\n",
    "\n",
    "\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(X_2)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    loss = loss_function.forward(activation2.output, Y_2)\n",
    "\n",
    "    predictions = (activation2.output > 0.5).astype(int)\n",
    "    accuracy = np.mean(predictions == Y_2)\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "\n",
    "    loss_function.backward(activation2.output, Y_2)\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb4793",
   "metadata": {},
   "source": [
    "Very bad results, and it took a very long time to run compared to the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811259e3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10641967",
   "metadata": {},
   "source": [
    "\n",
    "**d. Adapt the code so that it can run batches or mini batches. For an example, see solutions to Homework 3! (2p)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c0f6d",
   "metadata": {},
   "source": [
    "Now, we are going to update the training loop so that instead of feeding all the data at once (full batch), or just one sample at a time (stochastic), we feed the data in small groups (mini-batches).\n",
    "\n",
    "The purpose of implementing mini-batches is to:\n",
    "\n",
    "- Speed up training, as it uses less memory than a full batch, and is faster than one-sample-at-a-time (stochastic).\n",
    "\n",
    "- Reduces overfitting, due to some randomness added.\n",
    "\n",
    "- Allows more stable updates than stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d92477e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.656, loss: 0.693\n",
      "epoch: 0, acc: 0.562, loss: 0.692\n",
      "epoch: 0, acc: 0.812, loss: 0.687\n",
      "epoch: 0, acc: 0.656, loss: 0.686\n",
      "epoch: 0, acc: 0.594, loss: 0.688\n",
      "epoch: 0, acc: 0.562, loss: 0.689\n",
      "epoch: 0, acc: 0.500, loss: 0.694\n",
      "epoch: 0, acc: 0.562, loss: 0.689\n",
      "epoch: 0, acc: 0.656, loss: 0.682\n",
      "epoch: 0, acc: 0.469, loss: 0.697\n",
      "epoch: 0, acc: 0.469, loss: 0.697\n",
      "epoch: 0, acc: 0.312, loss: 0.709\n",
      "epoch: 0, acc: 0.594, loss: 0.688\n",
      "epoch: 0, acc: 0.469, loss: 0.696\n",
      "epoch: 0, acc: 0.469, loss: 0.696\n",
      "epoch: 0, acc: 0.531, loss: 0.692\n",
      "epoch: 0, acc: 0.688, loss: 0.682\n",
      "epoch: 0, acc: 0.719, loss: 0.677\n",
      "epoch: 0, acc: 0.500, loss: 0.694\n",
      "epoch: 0, acc: 0.750, loss: 0.670\n",
      "epoch: 0, acc: 0.719, loss: 0.669\n",
      "epoch: 0, acc: 0.531, loss: 0.691\n",
      "epoch: 0, acc: 0.469, loss: 0.700\n",
      "epoch: 0, acc: 0.688, loss: 0.671\n",
      "epoch: 0, acc: 0.688, loss: 0.668\n",
      "epoch: 0, acc: 0.594, loss: 0.681\n",
      "epoch: 0, acc: 0.594, loss: 0.681\n",
      "epoch: 0, acc: 0.625, loss: 0.675\n",
      "epoch: 0, acc: 0.625, loss: 0.675\n",
      "epoch: 0, acc: 0.625, loss: 0.674\n",
      "epoch: 0, acc: 0.688, loss: 0.661\n",
      "epoch: 0, acc: 0.469, loss: 0.705\n",
      "epoch: 0, acc: 0.762, loss: 0.645\n",
      "epoch: 100, acc: 0.938, loss: 0.399\n",
      "epoch: 100, acc: 0.719, loss: 0.520\n",
      "epoch: 100, acc: 0.812, loss: 0.496\n",
      "epoch: 100, acc: 0.594, loss: 0.710\n",
      "epoch: 100, acc: 0.875, loss: 0.314\n",
      "epoch: 100, acc: 0.906, loss: 0.340\n",
      "epoch: 100, acc: 0.938, loss: 0.252\n",
      "epoch: 100, acc: 0.781, loss: 0.510\n",
      "epoch: 100, acc: 0.844, loss: 0.339\n",
      "epoch: 100, acc: 0.812, loss: 0.441\n",
      "epoch: 100, acc: 0.844, loss: 0.376\n",
      "epoch: 100, acc: 0.688, loss: 0.508\n",
      "epoch: 100, acc: 0.844, loss: 0.461\n",
      "epoch: 100, acc: 0.812, loss: 0.395\n",
      "epoch: 100, acc: 0.781, loss: 0.385\n",
      "epoch: 100, acc: 0.844, loss: 0.330\n",
      "epoch: 100, acc: 0.781, loss: 0.485\n",
      "epoch: 100, acc: 0.812, loss: 0.366\n",
      "epoch: 100, acc: 0.906, loss: 0.303\n",
      "epoch: 100, acc: 0.656, loss: 0.666\n",
      "epoch: 100, acc: 0.812, loss: 0.386\n",
      "epoch: 100, acc: 0.875, loss: 0.288\n",
      "epoch: 100, acc: 0.750, loss: 0.466\n",
      "epoch: 100, acc: 0.938, loss: 0.302\n",
      "epoch: 100, acc: 0.750, loss: 0.443\n",
      "epoch: 100, acc: 0.906, loss: 0.358\n",
      "epoch: 100, acc: 0.688, loss: 0.618\n",
      "epoch: 100, acc: 0.781, loss: 0.442\n",
      "epoch: 100, acc: 0.812, loss: 0.398\n",
      "epoch: 100, acc: 0.812, loss: 0.363\n",
      "epoch: 100, acc: 0.906, loss: 0.406\n",
      "epoch: 100, acc: 0.750, loss: 0.581\n",
      "epoch: 100, acc: 0.667, loss: 0.613\n",
      "epoch: 200, acc: 0.719, loss: 0.392\n",
      "epoch: 200, acc: 0.750, loss: 0.640\n",
      "epoch: 200, acc: 0.875, loss: 0.309\n",
      "epoch: 200, acc: 0.844, loss: 0.337\n",
      "epoch: 200, acc: 0.906, loss: 0.303\n",
      "epoch: 200, acc: 0.781, loss: 0.525\n",
      "epoch: 200, acc: 0.844, loss: 0.318\n",
      "epoch: 200, acc: 0.812, loss: 0.444\n",
      "epoch: 200, acc: 0.812, loss: 0.440\n",
      "epoch: 200, acc: 0.812, loss: 0.468\n",
      "epoch: 200, acc: 0.812, loss: 0.450\n",
      "epoch: 200, acc: 0.750, loss: 0.515\n",
      "epoch: 200, acc: 0.781, loss: 0.474\n",
      "epoch: 200, acc: 0.812, loss: 0.425\n",
      "epoch: 200, acc: 0.844, loss: 0.515\n",
      "epoch: 200, acc: 0.938, loss: 0.232\n",
      "epoch: 200, acc: 0.719, loss: 0.520\n",
      "epoch: 200, acc: 0.781, loss: 0.476\n",
      "epoch: 200, acc: 0.844, loss: 0.430\n",
      "epoch: 200, acc: 0.750, loss: 0.478\n",
      "epoch: 200, acc: 0.906, loss: 0.326\n",
      "epoch: 200, acc: 0.781, loss: 0.509\n",
      "epoch: 200, acc: 0.781, loss: 0.391\n",
      "epoch: 200, acc: 0.875, loss: 0.353\n",
      "epoch: 200, acc: 0.719, loss: 0.455\n",
      "epoch: 200, acc: 0.906, loss: 0.342\n",
      "epoch: 200, acc: 0.812, loss: 0.455\n",
      "epoch: 200, acc: 0.938, loss: 0.344\n",
      "epoch: 200, acc: 0.812, loss: 0.442\n",
      "epoch: 200, acc: 0.719, loss: 0.469\n",
      "epoch: 200, acc: 0.844, loss: 0.343\n",
      "epoch: 200, acc: 0.750, loss: 0.627\n",
      "epoch: 200, acc: 0.952, loss: 0.256\n",
      "epoch: 300, acc: 0.906, loss: 0.258\n",
      "epoch: 300, acc: 0.812, loss: 0.385\n",
      "epoch: 300, acc: 0.781, loss: 0.466\n",
      "epoch: 300, acc: 0.781, loss: 0.417\n",
      "epoch: 300, acc: 0.875, loss: 0.389\n",
      "epoch: 300, acc: 0.844, loss: 0.356\n",
      "epoch: 300, acc: 0.875, loss: 0.327\n",
      "epoch: 300, acc: 0.812, loss: 0.377\n",
      "epoch: 300, acc: 0.719, loss: 0.565\n",
      "epoch: 300, acc: 0.906, loss: 0.342\n",
      "epoch: 300, acc: 0.719, loss: 0.593\n",
      "epoch: 300, acc: 0.906, loss: 0.281\n",
      "epoch: 300, acc: 0.875, loss: 0.273\n",
      "epoch: 300, acc: 0.844, loss: 0.401\n",
      "epoch: 300, acc: 0.719, loss: 0.542\n",
      "epoch: 300, acc: 0.781, loss: 0.534\n",
      "epoch: 300, acc: 0.812, loss: 0.455\n",
      "epoch: 300, acc: 0.844, loss: 0.479\n",
      "epoch: 300, acc: 0.781, loss: 0.399\n",
      "epoch: 300, acc: 0.875, loss: 0.324\n",
      "epoch: 300, acc: 0.719, loss: 0.495\n",
      "epoch: 300, acc: 0.844, loss: 0.512\n",
      "epoch: 300, acc: 0.750, loss: 0.487\n",
      "epoch: 300, acc: 0.812, loss: 0.404\n",
      "epoch: 300, acc: 0.938, loss: 0.274\n",
      "epoch: 300, acc: 0.719, loss: 0.587\n",
      "epoch: 300, acc: 0.875, loss: 0.403\n",
      "epoch: 300, acc: 0.781, loss: 0.523\n",
      "epoch: 300, acc: 0.750, loss: 0.443\n",
      "epoch: 300, acc: 0.781, loss: 0.433\n",
      "epoch: 300, acc: 0.781, loss: 0.484\n",
      "epoch: 300, acc: 0.781, loss: 0.451\n",
      "epoch: 300, acc: 0.905, loss: 0.275\n",
      "epoch: 400, acc: 0.844, loss: 0.469\n",
      "epoch: 400, acc: 0.844, loss: 0.293\n",
      "epoch: 400, acc: 0.781, loss: 0.398\n",
      "epoch: 400, acc: 0.906, loss: 0.306\n",
      "epoch: 400, acc: 0.750, loss: 0.443\n",
      "epoch: 400, acc: 0.875, loss: 0.361\n",
      "epoch: 400, acc: 0.781, loss: 0.432\n",
      "epoch: 400, acc: 0.844, loss: 0.542\n",
      "epoch: 400, acc: 0.781, loss: 0.504\n",
      "epoch: 400, acc: 0.781, loss: 0.543\n",
      "epoch: 400, acc: 0.906, loss: 0.301\n",
      "epoch: 400, acc: 0.781, loss: 0.483\n",
      "epoch: 400, acc: 0.781, loss: 0.475\n",
      "epoch: 400, acc: 0.719, loss: 0.576\n",
      "epoch: 400, acc: 0.906, loss: 0.423\n",
      "epoch: 400, acc: 0.750, loss: 0.452\n",
      "epoch: 400, acc: 0.781, loss: 0.406\n",
      "epoch: 400, acc: 0.656, loss: 0.713\n",
      "epoch: 400, acc: 0.844, loss: 0.350\n",
      "epoch: 400, acc: 0.781, loss: 0.427\n",
      "epoch: 400, acc: 0.812, loss: 0.427\n",
      "epoch: 400, acc: 0.781, loss: 0.428\n",
      "epoch: 400, acc: 0.812, loss: 0.395\n",
      "epoch: 400, acc: 0.844, loss: 0.320\n",
      "epoch: 400, acc: 0.781, loss: 0.393\n",
      "epoch: 400, acc: 0.906, loss: 0.314\n",
      "epoch: 400, acc: 0.812, loss: 0.456\n",
      "epoch: 400, acc: 0.781, loss: 0.503\n",
      "epoch: 400, acc: 0.844, loss: 0.391\n",
      "epoch: 400, acc: 0.812, loss: 0.413\n",
      "epoch: 400, acc: 0.875, loss: 0.328\n",
      "epoch: 400, acc: 0.875, loss: 0.310\n",
      "epoch: 400, acc: 0.905, loss: 0.383\n",
      "epoch: 500, acc: 0.875, loss: 0.332\n",
      "epoch: 500, acc: 0.781, loss: 0.569\n",
      "epoch: 500, acc: 0.938, loss: 0.220\n",
      "epoch: 500, acc: 0.750, loss: 0.506\n",
      "epoch: 500, acc: 0.688, loss: 0.652\n",
      "epoch: 500, acc: 0.906, loss: 0.273\n",
      "epoch: 500, acc: 0.875, loss: 0.321\n",
      "epoch: 500, acc: 0.812, loss: 0.495\n",
      "epoch: 500, acc: 0.719, loss: 0.624\n",
      "epoch: 500, acc: 0.812, loss: 0.446\n",
      "epoch: 500, acc: 0.875, loss: 0.309\n",
      "epoch: 500, acc: 0.781, loss: 0.439\n",
      "epoch: 500, acc: 0.781, loss: 0.419\n",
      "epoch: 500, acc: 0.938, loss: 0.243\n",
      "epoch: 500, acc: 0.750, loss: 0.568\n",
      "epoch: 500, acc: 0.844, loss: 0.382\n",
      "epoch: 500, acc: 0.688, loss: 0.555\n",
      "epoch: 500, acc: 0.844, loss: 0.372\n",
      "epoch: 500, acc: 0.875, loss: 0.334\n",
      "epoch: 500, acc: 0.844, loss: 0.409\n",
      "epoch: 500, acc: 0.844, loss: 0.327\n",
      "epoch: 500, acc: 0.906, loss: 0.334\n",
      "epoch: 500, acc: 0.812, loss: 0.492\n",
      "epoch: 500, acc: 0.844, loss: 0.320\n",
      "epoch: 500, acc: 0.781, loss: 0.506\n",
      "epoch: 500, acc: 0.781, loss: 0.465\n",
      "epoch: 500, acc: 0.750, loss: 0.579\n",
      "epoch: 500, acc: 0.781, loss: 0.446\n",
      "epoch: 500, acc: 0.875, loss: 0.286\n",
      "epoch: 500, acc: 0.688, loss: 0.675\n",
      "epoch: 500, acc: 0.844, loss: 0.363\n",
      "epoch: 500, acc: 0.875, loss: 0.388\n",
      "epoch: 500, acc: 0.905, loss: 0.307\n",
      "epoch: 600, acc: 0.656, loss: 0.645\n",
      "epoch: 600, acc: 0.719, loss: 0.615\n",
      "epoch: 600, acc: 0.969, loss: 0.226\n",
      "epoch: 600, acc: 0.812, loss: 0.346\n",
      "epoch: 600, acc: 0.844, loss: 0.421\n",
      "epoch: 600, acc: 0.688, loss: 0.513\n",
      "epoch: 600, acc: 0.875, loss: 0.360\n",
      "epoch: 600, acc: 0.875, loss: 0.334\n",
      "epoch: 600, acc: 0.750, loss: 0.514\n",
      "epoch: 600, acc: 0.906, loss: 0.281\n",
      "epoch: 600, acc: 0.875, loss: 0.357\n",
      "epoch: 600, acc: 0.719, loss: 0.575\n",
      "epoch: 600, acc: 0.781, loss: 0.517\n",
      "epoch: 600, acc: 0.781, loss: 0.592\n",
      "epoch: 600, acc: 0.906, loss: 0.340\n",
      "epoch: 600, acc: 0.875, loss: 0.267\n",
      "epoch: 600, acc: 0.812, loss: 0.407\n",
      "epoch: 600, acc: 0.844, loss: 0.435\n",
      "epoch: 600, acc: 0.875, loss: 0.306\n",
      "epoch: 600, acc: 0.750, loss: 0.565\n",
      "epoch: 600, acc: 0.844, loss: 0.314\n",
      "epoch: 600, acc: 0.781, loss: 0.403\n",
      "epoch: 600, acc: 0.938, loss: 0.240\n",
      "epoch: 600, acc: 0.812, loss: 0.399\n",
      "epoch: 600, acc: 0.750, loss: 0.527\n",
      "epoch: 600, acc: 0.844, loss: 0.365\n",
      "epoch: 600, acc: 0.781, loss: 0.676\n",
      "epoch: 600, acc: 0.812, loss: 0.494\n",
      "epoch: 600, acc: 0.812, loss: 0.424\n",
      "epoch: 600, acc: 0.844, loss: 0.377\n",
      "epoch: 600, acc: 0.938, loss: 0.182\n",
      "epoch: 600, acc: 0.750, loss: 0.595\n",
      "epoch: 600, acc: 0.857, loss: 0.317\n",
      "epoch: 700, acc: 0.750, loss: 0.506\n",
      "epoch: 700, acc: 0.875, loss: 0.416\n",
      "epoch: 700, acc: 0.938, loss: 0.242\n",
      "epoch: 700, acc: 0.719, loss: 0.523\n",
      "epoch: 700, acc: 0.906, loss: 0.266\n",
      "epoch: 700, acc: 0.781, loss: 0.449\n",
      "epoch: 700, acc: 0.750, loss: 0.624\n",
      "epoch: 700, acc: 0.812, loss: 0.502\n",
      "epoch: 700, acc: 0.844, loss: 0.384\n",
      "epoch: 700, acc: 0.844, loss: 0.369\n",
      "epoch: 700, acc: 0.844, loss: 0.411\n",
      "epoch: 700, acc: 0.844, loss: 0.429\n",
      "epoch: 700, acc: 0.750, loss: 0.394\n",
      "epoch: 700, acc: 0.781, loss: 0.445\n",
      "epoch: 700, acc: 0.844, loss: 0.403\n",
      "epoch: 700, acc: 0.781, loss: 0.393\n",
      "epoch: 700, acc: 0.875, loss: 0.280\n",
      "epoch: 700, acc: 0.875, loss: 0.327\n",
      "epoch: 700, acc: 0.844, loss: 0.413\n",
      "epoch: 700, acc: 0.844, loss: 0.619\n",
      "epoch: 700, acc: 0.812, loss: 0.487\n",
      "epoch: 700, acc: 0.844, loss: 0.366\n",
      "epoch: 700, acc: 0.844, loss: 0.309\n",
      "epoch: 700, acc: 0.812, loss: 0.443\n",
      "epoch: 700, acc: 0.844, loss: 0.365\n",
      "epoch: 700, acc: 0.812, loss: 0.389\n",
      "epoch: 700, acc: 0.906, loss: 0.311\n",
      "epoch: 700, acc: 0.844, loss: 0.474\n",
      "epoch: 700, acc: 0.812, loss: 0.466\n",
      "epoch: 700, acc: 0.719, loss: 0.402\n",
      "epoch: 700, acc: 0.656, loss: 0.587\n",
      "epoch: 700, acc: 0.781, loss: 0.489\n",
      "epoch: 700, acc: 0.762, loss: 0.476\n",
      "epoch: 800, acc: 0.906, loss: 0.310\n",
      "epoch: 800, acc: 0.844, loss: 0.477\n",
      "epoch: 800, acc: 0.938, loss: 0.253\n",
      "epoch: 800, acc: 0.750, loss: 0.557\n",
      "epoch: 800, acc: 0.781, loss: 0.429\n",
      "epoch: 800, acc: 0.844, loss: 0.386\n",
      "epoch: 800, acc: 0.781, loss: 0.405\n",
      "epoch: 800, acc: 0.844, loss: 0.403\n",
      "epoch: 800, acc: 0.812, loss: 0.378\n",
      "epoch: 800, acc: 0.875, loss: 0.440\n",
      "epoch: 800, acc: 0.781, loss: 0.371\n",
      "epoch: 800, acc: 0.781, loss: 0.450\n",
      "epoch: 800, acc: 0.781, loss: 0.414\n",
      "epoch: 800, acc: 0.812, loss: 0.444\n",
      "epoch: 800, acc: 0.781, loss: 0.387\n",
      "epoch: 800, acc: 0.906, loss: 0.291\n",
      "epoch: 800, acc: 0.906, loss: 0.381\n",
      "epoch: 800, acc: 0.781, loss: 0.428\n",
      "epoch: 800, acc: 0.688, loss: 0.537\n",
      "epoch: 800, acc: 0.844, loss: 0.429\n",
      "epoch: 800, acc: 0.812, loss: 0.398\n",
      "epoch: 800, acc: 0.906, loss: 0.308\n",
      "epoch: 800, acc: 0.750, loss: 0.522\n",
      "epoch: 800, acc: 0.750, loss: 0.639\n",
      "epoch: 800, acc: 0.781, loss: 0.510\n",
      "epoch: 800, acc: 0.781, loss: 0.584\n",
      "epoch: 800, acc: 0.844, loss: 0.431\n",
      "epoch: 800, acc: 0.750, loss: 0.517\n",
      "epoch: 800, acc: 0.875, loss: 0.334\n",
      "epoch: 800, acc: 0.812, loss: 0.386\n",
      "epoch: 800, acc: 0.906, loss: 0.341\n",
      "epoch: 800, acc: 0.781, loss: 0.398\n",
      "epoch: 800, acc: 0.905, loss: 0.337\n",
      "epoch: 900, acc: 0.844, loss: 0.424\n",
      "epoch: 900, acc: 0.938, loss: 0.275\n",
      "epoch: 900, acc: 0.812, loss: 0.431\n",
      "epoch: 900, acc: 0.719, loss: 0.578\n",
      "epoch: 900, acc: 0.812, loss: 0.369\n",
      "epoch: 900, acc: 0.812, loss: 0.440\n",
      "epoch: 900, acc: 0.812, loss: 0.499\n",
      "epoch: 900, acc: 0.781, loss: 0.431\n",
      "epoch: 900, acc: 0.938, loss: 0.282\n",
      "epoch: 900, acc: 0.812, loss: 0.358\n",
      "epoch: 900, acc: 0.938, loss: 0.410\n",
      "epoch: 900, acc: 0.812, loss: 0.439\n",
      "epoch: 900, acc: 0.781, loss: 0.374\n",
      "epoch: 900, acc: 0.875, loss: 0.381\n",
      "epoch: 900, acc: 0.875, loss: 0.313\n",
      "epoch: 900, acc: 0.906, loss: 0.325\n",
      "epoch: 900, acc: 0.844, loss: 0.378\n",
      "epoch: 900, acc: 0.781, loss: 0.522\n",
      "epoch: 900, acc: 0.844, loss: 0.405\n",
      "epoch: 900, acc: 0.781, loss: 0.455\n",
      "epoch: 900, acc: 0.844, loss: 0.348\n",
      "epoch: 900, acc: 0.844, loss: 0.373\n",
      "epoch: 900, acc: 0.656, loss: 0.642\n",
      "epoch: 900, acc: 0.781, loss: 0.433\n",
      "epoch: 900, acc: 0.781, loss: 0.464\n",
      "epoch: 900, acc: 0.750, loss: 0.469\n",
      "epoch: 900, acc: 0.812, loss: 0.373\n",
      "epoch: 900, acc: 0.812, loss: 0.349\n",
      "epoch: 900, acc: 0.812, loss: 0.381\n",
      "epoch: 900, acc: 0.875, loss: 0.341\n",
      "epoch: 900, acc: 0.750, loss: 0.612\n",
      "epoch: 900, acc: 0.812, loss: 0.521\n",
      "epoch: 900, acc: 0.762, loss: 0.536\n",
      "epoch: 1000, acc: 0.844, loss: 0.338\n",
      "epoch: 1000, acc: 0.812, loss: 0.393\n",
      "epoch: 1000, acc: 0.812, loss: 0.414\n",
      "epoch: 1000, acc: 0.906, loss: 0.288\n",
      "epoch: 1000, acc: 0.875, loss: 0.375\n",
      "epoch: 1000, acc: 0.875, loss: 0.345\n",
      "epoch: 1000, acc: 0.719, loss: 0.609\n",
      "epoch: 1000, acc: 0.875, loss: 0.255\n",
      "epoch: 1000, acc: 0.750, loss: 0.386\n",
      "epoch: 1000, acc: 0.781, loss: 0.449\n",
      "epoch: 1000, acc: 0.656, loss: 0.522\n",
      "epoch: 1000, acc: 0.812, loss: 0.382\n",
      "epoch: 1000, acc: 0.750, loss: 0.554\n",
      "epoch: 1000, acc: 0.625, loss: 0.692\n",
      "epoch: 1000, acc: 0.812, loss: 0.426\n",
      "epoch: 1000, acc: 0.812, loss: 0.477\n",
      "epoch: 1000, acc: 0.719, loss: 0.585\n",
      "epoch: 1000, acc: 0.875, loss: 0.323\n",
      "epoch: 1000, acc: 0.969, loss: 0.265\n",
      "epoch: 1000, acc: 0.812, loss: 0.511\n",
      "epoch: 1000, acc: 0.906, loss: 0.272\n",
      "epoch: 1000, acc: 0.844, loss: 0.378\n",
      "epoch: 1000, acc: 0.906, loss: 0.333\n",
      "epoch: 1000, acc: 0.719, loss: 0.599\n",
      "epoch: 1000, acc: 0.906, loss: 0.322\n",
      "epoch: 1000, acc: 0.906, loss: 0.271\n",
      "epoch: 1000, acc: 0.844, loss: 0.341\n",
      "epoch: 1000, acc: 0.781, loss: 0.578\n",
      "epoch: 1000, acc: 0.656, loss: 0.734\n",
      "epoch: 1000, acc: 0.844, loss: 0.348\n",
      "epoch: 1000, acc: 0.781, loss: 0.414\n",
      "epoch: 1000, acc: 0.812, loss: 0.442\n",
      "epoch: 1000, acc: 1.000, loss: 0.209\n",
      "epoch: 1100, acc: 0.906, loss: 0.292\n",
      "epoch: 1100, acc: 0.812, loss: 0.375\n",
      "epoch: 1100, acc: 0.844, loss: 0.472\n",
      "epoch: 1100, acc: 0.688, loss: 0.545\n",
      "epoch: 1100, acc: 0.781, loss: 0.415\n",
      "epoch: 1100, acc: 0.875, loss: 0.322\n",
      "epoch: 1100, acc: 0.812, loss: 0.413\n",
      "epoch: 1100, acc: 0.844, loss: 0.379\n",
      "epoch: 1100, acc: 0.812, loss: 0.435\n",
      "epoch: 1100, acc: 0.781, loss: 0.537\n",
      "epoch: 1100, acc: 0.875, loss: 0.364\n",
      "epoch: 1100, acc: 0.812, loss: 0.399\n",
      "epoch: 1100, acc: 0.781, loss: 0.438\n",
      "epoch: 1100, acc: 0.906, loss: 0.365\n",
      "epoch: 1100, acc: 0.688, loss: 0.641\n",
      "epoch: 1100, acc: 0.844, loss: 0.453\n",
      "epoch: 1100, acc: 0.812, loss: 0.445\n",
      "epoch: 1100, acc: 0.781, loss: 0.461\n",
      "epoch: 1100, acc: 0.812, loss: 0.330\n",
      "epoch: 1100, acc: 0.906, loss: 0.260\n",
      "epoch: 1100, acc: 0.812, loss: 0.411\n",
      "epoch: 1100, acc: 0.844, loss: 0.326\n",
      "epoch: 1100, acc: 0.812, loss: 0.336\n",
      "epoch: 1100, acc: 0.875, loss: 0.357\n",
      "epoch: 1100, acc: 0.812, loss: 0.489\n",
      "epoch: 1100, acc: 0.875, loss: 0.352\n",
      "epoch: 1100, acc: 0.781, loss: 0.443\n",
      "epoch: 1100, acc: 0.688, loss: 0.744\n",
      "epoch: 1100, acc: 0.875, loss: 0.328\n",
      "epoch: 1100, acc: 0.750, loss: 0.502\n",
      "epoch: 1100, acc: 0.844, loss: 0.451\n",
      "epoch: 1100, acc: 0.812, loss: 0.444\n",
      "epoch: 1100, acc: 0.857, loss: 0.362\n",
      "epoch: 1200, acc: 0.875, loss: 0.457\n",
      "epoch: 1200, acc: 0.812, loss: 0.448\n",
      "epoch: 1200, acc: 0.844, loss: 0.369\n",
      "epoch: 1200, acc: 0.906, loss: 0.279\n",
      "epoch: 1200, acc: 0.781, loss: 0.525\n",
      "epoch: 1200, acc: 0.875, loss: 0.487\n",
      "epoch: 1200, acc: 0.750, loss: 0.455\n",
      "epoch: 1200, acc: 0.844, loss: 0.351\n",
      "epoch: 1200, acc: 0.844, loss: 0.529\n",
      "epoch: 1200, acc: 0.875, loss: 0.403\n",
      "epoch: 1200, acc: 0.750, loss: 0.468\n",
      "epoch: 1200, acc: 0.875, loss: 0.291\n",
      "epoch: 1200, acc: 0.812, loss: 0.450\n",
      "epoch: 1200, acc: 0.875, loss: 0.336\n",
      "epoch: 1200, acc: 0.719, loss: 0.523\n",
      "epoch: 1200, acc: 0.688, loss: 0.545\n",
      "epoch: 1200, acc: 0.875, loss: 0.321\n",
      "epoch: 1200, acc: 0.875, loss: 0.369\n",
      "epoch: 1200, acc: 0.906, loss: 0.284\n",
      "epoch: 1200, acc: 0.812, loss: 0.397\n",
      "epoch: 1200, acc: 0.750, loss: 0.500\n",
      "epoch: 1200, acc: 0.875, loss: 0.390\n",
      "epoch: 1200, acc: 0.844, loss: 0.410\n",
      "epoch: 1200, acc: 0.812, loss: 0.348\n",
      "epoch: 1200, acc: 0.812, loss: 0.309\n",
      "epoch: 1200, acc: 0.719, loss: 0.490\n",
      "epoch: 1200, acc: 0.781, loss: 0.429\n",
      "epoch: 1200, acc: 0.812, loss: 0.442\n",
      "epoch: 1200, acc: 0.875, loss: 0.325\n",
      "epoch: 1200, acc: 0.844, loss: 0.407\n",
      "epoch: 1200, acc: 0.719, loss: 0.490\n",
      "epoch: 1200, acc: 0.812, loss: 0.557\n",
      "epoch: 1200, acc: 0.857, loss: 0.495\n",
      "epoch: 1300, acc: 0.844, loss: 0.404\n",
      "epoch: 1300, acc: 0.844, loss: 0.458\n",
      "epoch: 1300, acc: 0.781, loss: 0.542\n",
      "epoch: 1300, acc: 0.875, loss: 0.300\n",
      "epoch: 1300, acc: 0.844, loss: 0.323\n",
      "epoch: 1300, acc: 0.750, loss: 0.459\n",
      "epoch: 1300, acc: 0.750, loss: 0.614\n",
      "epoch: 1300, acc: 0.781, loss: 0.386\n",
      "epoch: 1300, acc: 0.781, loss: 0.463\n",
      "epoch: 1300, acc: 0.812, loss: 0.417\n",
      "epoch: 1300, acc: 0.844, loss: 0.310\n",
      "epoch: 1300, acc: 0.750, loss: 0.547\n",
      "epoch: 1300, acc: 0.875, loss: 0.384\n",
      "epoch: 1300, acc: 0.750, loss: 0.505\n",
      "epoch: 1300, acc: 0.781, loss: 0.565\n",
      "epoch: 1300, acc: 0.875, loss: 0.332\n",
      "epoch: 1300, acc: 0.812, loss: 0.375\n",
      "epoch: 1300, acc: 0.875, loss: 0.373\n",
      "epoch: 1300, acc: 0.844, loss: 0.343\n",
      "epoch: 1300, acc: 0.781, loss: 0.531\n",
      "epoch: 1300, acc: 0.875, loss: 0.379\n",
      "epoch: 1300, acc: 0.938, loss: 0.272\n",
      "epoch: 1300, acc: 0.812, loss: 0.461\n",
      "epoch: 1300, acc: 0.781, loss: 0.460\n",
      "epoch: 1300, acc: 0.688, loss: 0.545\n",
      "epoch: 1300, acc: 0.844, loss: 0.300\n",
      "epoch: 1300, acc: 0.719, loss: 0.625\n",
      "epoch: 1300, acc: 0.938, loss: 0.294\n",
      "epoch: 1300, acc: 0.812, loss: 0.411\n",
      "epoch: 1300, acc: 0.781, loss: 0.421\n",
      "epoch: 1300, acc: 0.875, loss: 0.394\n",
      "epoch: 1300, acc: 0.844, loss: 0.368\n",
      "epoch: 1300, acc: 0.952, loss: 0.229\n",
      "epoch: 1400, acc: 0.906, loss: 0.323\n",
      "epoch: 1400, acc: 0.750, loss: 0.578\n",
      "epoch: 1400, acc: 0.781, loss: 0.425\n",
      "epoch: 1400, acc: 0.812, loss: 0.463\n",
      "epoch: 1400, acc: 0.719, loss: 0.575\n",
      "epoch: 1400, acc: 0.938, loss: 0.225\n",
      "epoch: 1400, acc: 0.906, loss: 0.247\n",
      "epoch: 1400, acc: 0.812, loss: 0.388\n",
      "epoch: 1400, acc: 0.812, loss: 0.413\n",
      "epoch: 1400, acc: 0.719, loss: 0.544\n",
      "epoch: 1400, acc: 0.781, loss: 0.504\n",
      "epoch: 1400, acc: 0.812, loss: 0.393\n",
      "epoch: 1400, acc: 0.781, loss: 0.417\n",
      "epoch: 1400, acc: 0.844, loss: 0.446\n",
      "epoch: 1400, acc: 0.750, loss: 0.590\n",
      "epoch: 1400, acc: 0.875, loss: 0.372\n",
      "epoch: 1400, acc: 0.844, loss: 0.352\n",
      "epoch: 1400, acc: 0.969, loss: 0.255\n",
      "epoch: 1400, acc: 0.750, loss: 0.357\n",
      "epoch: 1400, acc: 0.844, loss: 0.542\n",
      "epoch: 1400, acc: 0.844, loss: 0.415\n",
      "epoch: 1400, acc: 0.875, loss: 0.393\n",
      "epoch: 1400, acc: 0.781, loss: 0.566\n",
      "epoch: 1400, acc: 0.781, loss: 0.334\n",
      "epoch: 1400, acc: 0.719, loss: 0.610\n",
      "epoch: 1400, acc: 0.750, loss: 0.445\n",
      "epoch: 1400, acc: 0.844, loss: 0.364\n",
      "epoch: 1400, acc: 0.812, loss: 0.351\n",
      "epoch: 1400, acc: 0.812, loss: 0.531\n",
      "epoch: 1400, acc: 0.844, loss: 0.337\n",
      "epoch: 1400, acc: 0.875, loss: 0.372\n",
      "epoch: 1400, acc: 0.844, loss: 0.358\n",
      "epoch: 1400, acc: 0.905, loss: 0.357\n",
      "epoch: 1500, acc: 0.938, loss: 0.282\n",
      "epoch: 1500, acc: 0.812, loss: 0.379\n",
      "epoch: 1500, acc: 0.875, loss: 0.290\n",
      "epoch: 1500, acc: 0.844, loss: 0.405\n",
      "epoch: 1500, acc: 0.844, loss: 0.372\n",
      "epoch: 1500, acc: 0.875, loss: 0.256\n",
      "epoch: 1500, acc: 0.750, loss: 0.544\n",
      "epoch: 1500, acc: 0.781, loss: 0.415\n",
      "epoch: 1500, acc: 0.844, loss: 0.363\n",
      "epoch: 1500, acc: 0.812, loss: 0.330\n",
      "epoch: 1500, acc: 0.719, loss: 0.559\n",
      "epoch: 1500, acc: 0.719, loss: 0.544\n",
      "epoch: 1500, acc: 0.875, loss: 0.361\n",
      "epoch: 1500, acc: 0.844, loss: 0.393\n",
      "epoch: 1500, acc: 0.781, loss: 0.400\n",
      "epoch: 1500, acc: 0.844, loss: 0.362\n",
      "epoch: 1500, acc: 0.844, loss: 0.331\n",
      "epoch: 1500, acc: 0.781, loss: 0.444\n",
      "epoch: 1500, acc: 0.750, loss: 0.531\n",
      "epoch: 1500, acc: 0.844, loss: 0.347\n",
      "epoch: 1500, acc: 0.844, loss: 0.403\n",
      "epoch: 1500, acc: 0.844, loss: 0.388\n",
      "epoch: 1500, acc: 0.844, loss: 0.451\n",
      "epoch: 1500, acc: 0.781, loss: 0.374\n",
      "epoch: 1500, acc: 0.719, loss: 0.609\n",
      "epoch: 1500, acc: 0.719, loss: 0.463\n",
      "epoch: 1500, acc: 0.844, loss: 0.573\n",
      "epoch: 1500, acc: 0.688, loss: 0.713\n",
      "epoch: 1500, acc: 0.906, loss: 0.316\n",
      "epoch: 1500, acc: 0.844, loss: 0.347\n",
      "epoch: 1500, acc: 0.844, loss: 0.478\n",
      "epoch: 1500, acc: 0.781, loss: 0.467\n",
      "epoch: 1500, acc: 0.905, loss: 0.366\n",
      "epoch: 1600, acc: 0.906, loss: 0.341\n",
      "epoch: 1600, acc: 0.875, loss: 0.295\n",
      "epoch: 1600, acc: 0.781, loss: 0.463\n",
      "epoch: 1600, acc: 0.875, loss: 0.359\n",
      "epoch: 1600, acc: 0.625, loss: 0.694\n",
      "epoch: 1600, acc: 0.781, loss: 0.608\n",
      "epoch: 1600, acc: 0.812, loss: 0.396\n",
      "epoch: 1600, acc: 0.844, loss: 0.341\n",
      "epoch: 1600, acc: 0.781, loss: 0.600\n",
      "epoch: 1600, acc: 0.875, loss: 0.417\n",
      "epoch: 1600, acc: 0.750, loss: 0.476\n",
      "epoch: 1600, acc: 0.781, loss: 0.538\n",
      "epoch: 1600, acc: 0.750, loss: 0.508\n",
      "epoch: 1600, acc: 0.875, loss: 0.311\n",
      "epoch: 1600, acc: 0.750, loss: 0.440\n",
      "epoch: 1600, acc: 0.875, loss: 0.328\n",
      "epoch: 1600, acc: 0.938, loss: 0.347\n",
      "epoch: 1600, acc: 0.844, loss: 0.427\n",
      "epoch: 1600, acc: 0.875, loss: 0.359\n",
      "epoch: 1600, acc: 0.812, loss: 0.419\n",
      "epoch: 1600, acc: 0.719, loss: 0.580\n",
      "epoch: 1600, acc: 0.781, loss: 0.397\n",
      "epoch: 1600, acc: 0.906, loss: 0.259\n",
      "epoch: 1600, acc: 0.750, loss: 0.525\n",
      "epoch: 1600, acc: 0.812, loss: 0.377\n",
      "epoch: 1600, acc: 0.906, loss: 0.222\n",
      "epoch: 1600, acc: 0.875, loss: 0.379\n",
      "epoch: 1600, acc: 0.750, loss: 0.523\n",
      "epoch: 1600, acc: 0.844, loss: 0.394\n",
      "epoch: 1600, acc: 0.812, loss: 0.445\n",
      "epoch: 1600, acc: 0.875, loss: 0.304\n",
      "epoch: 1600, acc: 0.812, loss: 0.329\n",
      "epoch: 1600, acc: 0.714, loss: 0.473\n",
      "epoch: 1700, acc: 0.719, loss: 0.552\n",
      "epoch: 1700, acc: 0.906, loss: 0.376\n",
      "epoch: 1700, acc: 0.938, loss: 0.262\n",
      "epoch: 1700, acc: 0.844, loss: 0.378\n",
      "epoch: 1700, acc: 0.906, loss: 0.309\n",
      "epoch: 1700, acc: 0.844, loss: 0.377\n",
      "epoch: 1700, acc: 0.844, loss: 0.379\n",
      "epoch: 1700, acc: 0.844, loss: 0.407\n",
      "epoch: 1700, acc: 0.844, loss: 0.341\n",
      "epoch: 1700, acc: 0.781, loss: 0.375\n",
      "epoch: 1700, acc: 0.812, loss: 0.453\n",
      "epoch: 1700, acc: 0.844, loss: 0.409\n",
      "epoch: 1700, acc: 0.906, loss: 0.334\n",
      "epoch: 1700, acc: 0.812, loss: 0.411\n",
      "epoch: 1700, acc: 0.875, loss: 0.411\n",
      "epoch: 1700, acc: 0.844, loss: 0.463\n",
      "epoch: 1700, acc: 0.844, loss: 0.339\n",
      "epoch: 1700, acc: 0.625, loss: 0.684\n",
      "epoch: 1700, acc: 0.719, loss: 0.533\n",
      "epoch: 1700, acc: 0.750, loss: 0.556\n",
      "epoch: 1700, acc: 0.781, loss: 0.510\n",
      "epoch: 1700, acc: 0.719, loss: 0.505\n",
      "epoch: 1700, acc: 0.875, loss: 0.291\n",
      "epoch: 1700, acc: 0.812, loss: 0.502\n",
      "epoch: 1700, acc: 0.906, loss: 0.292\n",
      "epoch: 1700, acc: 0.875, loss: 0.312\n",
      "epoch: 1700, acc: 0.750, loss: 0.463\n",
      "epoch: 1700, acc: 0.781, loss: 0.403\n",
      "epoch: 1700, acc: 0.875, loss: 0.435\n",
      "epoch: 1700, acc: 0.812, loss: 0.376\n",
      "epoch: 1700, acc: 0.844, loss: 0.400\n",
      "epoch: 1700, acc: 0.781, loss: 0.438\n",
      "epoch: 1700, acc: 0.667, loss: 0.647\n",
      "epoch: 1800, acc: 0.812, loss: 0.399\n",
      "epoch: 1800, acc: 0.875, loss: 0.314\n",
      "epoch: 1800, acc: 0.781, loss: 0.652\n",
      "epoch: 1800, acc: 0.844, loss: 0.405\n",
      "epoch: 1800, acc: 0.906, loss: 0.262\n",
      "epoch: 1800, acc: 0.781, loss: 0.463\n",
      "epoch: 1800, acc: 0.844, loss: 0.375\n",
      "epoch: 1800, acc: 0.812, loss: 0.426\n",
      "epoch: 1800, acc: 0.875, loss: 0.287\n",
      "epoch: 1800, acc: 0.844, loss: 0.460\n",
      "epoch: 1800, acc: 0.750, loss: 0.443\n",
      "epoch: 1800, acc: 0.781, loss: 0.396\n",
      "epoch: 1800, acc: 0.750, loss: 0.476\n",
      "epoch: 1800, acc: 0.906, loss: 0.344\n",
      "epoch: 1800, acc: 0.875, loss: 0.400\n",
      "epoch: 1800, acc: 0.844, loss: 0.417\n",
      "epoch: 1800, acc: 0.656, loss: 0.570\n",
      "epoch: 1800, acc: 0.812, loss: 0.329\n",
      "epoch: 1800, acc: 0.750, loss: 0.426\n",
      "epoch: 1800, acc: 0.906, loss: 0.309\n",
      "epoch: 1800, acc: 0.844, loss: 0.568\n",
      "epoch: 1800, acc: 0.875, loss: 0.263\n",
      "epoch: 1800, acc: 0.875, loss: 0.399\n",
      "epoch: 1800, acc: 0.875, loss: 0.339\n",
      "epoch: 1800, acc: 0.719, loss: 0.620\n",
      "epoch: 1800, acc: 0.781, loss: 0.510\n",
      "epoch: 1800, acc: 0.750, loss: 0.402\n",
      "epoch: 1800, acc: 0.844, loss: 0.398\n",
      "epoch: 1800, acc: 0.844, loss: 0.370\n",
      "epoch: 1800, acc: 0.781, loss: 0.465\n",
      "epoch: 1800, acc: 0.812, loss: 0.372\n",
      "epoch: 1800, acc: 0.844, loss: 0.497\n",
      "epoch: 1800, acc: 0.810, loss: 0.525\n",
      "epoch: 1900, acc: 0.906, loss: 0.318\n",
      "epoch: 1900, acc: 0.906, loss: 0.350\n",
      "epoch: 1900, acc: 0.812, loss: 0.364\n",
      "epoch: 1900, acc: 0.906, loss: 0.238\n",
      "epoch: 1900, acc: 0.781, loss: 0.410\n",
      "epoch: 1900, acc: 0.719, loss: 0.510\n",
      "epoch: 1900, acc: 0.750, loss: 0.547\n",
      "epoch: 1900, acc: 0.781, loss: 0.453\n",
      "epoch: 1900, acc: 0.875, loss: 0.417\n",
      "epoch: 1900, acc: 0.875, loss: 0.235\n",
      "epoch: 1900, acc: 0.844, loss: 0.388\n",
      "epoch: 1900, acc: 0.969, loss: 0.191\n",
      "epoch: 1900, acc: 0.750, loss: 0.591\n",
      "epoch: 1900, acc: 0.844, loss: 0.420\n",
      "epoch: 1900, acc: 0.875, loss: 0.378\n",
      "epoch: 1900, acc: 0.812, loss: 0.331\n",
      "epoch: 1900, acc: 0.812, loss: 0.438\n",
      "epoch: 1900, acc: 0.719, loss: 0.476\n",
      "epoch: 1900, acc: 0.750, loss: 0.465\n",
      "epoch: 1900, acc: 0.844, loss: 0.370\n",
      "epoch: 1900, acc: 0.812, loss: 0.412\n",
      "epoch: 1900, acc: 0.906, loss: 0.341\n",
      "epoch: 1900, acc: 0.875, loss: 0.441\n",
      "epoch: 1900, acc: 0.781, loss: 0.410\n",
      "epoch: 1900, acc: 0.750, loss: 0.438\n",
      "epoch: 1900, acc: 0.781, loss: 0.478\n",
      "epoch: 1900, acc: 0.719, loss: 0.678\n",
      "epoch: 1900, acc: 0.781, loss: 0.371\n",
      "epoch: 1900, acc: 0.781, loss: 0.555\n",
      "epoch: 1900, acc: 0.875, loss: 0.291\n",
      "epoch: 1900, acc: 0.781, loss: 0.596\n",
      "epoch: 1900, acc: 0.812, loss: 0.442\n",
      "epoch: 1900, acc: 0.714, loss: 0.528\n",
      "epoch: 2000, acc: 0.938, loss: 0.223\n",
      "epoch: 2000, acc: 0.906, loss: 0.253\n",
      "epoch: 2000, acc: 0.844, loss: 0.342\n",
      "epoch: 2000, acc: 0.781, loss: 0.425\n",
      "epoch: 2000, acc: 0.781, loss: 0.621\n",
      "epoch: 2000, acc: 0.750, loss: 0.395\n",
      "epoch: 2000, acc: 0.938, loss: 0.292\n",
      "epoch: 2000, acc: 0.812, loss: 0.569\n",
      "epoch: 2000, acc: 0.781, loss: 0.561\n",
      "epoch: 2000, acc: 0.719, loss: 0.539\n",
      "epoch: 2000, acc: 0.875, loss: 0.289\n",
      "epoch: 2000, acc: 0.844, loss: 0.449\n",
      "epoch: 2000, acc: 0.906, loss: 0.336\n",
      "epoch: 2000, acc: 0.812, loss: 0.483\n",
      "epoch: 2000, acc: 0.781, loss: 0.482\n",
      "epoch: 2000, acc: 0.688, loss: 0.580\n",
      "epoch: 2000, acc: 0.781, loss: 0.448\n",
      "epoch: 2000, acc: 0.750, loss: 0.590\n",
      "epoch: 2000, acc: 0.875, loss: 0.359\n",
      "epoch: 2000, acc: 0.875, loss: 0.310\n",
      "epoch: 2000, acc: 0.719, loss: 0.521\n",
      "epoch: 2000, acc: 0.906, loss: 0.279\n",
      "epoch: 2000, acc: 0.938, loss: 0.245\n",
      "epoch: 2000, acc: 0.750, loss: 0.647\n",
      "epoch: 2000, acc: 0.688, loss: 0.531\n",
      "epoch: 2000, acc: 0.844, loss: 0.274\n",
      "epoch: 2000, acc: 0.750, loss: 0.428\n",
      "epoch: 2000, acc: 0.750, loss: 0.616\n",
      "epoch: 2000, acc: 0.875, loss: 0.283\n",
      "epoch: 2000, acc: 0.938, loss: 0.233\n",
      "epoch: 2000, acc: 0.812, loss: 0.383\n",
      "epoch: 2000, acc: 0.812, loss: 0.441\n",
      "epoch: 2000, acc: 0.810, loss: 0.446\n",
      "epoch: 2100, acc: 0.812, loss: 0.461\n",
      "epoch: 2100, acc: 0.844, loss: 0.328\n",
      "epoch: 2100, acc: 0.812, loss: 0.330\n",
      "epoch: 2100, acc: 0.875, loss: 0.428\n",
      "epoch: 2100, acc: 0.844, loss: 0.506\n",
      "epoch: 2100, acc: 0.812, loss: 0.355\n",
      "epoch: 2100, acc: 0.906, loss: 0.326\n",
      "epoch: 2100, acc: 0.750, loss: 0.437\n",
      "epoch: 2100, acc: 0.781, loss: 0.440\n",
      "epoch: 2100, acc: 0.750, loss: 0.552\n",
      "epoch: 2100, acc: 0.844, loss: 0.341\n",
      "epoch: 2100, acc: 0.906, loss: 0.337\n",
      "epoch: 2100, acc: 0.875, loss: 0.314\n",
      "epoch: 2100, acc: 0.781, loss: 0.470\n",
      "epoch: 2100, acc: 0.812, loss: 0.351\n",
      "epoch: 2100, acc: 0.812, loss: 0.455\n",
      "epoch: 2100, acc: 0.906, loss: 0.416\n",
      "epoch: 2100, acc: 0.781, loss: 0.532\n",
      "epoch: 2100, acc: 0.781, loss: 0.492\n",
      "epoch: 2100, acc: 0.750, loss: 0.500\n",
      "epoch: 2100, acc: 0.844, loss: 0.389\n",
      "epoch: 2100, acc: 0.719, loss: 0.582\n",
      "epoch: 2100, acc: 0.844, loss: 0.505\n",
      "epoch: 2100, acc: 0.875, loss: 0.289\n",
      "epoch: 2100, acc: 0.875, loss: 0.328\n",
      "epoch: 2100, acc: 0.844, loss: 0.329\n",
      "epoch: 2100, acc: 0.812, loss: 0.446\n",
      "epoch: 2100, acc: 0.719, loss: 0.509\n",
      "epoch: 2100, acc: 0.750, loss: 0.598\n",
      "epoch: 2100, acc: 0.781, loss: 0.384\n",
      "epoch: 2100, acc: 0.812, loss: 0.338\n",
      "epoch: 2100, acc: 0.812, loss: 0.421\n",
      "epoch: 2100, acc: 0.857, loss: 0.296\n",
      "epoch: 2200, acc: 0.875, loss: 0.384\n",
      "epoch: 2200, acc: 0.844, loss: 0.352\n",
      "epoch: 2200, acc: 0.781, loss: 0.635\n",
      "epoch: 2200, acc: 0.844, loss: 0.448\n",
      "epoch: 2200, acc: 0.844, loss: 0.371\n",
      "epoch: 2200, acc: 0.781, loss: 0.497\n",
      "epoch: 2200, acc: 0.844, loss: 0.362\n",
      "epoch: 2200, acc: 0.875, loss: 0.312\n",
      "epoch: 2200, acc: 0.844, loss: 0.358\n",
      "epoch: 2200, acc: 0.781, loss: 0.380\n",
      "epoch: 2200, acc: 0.875, loss: 0.341\n",
      "epoch: 2200, acc: 0.938, loss: 0.246\n",
      "epoch: 2200, acc: 0.719, loss: 0.450\n",
      "epoch: 2200, acc: 0.781, loss: 0.465\n",
      "epoch: 2200, acc: 0.875, loss: 0.414\n",
      "epoch: 2200, acc: 0.812, loss: 0.464\n",
      "epoch: 2200, acc: 0.812, loss: 0.462\n",
      "epoch: 2200, acc: 0.844, loss: 0.441\n",
      "epoch: 2200, acc: 0.625, loss: 0.629\n",
      "epoch: 2200, acc: 0.844, loss: 0.464\n",
      "epoch: 2200, acc: 0.906, loss: 0.275\n",
      "epoch: 2200, acc: 0.812, loss: 0.365\n",
      "epoch: 2200, acc: 0.906, loss: 0.429\n",
      "epoch: 2200, acc: 0.969, loss: 0.214\n",
      "epoch: 2200, acc: 0.844, loss: 0.442\n",
      "epoch: 2200, acc: 0.719, loss: 0.508\n",
      "epoch: 2200, acc: 0.656, loss: 0.630\n",
      "epoch: 2200, acc: 0.781, loss: 0.407\n",
      "epoch: 2200, acc: 0.844, loss: 0.424\n",
      "epoch: 2200, acc: 0.906, loss: 0.283\n",
      "epoch: 2200, acc: 0.688, loss: 0.548\n",
      "epoch: 2200, acc: 0.906, loss: 0.284\n",
      "epoch: 2200, acc: 0.762, loss: 0.614\n",
      "epoch: 2300, acc: 0.812, loss: 0.413\n",
      "epoch: 2300, acc: 0.750, loss: 0.495\n",
      "epoch: 2300, acc: 0.750, loss: 0.570\n",
      "epoch: 2300, acc: 0.688, loss: 0.560\n",
      "epoch: 2300, acc: 0.938, loss: 0.283\n",
      "epoch: 2300, acc: 0.750, loss: 0.396\n",
      "epoch: 2300, acc: 0.750, loss: 0.431\n",
      "epoch: 2300, acc: 0.812, loss: 0.343\n",
      "epoch: 2300, acc: 0.812, loss: 0.486\n",
      "epoch: 2300, acc: 0.906, loss: 0.349\n",
      "epoch: 2300, acc: 0.812, loss: 0.496\n",
      "epoch: 2300, acc: 0.938, loss: 0.317\n",
      "epoch: 2300, acc: 0.844, loss: 0.289\n",
      "epoch: 2300, acc: 0.844, loss: 0.457\n",
      "epoch: 2300, acc: 0.844, loss: 0.404\n",
      "epoch: 2300, acc: 0.875, loss: 0.370\n",
      "epoch: 2300, acc: 0.906, loss: 0.334\n",
      "epoch: 2300, acc: 0.750, loss: 0.417\n",
      "epoch: 2300, acc: 0.812, loss: 0.400\n",
      "epoch: 2300, acc: 0.781, loss: 0.514\n",
      "epoch: 2300, acc: 0.656, loss: 0.546\n",
      "epoch: 2300, acc: 0.875, loss: 0.335\n",
      "epoch: 2300, acc: 0.781, loss: 0.470\n",
      "epoch: 2300, acc: 0.812, loss: 0.376\n",
      "epoch: 2300, acc: 0.750, loss: 0.493\n",
      "epoch: 2300, acc: 0.812, loss: 0.447\n",
      "epoch: 2300, acc: 0.812, loss: 0.475\n",
      "epoch: 2300, acc: 0.875, loss: 0.362\n",
      "epoch: 2300, acc: 0.812, loss: 0.523\n",
      "epoch: 2300, acc: 0.844, loss: 0.395\n",
      "epoch: 2300, acc: 0.812, loss: 0.385\n",
      "epoch: 2300, acc: 0.906, loss: 0.350\n",
      "epoch: 2300, acc: 0.857, loss: 0.372\n",
      "epoch: 2400, acc: 0.875, loss: 0.337\n",
      "epoch: 2400, acc: 0.688, loss: 0.615\n",
      "epoch: 2400, acc: 0.812, loss: 0.431\n",
      "epoch: 2400, acc: 0.906, loss: 0.300\n",
      "epoch: 2400, acc: 0.969, loss: 0.172\n",
      "epoch: 2400, acc: 0.812, loss: 0.469\n",
      "epoch: 2400, acc: 0.781, loss: 0.529\n",
      "epoch: 2400, acc: 0.719, loss: 0.571\n",
      "epoch: 2400, acc: 0.781, loss: 0.352\n",
      "epoch: 2400, acc: 0.750, loss: 0.443\n",
      "epoch: 2400, acc: 0.812, loss: 0.469\n",
      "epoch: 2400, acc: 0.750, loss: 0.485\n",
      "epoch: 2400, acc: 0.781, loss: 0.440\n",
      "epoch: 2400, acc: 0.812, loss: 0.484\n",
      "epoch: 2400, acc: 0.906, loss: 0.308\n",
      "epoch: 2400, acc: 0.875, loss: 0.286\n",
      "epoch: 2400, acc: 0.719, loss: 0.504\n",
      "epoch: 2400, acc: 0.906, loss: 0.270\n",
      "epoch: 2400, acc: 0.844, loss: 0.337\n",
      "epoch: 2400, acc: 0.750, loss: 0.489\n",
      "epoch: 2400, acc: 0.812, loss: 0.404\n",
      "epoch: 2400, acc: 0.812, loss: 0.397\n",
      "epoch: 2400, acc: 0.812, loss: 0.543\n",
      "epoch: 2400, acc: 0.906, loss: 0.280\n",
      "epoch: 2400, acc: 0.812, loss: 0.421\n",
      "epoch: 2400, acc: 0.812, loss: 0.451\n",
      "epoch: 2400, acc: 0.938, loss: 0.291\n",
      "epoch: 2400, acc: 0.906, loss: 0.231\n",
      "epoch: 2400, acc: 0.750, loss: 0.623\n",
      "epoch: 2400, acc: 0.844, loss: 0.445\n",
      "epoch: 2400, acc: 0.844, loss: 0.340\n",
      "epoch: 2400, acc: 0.750, loss: 0.526\n",
      "epoch: 2400, acc: 0.714, loss: 0.631\n",
      "epoch: 2500, acc: 0.844, loss: 0.385\n",
      "epoch: 2500, acc: 0.875, loss: 0.468\n",
      "epoch: 2500, acc: 0.750, loss: 0.665\n",
      "epoch: 2500, acc: 0.812, loss: 0.387\n",
      "epoch: 2500, acc: 0.844, loss: 0.324\n",
      "epoch: 2500, acc: 0.906, loss: 0.375\n",
      "epoch: 2500, acc: 0.938, loss: 0.211\n",
      "epoch: 2500, acc: 0.875, loss: 0.402\n",
      "epoch: 2500, acc: 0.750, loss: 0.403\n",
      "epoch: 2500, acc: 0.938, loss: 0.274\n",
      "epoch: 2500, acc: 0.688, loss: 0.527\n",
      "epoch: 2500, acc: 0.875, loss: 0.353\n",
      "epoch: 2500, acc: 0.906, loss: 0.406\n",
      "epoch: 2500, acc: 0.750, loss: 0.552\n",
      "epoch: 2500, acc: 0.844, loss: 0.411\n",
      "epoch: 2500, acc: 0.844, loss: 0.317\n",
      "epoch: 2500, acc: 0.750, loss: 0.521\n",
      "epoch: 2500, acc: 0.812, loss: 0.464\n",
      "epoch: 2500, acc: 0.781, loss: 0.414\n",
      "epoch: 2500, acc: 0.844, loss: 0.405\n",
      "epoch: 2500, acc: 0.844, loss: 0.398\n",
      "epoch: 2500, acc: 0.750, loss: 0.395\n",
      "epoch: 2500, acc: 0.750, loss: 0.476\n",
      "epoch: 2500, acc: 0.906, loss: 0.255\n",
      "epoch: 2500, acc: 0.781, loss: 0.466\n",
      "epoch: 2500, acc: 0.969, loss: 0.198\n",
      "epoch: 2500, acc: 0.781, loss: 0.454\n",
      "epoch: 2500, acc: 0.719, loss: 0.488\n",
      "epoch: 2500, acc: 0.750, loss: 0.466\n",
      "epoch: 2500, acc: 0.812, loss: 0.366\n",
      "epoch: 2500, acc: 0.719, loss: 0.588\n",
      "epoch: 2500, acc: 0.688, loss: 0.639\n",
      "epoch: 2500, acc: 0.905, loss: 0.316\n",
      "epoch: 2600, acc: 0.844, loss: 0.373\n",
      "epoch: 2600, acc: 0.750, loss: 0.451\n",
      "epoch: 2600, acc: 0.812, loss: 0.442\n",
      "epoch: 2600, acc: 0.781, loss: 0.517\n",
      "epoch: 2600, acc: 0.938, loss: 0.273\n",
      "epoch: 2600, acc: 0.875, loss: 0.341\n",
      "epoch: 2600, acc: 0.750, loss: 0.514\n",
      "epoch: 2600, acc: 0.719, loss: 0.595\n",
      "epoch: 2600, acc: 0.781, loss: 0.395\n",
      "epoch: 2600, acc: 0.750, loss: 0.477\n",
      "epoch: 2600, acc: 0.781, loss: 0.597\n",
      "epoch: 2600, acc: 0.812, loss: 0.443\n",
      "epoch: 2600, acc: 0.906, loss: 0.386\n",
      "epoch: 2600, acc: 0.844, loss: 0.305\n",
      "epoch: 2600, acc: 0.812, loss: 0.357\n",
      "epoch: 2600, acc: 0.875, loss: 0.310\n",
      "epoch: 2600, acc: 0.750, loss: 0.636\n",
      "epoch: 2600, acc: 0.906, loss: 0.341\n",
      "epoch: 2600, acc: 0.969, loss: 0.223\n",
      "epoch: 2600, acc: 0.906, loss: 0.266\n",
      "epoch: 2600, acc: 0.688, loss: 0.612\n",
      "epoch: 2600, acc: 0.781, loss: 0.408\n",
      "epoch: 2600, acc: 0.781, loss: 0.377\n",
      "epoch: 2600, acc: 0.781, loss: 0.433\n",
      "epoch: 2600, acc: 0.781, loss: 0.448\n",
      "epoch: 2600, acc: 0.844, loss: 0.384\n",
      "epoch: 2600, acc: 0.844, loss: 0.370\n",
      "epoch: 2600, acc: 0.906, loss: 0.278\n",
      "epoch: 2600, acc: 0.938, loss: 0.335\n",
      "epoch: 2600, acc: 0.625, loss: 0.839\n",
      "epoch: 2600, acc: 0.875, loss: 0.346\n",
      "epoch: 2600, acc: 0.781, loss: 0.391\n",
      "epoch: 2600, acc: 0.810, loss: 0.373\n",
      "epoch: 2700, acc: 0.906, loss: 0.303\n",
      "epoch: 2700, acc: 0.812, loss: 0.322\n",
      "epoch: 2700, acc: 0.844, loss: 0.393\n",
      "epoch: 2700, acc: 0.844, loss: 0.593\n",
      "epoch: 2700, acc: 0.906, loss: 0.250\n",
      "epoch: 2700, acc: 0.781, loss: 0.503\n",
      "epoch: 2700, acc: 0.750, loss: 0.548\n",
      "epoch: 2700, acc: 0.844, loss: 0.408\n",
      "epoch: 2700, acc: 0.781, loss: 0.575\n",
      "epoch: 2700, acc: 0.719, loss: 0.529\n",
      "epoch: 2700, acc: 0.750, loss: 0.599\n",
      "epoch: 2700, acc: 0.938, loss: 0.210\n",
      "epoch: 2700, acc: 0.781, loss: 0.473\n",
      "epoch: 2700, acc: 0.875, loss: 0.320\n",
      "epoch: 2700, acc: 0.750, loss: 0.510\n",
      "epoch: 2700, acc: 0.844, loss: 0.429\n",
      "epoch: 2700, acc: 0.844, loss: 0.317\n",
      "epoch: 2700, acc: 0.906, loss: 0.322\n",
      "epoch: 2700, acc: 0.844, loss: 0.450\n",
      "epoch: 2700, acc: 0.938, loss: 0.263\n",
      "epoch: 2700, acc: 0.719, loss: 0.594\n",
      "epoch: 2700, acc: 0.812, loss: 0.409\n",
      "epoch: 2700, acc: 0.844, loss: 0.382\n",
      "epoch: 2700, acc: 0.750, loss: 0.514\n",
      "epoch: 2700, acc: 0.812, loss: 0.475\n",
      "epoch: 2700, acc: 0.750, loss: 0.520\n",
      "epoch: 2700, acc: 0.719, loss: 0.429\n",
      "epoch: 2700, acc: 0.906, loss: 0.320\n",
      "epoch: 2700, acc: 0.906, loss: 0.269\n",
      "epoch: 2700, acc: 0.875, loss: 0.369\n",
      "epoch: 2700, acc: 0.719, loss: 0.546\n",
      "epoch: 2700, acc: 0.844, loss: 0.361\n",
      "epoch: 2700, acc: 0.857, loss: 0.295\n",
      "epoch: 2800, acc: 0.906, loss: 0.261\n",
      "epoch: 2800, acc: 0.781, loss: 0.450\n",
      "epoch: 2800, acc: 0.875, loss: 0.350\n",
      "epoch: 2800, acc: 0.781, loss: 0.395\n",
      "epoch: 2800, acc: 0.781, loss: 0.454\n",
      "epoch: 2800, acc: 0.938, loss: 0.281\n",
      "epoch: 2800, acc: 0.688, loss: 0.584\n",
      "epoch: 2800, acc: 0.750, loss: 0.576\n",
      "epoch: 2800, acc: 0.812, loss: 0.401\n",
      "epoch: 2800, acc: 0.938, loss: 0.251\n",
      "epoch: 2800, acc: 0.875, loss: 0.270\n",
      "epoch: 2800, acc: 0.781, loss: 0.350\n",
      "epoch: 2800, acc: 0.812, loss: 0.516\n",
      "epoch: 2800, acc: 0.781, loss: 0.538\n",
      "epoch: 2800, acc: 0.688, loss: 0.753\n",
      "epoch: 2800, acc: 0.812, loss: 0.368\n",
      "epoch: 2800, acc: 0.875, loss: 0.415\n",
      "epoch: 2800, acc: 0.750, loss: 0.563\n",
      "epoch: 2800, acc: 0.906, loss: 0.366\n",
      "epoch: 2800, acc: 0.812, loss: 0.407\n",
      "epoch: 2800, acc: 0.938, loss: 0.306\n",
      "epoch: 2800, acc: 0.875, loss: 0.360\n",
      "epoch: 2800, acc: 0.906, loss: 0.292\n",
      "epoch: 2800, acc: 0.906, loss: 0.299\n",
      "epoch: 2800, acc: 0.750, loss: 0.585\n",
      "epoch: 2800, acc: 0.750, loss: 0.634\n",
      "epoch: 2800, acc: 0.688, loss: 0.457\n",
      "epoch: 2800, acc: 0.750, loss: 0.410\n",
      "epoch: 2800, acc: 0.938, loss: 0.258\n",
      "epoch: 2800, acc: 0.688, loss: 0.639\n",
      "epoch: 2800, acc: 0.781, loss: 0.413\n",
      "epoch: 2800, acc: 0.812, loss: 0.381\n",
      "epoch: 2800, acc: 1.000, loss: 0.176\n",
      "epoch: 2900, acc: 0.844, loss: 0.317\n",
      "epoch: 2900, acc: 0.656, loss: 0.579\n",
      "epoch: 2900, acc: 0.844, loss: 0.427\n",
      "epoch: 2900, acc: 0.812, loss: 0.536\n",
      "epoch: 2900, acc: 0.781, loss: 0.524\n",
      "epoch: 2900, acc: 0.844, loss: 0.441\n",
      "epoch: 2900, acc: 0.875, loss: 0.454\n",
      "epoch: 2900, acc: 0.781, loss: 0.443\n",
      "epoch: 2900, acc: 0.750, loss: 0.503\n",
      "epoch: 2900, acc: 0.812, loss: 0.382\n",
      "epoch: 2900, acc: 0.875, loss: 0.288\n",
      "epoch: 2900, acc: 0.781, loss: 0.400\n",
      "epoch: 2900, acc: 0.844, loss: 0.392\n",
      "epoch: 2900, acc: 0.844, loss: 0.318\n",
      "epoch: 2900, acc: 0.719, loss: 0.487\n",
      "epoch: 2900, acc: 0.875, loss: 0.511\n",
      "epoch: 2900, acc: 0.750, loss: 0.740\n",
      "epoch: 2900, acc: 0.875, loss: 0.336\n",
      "epoch: 2900, acc: 0.781, loss: 0.427\n",
      "epoch: 2900, acc: 0.938, loss: 0.263\n",
      "epoch: 2900, acc: 0.875, loss: 0.332\n",
      "epoch: 2900, acc: 0.875, loss: 0.301\n",
      "epoch: 2900, acc: 0.875, loss: 0.348\n",
      "epoch: 2900, acc: 0.719, loss: 0.576\n",
      "epoch: 2900, acc: 0.719, loss: 0.525\n",
      "epoch: 2900, acc: 0.938, loss: 0.275\n",
      "epoch: 2900, acc: 0.812, loss: 0.441\n",
      "epoch: 2900, acc: 0.781, loss: 0.407\n",
      "epoch: 2900, acc: 0.719, loss: 0.459\n",
      "epoch: 2900, acc: 0.969, loss: 0.224\n",
      "epoch: 2900, acc: 0.781, loss: 0.502\n",
      "epoch: 2900, acc: 0.906, loss: 0.303\n",
      "epoch: 2900, acc: 0.857, loss: 0.364\n",
      "epoch: 3000, acc: 0.875, loss: 0.377\n",
      "epoch: 3000, acc: 0.812, loss: 0.535\n",
      "epoch: 3000, acc: 0.750, loss: 0.519\n",
      "epoch: 3000, acc: 0.688, loss: 0.553\n",
      "epoch: 3000, acc: 0.938, loss: 0.317\n",
      "epoch: 3000, acc: 0.750, loss: 0.453\n",
      "epoch: 3000, acc: 0.969, loss: 0.256\n",
      "epoch: 3000, acc: 0.750, loss: 0.491\n",
      "epoch: 3000, acc: 0.906, loss: 0.335\n",
      "epoch: 3000, acc: 0.844, loss: 0.278\n",
      "epoch: 3000, acc: 0.781, loss: 0.460\n",
      "epoch: 3000, acc: 0.812, loss: 0.407\n",
      "epoch: 3000, acc: 0.812, loss: 0.570\n",
      "epoch: 3000, acc: 0.812, loss: 0.369\n",
      "epoch: 3000, acc: 0.875, loss: 0.338\n",
      "epoch: 3000, acc: 0.812, loss: 0.413\n",
      "epoch: 3000, acc: 0.812, loss: 0.533\n",
      "epoch: 3000, acc: 0.906, loss: 0.348\n",
      "epoch: 3000, acc: 0.844, loss: 0.450\n",
      "epoch: 3000, acc: 0.812, loss: 0.415\n",
      "epoch: 3000, acc: 0.875, loss: 0.276\n",
      "epoch: 3000, acc: 0.750, loss: 0.475\n",
      "epoch: 3000, acc: 0.875, loss: 0.364\n",
      "epoch: 3000, acc: 0.844, loss: 0.366\n",
      "epoch: 3000, acc: 0.875, loss: 0.299\n",
      "epoch: 3000, acc: 0.969, loss: 0.264\n",
      "epoch: 3000, acc: 0.656, loss: 0.547\n",
      "epoch: 3000, acc: 0.875, loss: 0.286\n",
      "epoch: 3000, acc: 0.781, loss: 0.459\n",
      "epoch: 3000, acc: 0.750, loss: 0.598\n",
      "epoch: 3000, acc: 0.781, loss: 0.552\n",
      "epoch: 3000, acc: 0.625, loss: 0.524\n",
      "epoch: 3000, acc: 0.762, loss: 0.483\n",
      "epoch: 3100, acc: 0.938, loss: 0.254\n",
      "epoch: 3100, acc: 0.844, loss: 0.384\n",
      "epoch: 3100, acc: 0.781, loss: 0.490\n",
      "epoch: 3100, acc: 0.812, loss: 0.451\n",
      "epoch: 3100, acc: 0.656, loss: 0.661\n",
      "epoch: 3100, acc: 0.781, loss: 0.398\n",
      "epoch: 3100, acc: 1.000, loss: 0.183\n",
      "epoch: 3100, acc: 0.781, loss: 0.484\n",
      "epoch: 3100, acc: 0.812, loss: 0.378\n",
      "epoch: 3100, acc: 0.812, loss: 0.414\n",
      "epoch: 3100, acc: 0.719, loss: 0.541\n",
      "epoch: 3100, acc: 0.906, loss: 0.304\n",
      "epoch: 3100, acc: 0.875, loss: 0.301\n",
      "epoch: 3100, acc: 0.844, loss: 0.381\n",
      "epoch: 3100, acc: 0.812, loss: 0.428\n",
      "epoch: 3100, acc: 0.812, loss: 0.515\n",
      "epoch: 3100, acc: 0.844, loss: 0.466\n",
      "epoch: 3100, acc: 0.719, loss: 0.522\n",
      "epoch: 3100, acc: 0.812, loss: 0.456\n",
      "epoch: 3100, acc: 0.812, loss: 0.407\n",
      "epoch: 3100, acc: 0.875, loss: 0.349\n",
      "epoch: 3100, acc: 0.906, loss: 0.302\n",
      "epoch: 3100, acc: 0.844, loss: 0.409\n",
      "epoch: 3100, acc: 0.812, loss: 0.616\n",
      "epoch: 3100, acc: 0.781, loss: 0.497\n",
      "epoch: 3100, acc: 0.750, loss: 0.512\n",
      "epoch: 3100, acc: 0.844, loss: 0.344\n",
      "epoch: 3100, acc: 0.906, loss: 0.226\n",
      "epoch: 3100, acc: 0.625, loss: 0.537\n",
      "epoch: 3100, acc: 0.844, loss: 0.467\n",
      "epoch: 3100, acc: 0.844, loss: 0.405\n",
      "epoch: 3100, acc: 0.875, loss: 0.319\n",
      "epoch: 3100, acc: 0.810, loss: 0.476\n",
      "epoch: 3200, acc: 0.750, loss: 0.423\n",
      "epoch: 3200, acc: 0.812, loss: 0.368\n",
      "epoch: 3200, acc: 0.844, loss: 0.392\n",
      "epoch: 3200, acc: 0.781, loss: 0.593\n",
      "epoch: 3200, acc: 0.875, loss: 0.479\n",
      "epoch: 3200, acc: 0.938, loss: 0.273\n",
      "epoch: 3200, acc: 0.781, loss: 0.441\n",
      "epoch: 3200, acc: 0.844, loss: 0.403\n",
      "epoch: 3200, acc: 0.844, loss: 0.341\n",
      "epoch: 3200, acc: 0.750, loss: 0.571\n",
      "epoch: 3200, acc: 0.844, loss: 0.337\n",
      "epoch: 3200, acc: 0.719, loss: 0.507\n",
      "epoch: 3200, acc: 0.750, loss: 0.456\n",
      "epoch: 3200, acc: 0.844, loss: 0.398\n",
      "epoch: 3200, acc: 0.906, loss: 0.323\n",
      "epoch: 3200, acc: 0.906, loss: 0.208\n",
      "epoch: 3200, acc: 0.688, loss: 0.520\n",
      "epoch: 3200, acc: 0.656, loss: 0.635\n",
      "epoch: 3200, acc: 0.781, loss: 0.462\n",
      "epoch: 3200, acc: 0.875, loss: 0.308\n",
      "epoch: 3200, acc: 0.844, loss: 0.411\n",
      "epoch: 3200, acc: 0.938, loss: 0.354\n",
      "epoch: 3200, acc: 0.812, loss: 0.471\n",
      "epoch: 3200, acc: 0.750, loss: 0.546\n",
      "epoch: 3200, acc: 0.844, loss: 0.326\n",
      "epoch: 3200, acc: 0.844, loss: 0.401\n",
      "epoch: 3200, acc: 0.875, loss: 0.334\n",
      "epoch: 3200, acc: 0.875, loss: 0.474\n",
      "epoch: 3200, acc: 0.844, loss: 0.424\n",
      "epoch: 3200, acc: 0.781, loss: 0.490\n",
      "epoch: 3200, acc: 0.688, loss: 0.554\n",
      "epoch: 3200, acc: 0.906, loss: 0.315\n",
      "epoch: 3200, acc: 0.952, loss: 0.235\n",
      "epoch: 3300, acc: 0.844, loss: 0.341\n",
      "epoch: 3300, acc: 0.812, loss: 0.397\n",
      "epoch: 3300, acc: 0.844, loss: 0.326\n",
      "epoch: 3300, acc: 0.844, loss: 0.465\n",
      "epoch: 3300, acc: 0.906, loss: 0.244\n",
      "epoch: 3300, acc: 0.906, loss: 0.338\n",
      "epoch: 3300, acc: 0.875, loss: 0.260\n",
      "epoch: 3300, acc: 0.656, loss: 0.677\n",
      "epoch: 3300, acc: 0.875, loss: 0.335\n",
      "epoch: 3300, acc: 0.875, loss: 0.292\n",
      "epoch: 3300, acc: 0.906, loss: 0.401\n",
      "epoch: 3300, acc: 0.781, loss: 0.458\n",
      "epoch: 3300, acc: 0.875, loss: 0.262\n",
      "epoch: 3300, acc: 0.781, loss: 0.506\n",
      "epoch: 3300, acc: 0.812, loss: 0.435\n",
      "epoch: 3300, acc: 0.844, loss: 0.377\n",
      "epoch: 3300, acc: 0.812, loss: 0.452\n",
      "epoch: 3300, acc: 0.781, loss: 0.316\n",
      "epoch: 3300, acc: 0.812, loss: 0.463\n",
      "epoch: 3300, acc: 0.656, loss: 0.581\n",
      "epoch: 3300, acc: 0.688, loss: 0.473\n",
      "epoch: 3300, acc: 0.781, loss: 0.361\n",
      "epoch: 3300, acc: 0.812, loss: 0.468\n",
      "epoch: 3300, acc: 0.688, loss: 0.494\n",
      "epoch: 3300, acc: 0.875, loss: 0.357\n",
      "epoch: 3300, acc: 0.812, loss: 0.430\n",
      "epoch: 3300, acc: 0.906, loss: 0.288\n",
      "epoch: 3300, acc: 0.875, loss: 0.485\n",
      "epoch: 3300, acc: 0.906, loss: 0.329\n",
      "epoch: 3300, acc: 0.781, loss: 0.795\n",
      "epoch: 3300, acc: 0.750, loss: 0.517\n",
      "epoch: 3300, acc: 0.812, loss: 0.500\n",
      "epoch: 3300, acc: 0.762, loss: 0.429\n",
      "epoch: 3400, acc: 0.844, loss: 0.389\n",
      "epoch: 3400, acc: 0.844, loss: 0.392\n",
      "epoch: 3400, acc: 0.844, loss: 0.425\n",
      "epoch: 3400, acc: 0.719, loss: 0.535\n",
      "epoch: 3400, acc: 0.781, loss: 0.587\n",
      "epoch: 3400, acc: 0.875, loss: 0.425\n",
      "epoch: 3400, acc: 0.844, loss: 0.400\n",
      "epoch: 3400, acc: 0.750, loss: 0.464\n",
      "epoch: 3400, acc: 0.844, loss: 0.448\n",
      "epoch: 3400, acc: 0.844, loss: 0.378\n",
      "epoch: 3400, acc: 0.812, loss: 0.356\n",
      "epoch: 3400, acc: 0.812, loss: 0.447\n",
      "epoch: 3400, acc: 0.812, loss: 0.399\n",
      "epoch: 3400, acc: 0.906, loss: 0.270\n",
      "epoch: 3400, acc: 0.844, loss: 0.324\n",
      "epoch: 3400, acc: 0.812, loss: 0.485\n",
      "epoch: 3400, acc: 0.875, loss: 0.322\n",
      "epoch: 3400, acc: 0.844, loss: 0.454\n",
      "epoch: 3400, acc: 0.750, loss: 0.614\n",
      "epoch: 3400, acc: 0.781, loss: 0.450\n",
      "epoch: 3400, acc: 0.750, loss: 0.506\n",
      "epoch: 3400, acc: 0.875, loss: 0.308\n",
      "epoch: 3400, acc: 0.875, loss: 0.364\n",
      "epoch: 3400, acc: 0.688, loss: 0.615\n",
      "epoch: 3400, acc: 0.719, loss: 0.455\n",
      "epoch: 3400, acc: 0.719, loss: 0.433\n",
      "epoch: 3400, acc: 0.938, loss: 0.247\n",
      "epoch: 3400, acc: 0.938, loss: 0.263\n",
      "epoch: 3400, acc: 0.781, loss: 0.456\n",
      "epoch: 3400, acc: 0.906, loss: 0.254\n",
      "epoch: 3400, acc: 0.812, loss: 0.340\n",
      "epoch: 3400, acc: 0.781, loss: 0.542\n",
      "epoch: 3400, acc: 0.762, loss: 0.578\n",
      "epoch: 3500, acc: 0.875, loss: 0.354\n",
      "epoch: 3500, acc: 0.812, loss: 0.646\n",
      "epoch: 3500, acc: 0.875, loss: 0.344\n",
      "epoch: 3500, acc: 0.938, loss: 0.215\n",
      "epoch: 3500, acc: 0.938, loss: 0.315\n",
      "epoch: 3500, acc: 0.875, loss: 0.314\n",
      "epoch: 3500, acc: 0.812, loss: 0.383\n",
      "epoch: 3500, acc: 0.750, loss: 0.452\n",
      "epoch: 3500, acc: 0.812, loss: 0.362\n",
      "epoch: 3500, acc: 0.875, loss: 0.365\n",
      "epoch: 3500, acc: 0.812, loss: 0.424\n",
      "epoch: 3500, acc: 0.781, loss: 0.447\n",
      "epoch: 3500, acc: 0.781, loss: 0.560\n",
      "epoch: 3500, acc: 0.906, loss: 0.275\n",
      "epoch: 3500, acc: 0.781, loss: 0.305\n",
      "epoch: 3500, acc: 0.875, loss: 0.400\n",
      "epoch: 3500, acc: 0.688, loss: 0.644\n",
      "epoch: 3500, acc: 0.781, loss: 0.517\n",
      "epoch: 3500, acc: 0.781, loss: 0.402\n",
      "epoch: 3500, acc: 0.875, loss: 0.363\n",
      "epoch: 3500, acc: 0.781, loss: 0.551\n",
      "epoch: 3500, acc: 0.719, loss: 0.496\n",
      "epoch: 3500, acc: 0.719, loss: 0.488\n",
      "epoch: 3500, acc: 0.844, loss: 0.343\n",
      "epoch: 3500, acc: 0.844, loss: 0.360\n",
      "epoch: 3500, acc: 0.875, loss: 0.377\n",
      "epoch: 3500, acc: 0.750, loss: 0.437\n",
      "epoch: 3500, acc: 0.875, loss: 0.410\n",
      "epoch: 3500, acc: 0.781, loss: 0.490\n",
      "epoch: 3500, acc: 0.875, loss: 0.322\n",
      "epoch: 3500, acc: 0.656, loss: 0.739\n",
      "epoch: 3500, acc: 0.812, loss: 0.375\n",
      "epoch: 3500, acc: 0.857, loss: 0.308\n",
      "epoch: 3600, acc: 0.562, loss: 0.781\n",
      "epoch: 3600, acc: 0.844, loss: 0.375\n",
      "epoch: 3600, acc: 0.844, loss: 0.366\n",
      "epoch: 3600, acc: 0.844, loss: 0.328\n",
      "epoch: 3600, acc: 0.875, loss: 0.387\n",
      "epoch: 3600, acc: 0.906, loss: 0.351\n",
      "epoch: 3600, acc: 0.781, loss: 0.510\n",
      "epoch: 3600, acc: 0.844, loss: 0.421\n",
      "epoch: 3600, acc: 0.844, loss: 0.379\n",
      "epoch: 3600, acc: 0.844, loss: 0.397\n",
      "epoch: 3600, acc: 0.719, loss: 0.560\n",
      "epoch: 3600, acc: 0.875, loss: 0.367\n",
      "epoch: 3600, acc: 0.750, loss: 0.545\n",
      "epoch: 3600, acc: 0.750, loss: 0.485\n",
      "epoch: 3600, acc: 0.812, loss: 0.414\n",
      "epoch: 3600, acc: 0.781, loss: 0.438\n",
      "epoch: 3600, acc: 0.906, loss: 0.281\n",
      "epoch: 3600, acc: 0.750, loss: 0.447\n",
      "epoch: 3600, acc: 0.906, loss: 0.335\n",
      "epoch: 3600, acc: 0.875, loss: 0.274\n",
      "epoch: 3600, acc: 0.844, loss: 0.378\n",
      "epoch: 3600, acc: 0.906, loss: 0.303\n",
      "epoch: 3600, acc: 0.812, loss: 0.587\n",
      "epoch: 3600, acc: 0.875, loss: 0.448\n",
      "epoch: 3600, acc: 0.781, loss: 0.451\n",
      "epoch: 3600, acc: 0.781, loss: 0.389\n",
      "epoch: 3600, acc: 0.750, loss: 0.512\n",
      "epoch: 3600, acc: 0.844, loss: 0.388\n",
      "epoch: 3600, acc: 0.781, loss: 0.548\n",
      "epoch: 3600, acc: 0.844, loss: 0.379\n",
      "epoch: 3600, acc: 0.750, loss: 0.437\n",
      "epoch: 3600, acc: 0.938, loss: 0.206\n",
      "epoch: 3600, acc: 0.857, loss: 0.347\n",
      "epoch: 3700, acc: 0.875, loss: 0.376\n",
      "epoch: 3700, acc: 0.812, loss: 0.347\n",
      "epoch: 3700, acc: 0.812, loss: 0.387\n",
      "epoch: 3700, acc: 0.875, loss: 0.324\n",
      "epoch: 3700, acc: 0.875, loss: 0.331\n",
      "epoch: 3700, acc: 0.875, loss: 0.340\n",
      "epoch: 3700, acc: 0.719, loss: 0.605\n",
      "epoch: 3700, acc: 0.844, loss: 0.365\n",
      "epoch: 3700, acc: 0.812, loss: 0.350\n",
      "epoch: 3700, acc: 0.844, loss: 0.384\n",
      "epoch: 3700, acc: 0.906, loss: 0.369\n",
      "epoch: 3700, acc: 0.875, loss: 0.287\n",
      "epoch: 3700, acc: 0.781, loss: 0.460\n",
      "epoch: 3700, acc: 0.781, loss: 0.464\n",
      "epoch: 3700, acc: 0.812, loss: 0.441\n",
      "epoch: 3700, acc: 0.812, loss: 0.323\n",
      "epoch: 3700, acc: 0.781, loss: 0.481\n",
      "epoch: 3700, acc: 0.812, loss: 0.379\n",
      "epoch: 3700, acc: 0.875, loss: 0.324\n",
      "epoch: 3700, acc: 0.750, loss: 0.595\n",
      "epoch: 3700, acc: 0.719, loss: 0.495\n",
      "epoch: 3700, acc: 0.844, loss: 0.298\n",
      "epoch: 3700, acc: 0.875, loss: 0.320\n",
      "epoch: 3700, acc: 0.781, loss: 0.462\n",
      "epoch: 3700, acc: 0.875, loss: 0.428\n",
      "epoch: 3700, acc: 0.844, loss: 0.441\n",
      "epoch: 3700, acc: 0.844, loss: 0.437\n",
      "epoch: 3700, acc: 0.781, loss: 0.446\n",
      "epoch: 3700, acc: 0.781, loss: 0.634\n",
      "epoch: 3700, acc: 0.875, loss: 0.393\n",
      "epoch: 3700, acc: 0.688, loss: 0.521\n",
      "epoch: 3700, acc: 0.844, loss: 0.431\n",
      "epoch: 3700, acc: 0.810, loss: 0.734\n",
      "epoch: 3800, acc: 0.844, loss: 0.363\n",
      "epoch: 3800, acc: 0.812, loss: 0.517\n",
      "epoch: 3800, acc: 0.750, loss: 0.450\n",
      "epoch: 3800, acc: 0.781, loss: 0.528\n",
      "epoch: 3800, acc: 0.969, loss: 0.219\n",
      "epoch: 3800, acc: 0.844, loss: 0.342\n",
      "epoch: 3800, acc: 0.781, loss: 0.399\n",
      "epoch: 3800, acc: 0.875, loss: 0.383\n",
      "epoch: 3800, acc: 0.875, loss: 0.339\n",
      "epoch: 3800, acc: 0.750, loss: 0.535\n",
      "epoch: 3800, acc: 0.562, loss: 0.827\n",
      "epoch: 3800, acc: 0.781, loss: 0.474\n",
      "epoch: 3800, acc: 0.844, loss: 0.503\n",
      "epoch: 3800, acc: 0.812, loss: 0.481\n",
      "epoch: 3800, acc: 0.781, loss: 0.385\n",
      "epoch: 3800, acc: 0.750, loss: 0.557\n",
      "epoch: 3800, acc: 0.812, loss: 0.343\n",
      "epoch: 3800, acc: 0.906, loss: 0.262\n",
      "epoch: 3800, acc: 0.844, loss: 0.589\n",
      "epoch: 3800, acc: 0.844, loss: 0.364\n",
      "epoch: 3800, acc: 0.750, loss: 0.469\n",
      "epoch: 3800, acc: 0.875, loss: 0.349\n",
      "epoch: 3800, acc: 0.844, loss: 0.399\n",
      "epoch: 3800, acc: 0.750, loss: 0.554\n",
      "epoch: 3800, acc: 0.906, loss: 0.295\n",
      "epoch: 3800, acc: 0.844, loss: 0.315\n",
      "epoch: 3800, acc: 0.906, loss: 0.387\n",
      "epoch: 3800, acc: 0.875, loss: 0.244\n",
      "epoch: 3800, acc: 0.719, loss: 0.459\n",
      "epoch: 3800, acc: 0.875, loss: 0.357\n",
      "epoch: 3800, acc: 0.812, loss: 0.420\n",
      "epoch: 3800, acc: 0.812, loss: 0.331\n",
      "epoch: 3800, acc: 0.762, loss: 0.441\n",
      "epoch: 3900, acc: 0.812, loss: 0.439\n",
      "epoch: 3900, acc: 0.688, loss: 0.733\n",
      "epoch: 3900, acc: 0.812, loss: 0.367\n",
      "epoch: 3900, acc: 0.812, loss: 0.426\n",
      "epoch: 3900, acc: 0.844, loss: 0.452\n",
      "epoch: 3900, acc: 0.844, loss: 0.382\n",
      "epoch: 3900, acc: 0.875, loss: 0.414\n",
      "epoch: 3900, acc: 0.812, loss: 0.381\n",
      "epoch: 3900, acc: 0.812, loss: 0.485\n",
      "epoch: 3900, acc: 0.844, loss: 0.340\n",
      "epoch: 3900, acc: 0.844, loss: 0.333\n",
      "epoch: 3900, acc: 0.750, loss: 0.382\n",
      "epoch: 3900, acc: 0.938, loss: 0.206\n",
      "epoch: 3900, acc: 0.844, loss: 0.422\n",
      "epoch: 3900, acc: 0.875, loss: 0.406\n",
      "epoch: 3900, acc: 0.719, loss: 0.549\n",
      "epoch: 3900, acc: 0.719, loss: 0.483\n",
      "epoch: 3900, acc: 0.719, loss: 0.528\n",
      "epoch: 3900, acc: 0.875, loss: 0.331\n",
      "epoch: 3900, acc: 0.719, loss: 0.527\n",
      "epoch: 3900, acc: 0.875, loss: 0.359\n",
      "epoch: 3900, acc: 0.906, loss: 0.243\n",
      "epoch: 3900, acc: 0.750, loss: 0.486\n",
      "epoch: 3900, acc: 0.812, loss: 0.411\n",
      "epoch: 3900, acc: 0.781, loss: 0.386\n",
      "epoch: 3900, acc: 0.812, loss: 0.416\n",
      "epoch: 3900, acc: 0.750, loss: 0.591\n",
      "epoch: 3900, acc: 0.750, loss: 0.550\n",
      "epoch: 3900, acc: 0.875, loss: 0.302\n",
      "epoch: 3900, acc: 0.906, loss: 0.327\n",
      "epoch: 3900, acc: 0.906, loss: 0.375\n",
      "epoch: 3900, acc: 0.875, loss: 0.458\n",
      "epoch: 3900, acc: 0.857, loss: 0.366\n",
      "epoch: 4000, acc: 0.719, loss: 0.439\n",
      "epoch: 4000, acc: 0.750, loss: 0.536\n",
      "epoch: 4000, acc: 0.938, loss: 0.307\n",
      "epoch: 4000, acc: 0.750, loss: 0.482\n",
      "epoch: 4000, acc: 0.875, loss: 0.333\n",
      "epoch: 4000, acc: 0.844, loss: 0.428\n",
      "epoch: 4000, acc: 0.875, loss: 0.318\n",
      "epoch: 4000, acc: 0.750, loss: 0.440\n",
      "epoch: 4000, acc: 0.719, loss: 0.487\n",
      "epoch: 4000, acc: 0.844, loss: 0.431\n",
      "epoch: 4000, acc: 0.875, loss: 0.379\n",
      "epoch: 4000, acc: 0.844, loss: 0.322\n",
      "epoch: 4000, acc: 0.844, loss: 0.376\n",
      "epoch: 4000, acc: 0.938, loss: 0.252\n",
      "epoch: 4000, acc: 0.875, loss: 0.368\n",
      "epoch: 4000, acc: 0.750, loss: 0.716\n",
      "epoch: 4000, acc: 0.875, loss: 0.290\n",
      "epoch: 4000, acc: 0.875, loss: 0.348\n",
      "epoch: 4000, acc: 0.844, loss: 0.439\n",
      "epoch: 4000, acc: 0.812, loss: 0.536\n",
      "epoch: 4000, acc: 0.781, loss: 0.445\n",
      "epoch: 4000, acc: 0.844, loss: 0.535\n",
      "epoch: 4000, acc: 0.875, loss: 0.347\n",
      "epoch: 4000, acc: 0.812, loss: 0.536\n",
      "epoch: 4000, acc: 0.656, loss: 0.649\n",
      "epoch: 4000, acc: 0.812, loss: 0.410\n",
      "epoch: 4000, acc: 0.875, loss: 0.373\n",
      "epoch: 4000, acc: 0.781, loss: 0.475\n",
      "epoch: 4000, acc: 0.875, loss: 0.337\n",
      "epoch: 4000, acc: 0.719, loss: 0.420\n",
      "epoch: 4000, acc: 0.844, loss: 0.336\n",
      "epoch: 4000, acc: 0.844, loss: 0.328\n",
      "epoch: 4000, acc: 0.762, loss: 0.438\n",
      "epoch: 4100, acc: 0.969, loss: 0.229\n",
      "epoch: 4100, acc: 0.656, loss: 0.725\n",
      "epoch: 4100, acc: 0.781, loss: 0.493\n",
      "epoch: 4100, acc: 0.812, loss: 0.393\n",
      "epoch: 4100, acc: 0.844, loss: 0.360\n",
      "epoch: 4100, acc: 0.812, loss: 0.487\n",
      "epoch: 4100, acc: 0.875, loss: 0.356\n",
      "epoch: 4100, acc: 0.969, loss: 0.201\n",
      "epoch: 4100, acc: 0.875, loss: 0.389\n",
      "epoch: 4100, acc: 0.938, loss: 0.190\n",
      "epoch: 4100, acc: 0.844, loss: 0.405\n",
      "epoch: 4100, acc: 0.906, loss: 0.335\n",
      "epoch: 4100, acc: 0.875, loss: 0.364\n",
      "epoch: 4100, acc: 0.875, loss: 0.402\n",
      "epoch: 4100, acc: 0.812, loss: 0.399\n",
      "epoch: 4100, acc: 0.906, loss: 0.288\n",
      "epoch: 4100, acc: 0.906, loss: 0.436\n",
      "epoch: 4100, acc: 0.625, loss: 0.727\n",
      "epoch: 4100, acc: 0.750, loss: 0.542\n",
      "epoch: 4100, acc: 0.781, loss: 0.424\n",
      "epoch: 4100, acc: 0.781, loss: 0.502\n",
      "epoch: 4100, acc: 0.688, loss: 0.487\n",
      "epoch: 4100, acc: 0.781, loss: 0.459\n",
      "epoch: 4100, acc: 0.688, loss: 0.556\n",
      "epoch: 4100, acc: 0.750, loss: 0.460\n",
      "epoch: 4100, acc: 0.844, loss: 0.393\n",
      "epoch: 4100, acc: 0.906, loss: 0.280\n",
      "epoch: 4100, acc: 0.781, loss: 0.480\n",
      "epoch: 4100, acc: 0.875, loss: 0.352\n",
      "epoch: 4100, acc: 0.812, loss: 0.400\n",
      "epoch: 4100, acc: 0.906, loss: 0.316\n",
      "epoch: 4100, acc: 0.812, loss: 0.425\n",
      "epoch: 4100, acc: 0.619, loss: 0.656\n",
      "epoch: 4200, acc: 0.719, loss: 0.548\n",
      "epoch: 4200, acc: 0.875, loss: 0.340\n",
      "epoch: 4200, acc: 0.844, loss: 0.416\n",
      "epoch: 4200, acc: 0.781, loss: 0.439\n",
      "epoch: 4200, acc: 0.844, loss: 0.417\n",
      "epoch: 4200, acc: 0.844, loss: 0.423\n",
      "epoch: 4200, acc: 0.906, loss: 0.331\n",
      "epoch: 4200, acc: 0.750, loss: 0.417\n",
      "epoch: 4200, acc: 0.812, loss: 0.376\n",
      "epoch: 4200, acc: 0.844, loss: 0.342\n",
      "epoch: 4200, acc: 0.781, loss: 0.430\n",
      "epoch: 4200, acc: 0.812, loss: 0.375\n",
      "epoch: 4200, acc: 0.750, loss: 0.578\n",
      "epoch: 4200, acc: 0.812, loss: 0.499\n",
      "epoch: 4200, acc: 0.750, loss: 0.494\n",
      "epoch: 4200, acc: 0.688, loss: 0.637\n",
      "epoch: 4200, acc: 0.906, loss: 0.293\n",
      "epoch: 4200, acc: 0.844, loss: 0.369\n",
      "epoch: 4200, acc: 0.781, loss: 0.430\n",
      "epoch: 4200, acc: 0.875, loss: 0.320\n",
      "epoch: 4200, acc: 0.906, loss: 0.386\n",
      "epoch: 4200, acc: 0.875, loss: 0.357\n",
      "epoch: 4200, acc: 0.750, loss: 0.571\n",
      "epoch: 4200, acc: 0.906, loss: 0.334\n",
      "epoch: 4200, acc: 0.688, loss: 0.510\n",
      "epoch: 4200, acc: 0.750, loss: 0.522\n",
      "epoch: 4200, acc: 0.875, loss: 0.362\n",
      "epoch: 4200, acc: 0.875, loss: 0.388\n",
      "epoch: 4200, acc: 0.781, loss: 0.465\n",
      "epoch: 4200, acc: 0.906, loss: 0.406\n",
      "epoch: 4200, acc: 0.906, loss: 0.342\n",
      "epoch: 4200, acc: 0.812, loss: 0.364\n",
      "epoch: 4200, acc: 0.857, loss: 0.325\n",
      "epoch: 4300, acc: 0.844, loss: 0.385\n",
      "epoch: 4300, acc: 0.688, loss: 0.677\n",
      "epoch: 4300, acc: 0.688, loss: 0.663\n",
      "epoch: 4300, acc: 0.875, loss: 0.292\n",
      "epoch: 4300, acc: 0.844, loss: 0.351\n",
      "epoch: 4300, acc: 0.750, loss: 0.426\n",
      "epoch: 4300, acc: 0.812, loss: 0.444\n",
      "epoch: 4300, acc: 0.875, loss: 0.350\n",
      "epoch: 4300, acc: 0.938, loss: 0.236\n",
      "epoch: 4300, acc: 0.844, loss: 0.455\n",
      "epoch: 4300, acc: 0.906, loss: 0.340\n",
      "epoch: 4300, acc: 0.812, loss: 0.431\n",
      "epoch: 4300, acc: 0.875, loss: 0.244\n",
      "epoch: 4300, acc: 0.844, loss: 0.372\n",
      "epoch: 4300, acc: 0.844, loss: 0.328\n",
      "epoch: 4300, acc: 0.812, loss: 0.408\n",
      "epoch: 4300, acc: 0.781, loss: 0.490\n",
      "epoch: 4300, acc: 0.844, loss: 0.364\n",
      "epoch: 4300, acc: 0.844, loss: 0.448\n",
      "epoch: 4300, acc: 0.875, loss: 0.364\n",
      "epoch: 4300, acc: 0.719, loss: 0.581\n",
      "epoch: 4300, acc: 0.750, loss: 0.393\n",
      "epoch: 4300, acc: 0.844, loss: 0.421\n",
      "epoch: 4300, acc: 0.812, loss: 0.475\n",
      "epoch: 4300, acc: 0.875, loss: 0.321\n",
      "epoch: 4300, acc: 0.844, loss: 0.317\n",
      "epoch: 4300, acc: 0.656, loss: 0.649\n",
      "epoch: 4300, acc: 0.781, loss: 0.476\n",
      "epoch: 4300, acc: 0.812, loss: 0.418\n",
      "epoch: 4300, acc: 0.750, loss: 0.580\n",
      "epoch: 4300, acc: 0.938, loss: 0.368\n",
      "epoch: 4300, acc: 0.781, loss: 0.432\n",
      "epoch: 4300, acc: 0.905, loss: 0.354\n",
      "epoch: 4400, acc: 0.812, loss: 0.321\n",
      "epoch: 4400, acc: 0.875, loss: 0.424\n",
      "epoch: 4400, acc: 0.750, loss: 0.471\n",
      "epoch: 4400, acc: 0.906, loss: 0.297\n",
      "epoch: 4400, acc: 1.000, loss: 0.193\n",
      "epoch: 4400, acc: 0.781, loss: 0.564\n",
      "epoch: 4400, acc: 0.875, loss: 0.316\n",
      "epoch: 4400, acc: 0.844, loss: 0.357\n",
      "epoch: 4400, acc: 0.656, loss: 0.554\n",
      "epoch: 4400, acc: 0.812, loss: 0.390\n",
      "epoch: 4400, acc: 0.750, loss: 0.464\n",
      "epoch: 4400, acc: 0.938, loss: 0.301\n",
      "epoch: 4400, acc: 0.844, loss: 0.509\n",
      "epoch: 4400, acc: 0.844, loss: 0.323\n",
      "epoch: 4400, acc: 0.875, loss: 0.369\n",
      "epoch: 4400, acc: 0.719, loss: 0.587\n",
      "epoch: 4400, acc: 0.844, loss: 0.466\n",
      "epoch: 4400, acc: 0.750, loss: 0.533\n",
      "epoch: 4400, acc: 0.719, loss: 0.462\n",
      "epoch: 4400, acc: 0.938, loss: 0.295\n",
      "epoch: 4400, acc: 0.812, loss: 0.365\n",
      "epoch: 4400, acc: 0.844, loss: 0.466\n",
      "epoch: 4400, acc: 0.844, loss: 0.366\n",
      "epoch: 4400, acc: 0.688, loss: 0.597\n",
      "epoch: 4400, acc: 0.844, loss: 0.398\n",
      "epoch: 4400, acc: 0.875, loss: 0.303\n",
      "epoch: 4400, acc: 0.812, loss: 0.411\n",
      "epoch: 4400, acc: 0.812, loss: 0.401\n",
      "epoch: 4400, acc: 0.844, loss: 0.399\n",
      "epoch: 4400, acc: 0.812, loss: 0.357\n",
      "epoch: 4400, acc: 0.688, loss: 0.541\n",
      "epoch: 4400, acc: 0.781, loss: 0.583\n",
      "epoch: 4400, acc: 0.714, loss: 0.520\n",
      "epoch: 4500, acc: 0.688, loss: 0.587\n",
      "epoch: 4500, acc: 0.812, loss: 0.411\n",
      "epoch: 4500, acc: 0.781, loss: 0.436\n",
      "epoch: 4500, acc: 0.812, loss: 0.379\n",
      "epoch: 4500, acc: 0.750, loss: 0.505\n",
      "epoch: 4500, acc: 0.906, loss: 0.321\n",
      "epoch: 4500, acc: 0.781, loss: 0.369\n",
      "epoch: 4500, acc: 0.750, loss: 0.482\n",
      "epoch: 4500, acc: 0.875, loss: 0.394\n",
      "epoch: 4500, acc: 0.750, loss: 0.476\n",
      "epoch: 4500, acc: 0.875, loss: 0.353\n",
      "epoch: 4500, acc: 0.750, loss: 0.500\n",
      "epoch: 4500, acc: 0.875, loss: 0.252\n",
      "epoch: 4500, acc: 0.812, loss: 0.553\n",
      "epoch: 4500, acc: 0.844, loss: 0.467\n",
      "epoch: 4500, acc: 0.875, loss: 0.375\n",
      "epoch: 4500, acc: 0.750, loss: 0.490\n",
      "epoch: 4500, acc: 0.812, loss: 0.377\n",
      "epoch: 4500, acc: 0.719, loss: 0.727\n",
      "epoch: 4500, acc: 0.938, loss: 0.248\n",
      "epoch: 4500, acc: 0.781, loss: 0.459\n",
      "epoch: 4500, acc: 0.906, loss: 0.355\n",
      "epoch: 4500, acc: 0.812, loss: 0.554\n",
      "epoch: 4500, acc: 0.906, loss: 0.300\n",
      "epoch: 4500, acc: 0.781, loss: 0.421\n",
      "epoch: 4500, acc: 0.812, loss: 0.479\n",
      "epoch: 4500, acc: 0.812, loss: 0.458\n",
      "epoch: 4500, acc: 0.812, loss: 0.387\n",
      "epoch: 4500, acc: 0.906, loss: 0.320\n",
      "epoch: 4500, acc: 0.844, loss: 0.388\n",
      "epoch: 4500, acc: 0.875, loss: 0.282\n",
      "epoch: 4500, acc: 0.812, loss: 0.400\n",
      "epoch: 4500, acc: 0.905, loss: 0.275\n",
      "epoch: 4600, acc: 0.844, loss: 0.325\n",
      "epoch: 4600, acc: 0.875, loss: 0.342\n",
      "epoch: 4600, acc: 0.844, loss: 0.428\n",
      "epoch: 4600, acc: 0.812, loss: 0.433\n",
      "epoch: 4600, acc: 0.875, loss: 0.343\n",
      "epoch: 4600, acc: 0.906, loss: 0.327\n",
      "epoch: 4600, acc: 0.906, loss: 0.286\n",
      "epoch: 4600, acc: 0.781, loss: 0.549\n",
      "epoch: 4600, acc: 0.781, loss: 0.467\n",
      "epoch: 4600, acc: 0.875, loss: 0.334\n",
      "epoch: 4600, acc: 0.812, loss: 0.520\n",
      "epoch: 4600, acc: 0.812, loss: 0.394\n",
      "epoch: 4600, acc: 0.844, loss: 0.311\n",
      "epoch: 4600, acc: 0.906, loss: 0.346\n",
      "epoch: 4600, acc: 0.781, loss: 0.537\n",
      "epoch: 4600, acc: 0.844, loss: 0.388\n",
      "epoch: 4600, acc: 0.844, loss: 0.457\n",
      "epoch: 4600, acc: 0.875, loss: 0.321\n",
      "epoch: 4600, acc: 0.719, loss: 0.492\n",
      "epoch: 4600, acc: 0.719, loss: 0.566\n",
      "epoch: 4600, acc: 0.719, loss: 0.520\n",
      "epoch: 4600, acc: 0.906, loss: 0.296\n",
      "epoch: 4600, acc: 0.844, loss: 0.400\n",
      "epoch: 4600, acc: 0.781, loss: 0.302\n",
      "epoch: 4600, acc: 0.781, loss: 0.360\n",
      "epoch: 4600, acc: 0.781, loss: 0.451\n",
      "epoch: 4600, acc: 0.781, loss: 0.468\n",
      "epoch: 4600, acc: 0.812, loss: 0.365\n",
      "epoch: 4600, acc: 0.875, loss: 0.351\n",
      "epoch: 4600, acc: 0.688, loss: 0.562\n",
      "epoch: 4600, acc: 0.719, loss: 0.561\n",
      "epoch: 4600, acc: 0.625, loss: 0.728\n",
      "epoch: 4600, acc: 0.905, loss: 0.263\n",
      "epoch: 4700, acc: 0.812, loss: 0.422\n",
      "epoch: 4700, acc: 0.812, loss: 0.475\n",
      "epoch: 4700, acc: 0.750, loss: 0.590\n",
      "epoch: 4700, acc: 0.906, loss: 0.403\n",
      "epoch: 4700, acc: 0.875, loss: 0.302\n",
      "epoch: 4700, acc: 0.781, loss: 0.455\n",
      "epoch: 4700, acc: 0.812, loss: 0.408\n",
      "epoch: 4700, acc: 0.875, loss: 0.274\n",
      "epoch: 4700, acc: 0.875, loss: 0.412\n",
      "epoch: 4700, acc: 0.875, loss: 0.416\n",
      "epoch: 4700, acc: 0.781, loss: 0.525\n",
      "epoch: 4700, acc: 0.906, loss: 0.318\n",
      "epoch: 4700, acc: 0.844, loss: 0.383\n",
      "epoch: 4700, acc: 0.875, loss: 0.305\n",
      "epoch: 4700, acc: 0.875, loss: 0.431\n",
      "epoch: 4700, acc: 0.875, loss: 0.318\n",
      "epoch: 4700, acc: 0.812, loss: 0.490\n",
      "epoch: 4700, acc: 0.719, loss: 0.553\n",
      "epoch: 4700, acc: 0.875, loss: 0.360\n",
      "epoch: 4700, acc: 0.875, loss: 0.280\n",
      "epoch: 4700, acc: 0.656, loss: 0.534\n",
      "epoch: 4700, acc: 0.938, loss: 0.368\n",
      "epoch: 4700, acc: 0.750, loss: 0.575\n",
      "epoch: 4700, acc: 0.844, loss: 0.341\n",
      "epoch: 4700, acc: 0.812, loss: 0.410\n",
      "epoch: 4700, acc: 0.875, loss: 0.333\n",
      "epoch: 4700, acc: 0.812, loss: 0.392\n",
      "epoch: 4700, acc: 0.750, loss: 0.478\n",
      "epoch: 4700, acc: 0.906, loss: 0.249\n",
      "epoch: 4700, acc: 0.812, loss: 0.495\n",
      "epoch: 4700, acc: 0.688, loss: 0.557\n",
      "epoch: 4700, acc: 0.625, loss: 0.588\n",
      "epoch: 4700, acc: 0.857, loss: 0.351\n",
      "epoch: 4800, acc: 0.844, loss: 0.273\n",
      "epoch: 4800, acc: 0.875, loss: 0.329\n",
      "epoch: 4800, acc: 0.906, loss: 0.309\n",
      "epoch: 4800, acc: 0.781, loss: 0.516\n",
      "epoch: 4800, acc: 0.719, loss: 0.495\n",
      "epoch: 4800, acc: 0.812, loss: 0.432\n",
      "epoch: 4800, acc: 0.719, loss: 0.506\n",
      "epoch: 4800, acc: 0.812, loss: 0.429\n",
      "epoch: 4800, acc: 0.812, loss: 0.388\n",
      "epoch: 4800, acc: 0.812, loss: 0.466\n",
      "epoch: 4800, acc: 0.875, loss: 0.316\n",
      "epoch: 4800, acc: 0.938, loss: 0.236\n",
      "epoch: 4800, acc: 0.844, loss: 0.466\n",
      "epoch: 4800, acc: 0.812, loss: 0.418\n",
      "epoch: 4800, acc: 0.906, loss: 0.314\n",
      "epoch: 4800, acc: 0.906, loss: 0.242\n",
      "epoch: 4800, acc: 0.750, loss: 0.438\n",
      "epoch: 4800, acc: 0.688, loss: 0.745\n",
      "epoch: 4800, acc: 0.875, loss: 0.339\n",
      "epoch: 4800, acc: 0.719, loss: 0.551\n",
      "epoch: 4800, acc: 0.812, loss: 0.511\n",
      "epoch: 4800, acc: 0.750, loss: 0.459\n",
      "epoch: 4800, acc: 0.750, loss: 0.439\n",
      "epoch: 4800, acc: 0.812, loss: 0.439\n",
      "epoch: 4800, acc: 0.875, loss: 0.435\n",
      "epoch: 4800, acc: 0.844, loss: 0.293\n",
      "epoch: 4800, acc: 0.781, loss: 0.474\n",
      "epoch: 4800, acc: 0.812, loss: 0.397\n",
      "epoch: 4800, acc: 0.812, loss: 0.434\n",
      "epoch: 4800, acc: 0.875, loss: 0.319\n",
      "epoch: 4800, acc: 0.812, loss: 0.586\n",
      "epoch: 4800, acc: 0.812, loss: 0.362\n",
      "epoch: 4800, acc: 0.714, loss: 0.526\n",
      "epoch: 4900, acc: 0.875, loss: 0.341\n",
      "epoch: 4900, acc: 0.812, loss: 0.400\n",
      "epoch: 4900, acc: 0.812, loss: 0.497\n",
      "epoch: 4900, acc: 0.812, loss: 0.445\n",
      "epoch: 4900, acc: 0.719, loss: 0.431\n",
      "epoch: 4900, acc: 0.812, loss: 0.392\n",
      "epoch: 4900, acc: 0.719, loss: 0.493\n",
      "epoch: 4900, acc: 0.781, loss: 0.409\n",
      "epoch: 4900, acc: 0.844, loss: 0.327\n",
      "epoch: 4900, acc: 0.812, loss: 0.422\n",
      "epoch: 4900, acc: 0.625, loss: 0.772\n",
      "epoch: 4900, acc: 0.875, loss: 0.281\n",
      "epoch: 4900, acc: 0.875, loss: 0.384\n",
      "epoch: 4900, acc: 0.781, loss: 0.537\n",
      "epoch: 4900, acc: 0.781, loss: 0.512\n",
      "epoch: 4900, acc: 0.938, loss: 0.238\n",
      "epoch: 4900, acc: 0.875, loss: 0.391\n",
      "epoch: 4900, acc: 0.750, loss: 0.485\n",
      "epoch: 4900, acc: 0.906, loss: 0.227\n",
      "epoch: 4900, acc: 0.719, loss: 0.600\n",
      "epoch: 4900, acc: 0.781, loss: 0.462\n",
      "epoch: 4900, acc: 0.844, loss: 0.466\n",
      "epoch: 4900, acc: 0.875, loss: 0.359\n",
      "epoch: 4900, acc: 0.812, loss: 0.362\n",
      "epoch: 4900, acc: 0.812, loss: 0.436\n",
      "epoch: 4900, acc: 0.906, loss: 0.306\n",
      "epoch: 4900, acc: 0.719, loss: 0.582\n",
      "epoch: 4900, acc: 0.875, loss: 0.360\n",
      "epoch: 4900, acc: 0.844, loss: 0.293\n",
      "epoch: 4900, acc: 0.844, loss: 0.403\n",
      "epoch: 4900, acc: 0.844, loss: 0.437\n",
      "epoch: 4900, acc: 0.812, loss: 0.506\n",
      "epoch: 4900, acc: 0.952, loss: 0.242\n",
      "epoch: 5000, acc: 0.906, loss: 0.292\n",
      "epoch: 5000, acc: 0.812, loss: 0.425\n",
      "epoch: 5000, acc: 0.719, loss: 0.556\n",
      "epoch: 5000, acc: 0.938, loss: 0.329\n",
      "epoch: 5000, acc: 0.750, loss: 0.449\n",
      "epoch: 5000, acc: 0.812, loss: 0.379\n",
      "epoch: 5000, acc: 0.906, loss: 0.282\n",
      "epoch: 5000, acc: 0.844, loss: 0.331\n",
      "epoch: 5000, acc: 0.906, loss: 0.301\n",
      "epoch: 5000, acc: 0.781, loss: 0.632\n",
      "epoch: 5000, acc: 0.875, loss: 0.353\n",
      "epoch: 5000, acc: 0.906, loss: 0.363\n",
      "epoch: 5000, acc: 0.906, loss: 0.311\n",
      "epoch: 5000, acc: 0.844, loss: 0.355\n",
      "epoch: 5000, acc: 0.875, loss: 0.412\n",
      "epoch: 5000, acc: 0.812, loss: 0.321\n",
      "epoch: 5000, acc: 0.750, loss: 0.436\n",
      "epoch: 5000, acc: 0.688, loss: 0.551\n",
      "epoch: 5000, acc: 0.875, loss: 0.349\n",
      "epoch: 5000, acc: 0.812, loss: 0.389\n",
      "epoch: 5000, acc: 0.812, loss: 0.358\n",
      "epoch: 5000, acc: 0.875, loss: 0.347\n",
      "epoch: 5000, acc: 0.750, loss: 0.516\n",
      "epoch: 5000, acc: 0.781, loss: 0.664\n",
      "epoch: 5000, acc: 0.750, loss: 0.422\n",
      "epoch: 5000, acc: 0.750, loss: 0.527\n",
      "epoch: 5000, acc: 0.844, loss: 0.361\n",
      "epoch: 5000, acc: 0.844, loss: 0.381\n",
      "epoch: 5000, acc: 0.844, loss: 0.484\n",
      "epoch: 5000, acc: 0.812, loss: 0.530\n",
      "epoch: 5000, acc: 0.719, loss: 0.574\n",
      "epoch: 5000, acc: 0.875, loss: 0.299\n",
      "epoch: 5000, acc: 0.714, loss: 0.613\n",
      "epoch: 5100, acc: 0.781, loss: 0.443\n",
      "epoch: 5100, acc: 0.750, loss: 0.430\n",
      "epoch: 5100, acc: 0.875, loss: 0.313\n",
      "epoch: 5100, acc: 0.875, loss: 0.348\n",
      "epoch: 5100, acc: 0.812, loss: 0.472\n",
      "epoch: 5100, acc: 0.906, loss: 0.355\n",
      "epoch: 5100, acc: 0.656, loss: 0.718\n",
      "epoch: 5100, acc: 0.844, loss: 0.490\n",
      "epoch: 5100, acc: 0.875, loss: 0.355\n",
      "epoch: 5100, acc: 0.906, loss: 0.255\n",
      "epoch: 5100, acc: 0.688, loss: 0.585\n",
      "epoch: 5100, acc: 0.781, loss: 0.484\n",
      "epoch: 5100, acc: 0.781, loss: 0.482\n",
      "epoch: 5100, acc: 0.875, loss: 0.360\n",
      "epoch: 5100, acc: 0.812, loss: 0.347\n",
      "epoch: 5100, acc: 0.844, loss: 0.317\n",
      "epoch: 5100, acc: 0.844, loss: 0.374\n",
      "epoch: 5100, acc: 0.656, loss: 0.640\n",
      "epoch: 5100, acc: 0.688, loss: 0.532\n",
      "epoch: 5100, acc: 0.750, loss: 0.458\n",
      "epoch: 5100, acc: 0.781, loss: 0.510\n",
      "epoch: 5100, acc: 0.906, loss: 0.258\n",
      "epoch: 5100, acc: 0.938, loss: 0.251\n",
      "epoch: 5100, acc: 0.875, loss: 0.371\n",
      "epoch: 5100, acc: 0.875, loss: 0.423\n",
      "epoch: 5100, acc: 0.781, loss: 0.504\n",
      "epoch: 5100, acc: 0.906, loss: 0.259\n",
      "epoch: 5100, acc: 0.906, loss: 0.346\n",
      "epoch: 5100, acc: 0.781, loss: 0.547\n",
      "epoch: 5100, acc: 0.781, loss: 0.438\n",
      "epoch: 5100, acc: 0.906, loss: 0.341\n",
      "epoch: 5100, acc: 0.812, loss: 0.379\n",
      "epoch: 5100, acc: 0.714, loss: 0.546\n",
      "epoch: 5200, acc: 0.781, loss: 0.499\n",
      "epoch: 5200, acc: 0.781, loss: 0.491\n",
      "epoch: 5200, acc: 0.781, loss: 0.512\n",
      "epoch: 5200, acc: 0.875, loss: 0.301\n",
      "epoch: 5200, acc: 0.812, loss: 0.487\n",
      "epoch: 5200, acc: 0.875, loss: 0.352\n",
      "epoch: 5200, acc: 0.875, loss: 0.424\n",
      "epoch: 5200, acc: 0.906, loss: 0.295\n",
      "epoch: 5200, acc: 0.750, loss: 0.483\n",
      "epoch: 5200, acc: 0.781, loss: 0.432\n",
      "epoch: 5200, acc: 0.875, loss: 0.353\n",
      "epoch: 5200, acc: 0.812, loss: 0.358\n",
      "epoch: 5200, acc: 0.844, loss: 0.379\n",
      "epoch: 5200, acc: 0.812, loss: 0.368\n",
      "epoch: 5200, acc: 0.906, loss: 0.296\n",
      "epoch: 5200, acc: 0.812, loss: 0.384\n",
      "epoch: 5200, acc: 0.781, loss: 0.463\n",
      "epoch: 5200, acc: 0.812, loss: 0.411\n",
      "epoch: 5200, acc: 0.781, loss: 0.430\n",
      "epoch: 5200, acc: 0.875, loss: 0.306\n",
      "epoch: 5200, acc: 0.812, loss: 0.518\n",
      "epoch: 5200, acc: 0.750, loss: 0.539\n",
      "epoch: 5200, acc: 0.719, loss: 0.637\n",
      "epoch: 5200, acc: 0.750, loss: 0.490\n",
      "epoch: 5200, acc: 0.844, loss: 0.306\n",
      "epoch: 5200, acc: 0.719, loss: 0.697\n",
      "epoch: 5200, acc: 0.875, loss: 0.354\n",
      "epoch: 5200, acc: 0.812, loss: 0.369\n",
      "epoch: 5200, acc: 0.906, loss: 0.270\n",
      "epoch: 5200, acc: 0.844, loss: 0.383\n",
      "epoch: 5200, acc: 0.812, loss: 0.398\n",
      "epoch: 5200, acc: 0.812, loss: 0.399\n",
      "epoch: 5200, acc: 0.810, loss: 0.454\n",
      "epoch: 5300, acc: 0.781, loss: 0.453\n",
      "epoch: 5300, acc: 0.781, loss: 0.489\n",
      "epoch: 5300, acc: 0.906, loss: 0.270\n",
      "epoch: 5300, acc: 0.781, loss: 0.517\n",
      "epoch: 5300, acc: 0.812, loss: 0.487\n",
      "epoch: 5300, acc: 0.844, loss: 0.352\n",
      "epoch: 5300, acc: 0.844, loss: 0.318\n",
      "epoch: 5300, acc: 0.875, loss: 0.334\n",
      "epoch: 5300, acc: 0.906, loss: 0.304\n",
      "epoch: 5300, acc: 0.812, loss: 0.444\n",
      "epoch: 5300, acc: 0.750, loss: 0.508\n",
      "epoch: 5300, acc: 0.844, loss: 0.302\n",
      "epoch: 5300, acc: 0.750, loss: 0.478\n",
      "epoch: 5300, acc: 0.812, loss: 0.431\n",
      "epoch: 5300, acc: 0.906, loss: 0.354\n",
      "epoch: 5300, acc: 0.844, loss: 0.384\n",
      "epoch: 5300, acc: 0.906, loss: 0.301\n",
      "epoch: 5300, acc: 0.938, loss: 0.366\n",
      "epoch: 5300, acc: 0.906, loss: 0.369\n",
      "epoch: 5300, acc: 0.844, loss: 0.416\n",
      "epoch: 5300, acc: 0.688, loss: 0.498\n",
      "epoch: 5300, acc: 0.875, loss: 0.260\n",
      "epoch: 5300, acc: 0.875, loss: 0.288\n",
      "epoch: 5300, acc: 0.844, loss: 0.404\n",
      "epoch: 5300, acc: 0.719, loss: 0.431\n",
      "epoch: 5300, acc: 0.844, loss: 0.356\n",
      "epoch: 5300, acc: 0.781, loss: 0.626\n",
      "epoch: 5300, acc: 0.781, loss: 0.455\n",
      "epoch: 5300, acc: 0.812, loss: 0.497\n",
      "epoch: 5300, acc: 0.812, loss: 0.530\n",
      "epoch: 5300, acc: 0.812, loss: 0.378\n",
      "epoch: 5300, acc: 0.625, loss: 0.639\n",
      "epoch: 5300, acc: 0.667, loss: 0.642\n",
      "epoch: 5400, acc: 0.844, loss: 0.400\n",
      "epoch: 5400, acc: 0.875, loss: 0.317\n",
      "epoch: 5400, acc: 0.812, loss: 0.379\n",
      "epoch: 5400, acc: 0.812, loss: 0.473\n",
      "epoch: 5400, acc: 0.719, loss: 0.482\n",
      "epoch: 5400, acc: 0.812, loss: 0.439\n",
      "epoch: 5400, acc: 0.938, loss: 0.278\n",
      "epoch: 5400, acc: 0.844, loss: 0.420\n",
      "epoch: 5400, acc: 0.750, loss: 0.503\n",
      "epoch: 5400, acc: 0.781, loss: 0.409\n",
      "epoch: 5400, acc: 0.844, loss: 0.379\n",
      "epoch: 5400, acc: 0.781, loss: 0.458\n",
      "epoch: 5400, acc: 0.688, loss: 0.627\n",
      "epoch: 5400, acc: 0.875, loss: 0.275\n",
      "epoch: 5400, acc: 0.938, loss: 0.287\n",
      "epoch: 5400, acc: 0.719, loss: 0.500\n",
      "epoch: 5400, acc: 0.906, loss: 0.335\n",
      "epoch: 5400, acc: 0.750, loss: 0.647\n",
      "epoch: 5400, acc: 0.844, loss: 0.364\n",
      "epoch: 5400, acc: 0.844, loss: 0.320\n",
      "epoch: 5400, acc: 0.844, loss: 0.337\n",
      "epoch: 5400, acc: 0.844, loss: 0.436\n",
      "epoch: 5400, acc: 0.875, loss: 0.353\n",
      "epoch: 5400, acc: 0.750, loss: 0.434\n",
      "epoch: 5400, acc: 0.781, loss: 0.434\n",
      "epoch: 5400, acc: 0.812, loss: 0.428\n",
      "epoch: 5400, acc: 0.812, loss: 0.436\n",
      "epoch: 5400, acc: 0.812, loss: 0.345\n",
      "epoch: 5400, acc: 0.781, loss: 0.569\n",
      "epoch: 5400, acc: 0.812, loss: 0.419\n",
      "epoch: 5400, acc: 0.906, loss: 0.388\n",
      "epoch: 5400, acc: 0.844, loss: 0.481\n",
      "epoch: 5400, acc: 0.762, loss: 0.571\n",
      "epoch: 5500, acc: 0.812, loss: 0.296\n",
      "epoch: 5500, acc: 0.844, loss: 0.465\n",
      "epoch: 5500, acc: 0.625, loss: 0.791\n",
      "epoch: 5500, acc: 0.938, loss: 0.258\n",
      "epoch: 5500, acc: 0.844, loss: 0.308\n",
      "epoch: 5500, acc: 0.906, loss: 0.464\n",
      "epoch: 5500, acc: 0.719, loss: 0.433\n",
      "epoch: 5500, acc: 0.844, loss: 0.373\n",
      "epoch: 5500, acc: 0.750, loss: 0.439\n",
      "epoch: 5500, acc: 0.781, loss: 0.395\n",
      "epoch: 5500, acc: 0.844, loss: 0.376\n",
      "epoch: 5500, acc: 0.688, loss: 0.610\n",
      "epoch: 5500, acc: 0.844, loss: 0.538\n",
      "epoch: 5500, acc: 0.844, loss: 0.342\n",
      "epoch: 5500, acc: 0.875, loss: 0.344\n",
      "epoch: 5500, acc: 0.781, loss: 0.488\n",
      "epoch: 5500, acc: 0.781, loss: 0.660\n",
      "epoch: 5500, acc: 0.812, loss: 0.424\n",
      "epoch: 5500, acc: 0.750, loss: 0.581\n",
      "epoch: 5500, acc: 0.812, loss: 0.416\n",
      "epoch: 5500, acc: 0.812, loss: 0.399\n",
      "epoch: 5500, acc: 0.844, loss: 0.365\n",
      "epoch: 5500, acc: 0.844, loss: 0.436\n",
      "epoch: 5500, acc: 0.875, loss: 0.348\n",
      "epoch: 5500, acc: 0.906, loss: 0.267\n",
      "epoch: 5500, acc: 0.812, loss: 0.438\n",
      "epoch: 5500, acc: 0.719, loss: 0.579\n",
      "epoch: 5500, acc: 0.875, loss: 0.310\n",
      "epoch: 5500, acc: 0.938, loss: 0.225\n",
      "epoch: 5500, acc: 0.875, loss: 0.274\n",
      "epoch: 5500, acc: 0.781, loss: 0.461\n",
      "epoch: 5500, acc: 0.906, loss: 0.240\n",
      "epoch: 5500, acc: 0.762, loss: 0.624\n",
      "epoch: 5600, acc: 0.844, loss: 0.341\n",
      "epoch: 5600, acc: 0.750, loss: 0.554\n",
      "epoch: 5600, acc: 0.750, loss: 0.553\n",
      "epoch: 5600, acc: 0.812, loss: 0.384\n",
      "epoch: 5600, acc: 0.781, loss: 0.421\n",
      "epoch: 5600, acc: 0.812, loss: 0.458\n",
      "epoch: 5600, acc: 0.906, loss: 0.287\n",
      "epoch: 5600, acc: 0.844, loss: 0.363\n",
      "epoch: 5600, acc: 0.812, loss: 0.431\n",
      "epoch: 5600, acc: 0.938, loss: 0.271\n",
      "epoch: 5600, acc: 0.781, loss: 0.446\n",
      "epoch: 5600, acc: 0.812, loss: 0.487\n",
      "epoch: 5600, acc: 0.781, loss: 0.532\n",
      "epoch: 5600, acc: 0.750, loss: 0.500\n",
      "epoch: 5600, acc: 0.719, loss: 0.788\n",
      "epoch: 5600, acc: 0.812, loss: 0.476\n",
      "epoch: 5600, acc: 0.781, loss: 0.411\n",
      "epoch: 5600, acc: 0.875, loss: 0.359\n",
      "epoch: 5600, acc: 0.812, loss: 0.414\n",
      "epoch: 5600, acc: 0.844, loss: 0.488\n",
      "epoch: 5600, acc: 0.750, loss: 0.527\n",
      "epoch: 5600, acc: 0.969, loss: 0.215\n",
      "epoch: 5600, acc: 0.906, loss: 0.350\n",
      "epoch: 5600, acc: 0.875, loss: 0.324\n",
      "epoch: 5600, acc: 0.719, loss: 0.432\n",
      "epoch: 5600, acc: 0.781, loss: 0.516\n",
      "epoch: 5600, acc: 0.750, loss: 0.448\n",
      "epoch: 5600, acc: 0.812, loss: 0.349\n",
      "epoch: 5600, acc: 0.906, loss: 0.303\n",
      "epoch: 5600, acc: 0.812, loss: 0.397\n",
      "epoch: 5600, acc: 0.875, loss: 0.338\n",
      "epoch: 5600, acc: 0.812, loss: 0.380\n",
      "epoch: 5600, acc: 0.905, loss: 0.245\n",
      "epoch: 5700, acc: 0.781, loss: 0.543\n",
      "epoch: 5700, acc: 0.812, loss: 0.508\n",
      "epoch: 5700, acc: 0.719, loss: 0.429\n",
      "epoch: 5700, acc: 0.844, loss: 0.356\n",
      "epoch: 5700, acc: 0.750, loss: 0.514\n",
      "epoch: 5700, acc: 0.875, loss: 0.302\n",
      "epoch: 5700, acc: 0.875, loss: 0.286\n",
      "epoch: 5700, acc: 0.719, loss: 0.594\n",
      "epoch: 5700, acc: 0.781, loss: 0.582\n",
      "epoch: 5700, acc: 0.875, loss: 0.292\n",
      "epoch: 5700, acc: 0.781, loss: 0.579\n",
      "epoch: 5700, acc: 0.750, loss: 0.525\n",
      "epoch: 5700, acc: 0.812, loss: 0.529\n",
      "epoch: 5700, acc: 0.844, loss: 0.295\n",
      "epoch: 5700, acc: 0.812, loss: 0.360\n",
      "epoch: 5700, acc: 0.812, loss: 0.408\n",
      "epoch: 5700, acc: 0.844, loss: 0.352\n",
      "epoch: 5700, acc: 0.844, loss: 0.349\n",
      "epoch: 5700, acc: 0.781, loss: 0.385\n",
      "epoch: 5700, acc: 0.781, loss: 0.414\n",
      "epoch: 5700, acc: 0.844, loss: 0.323\n",
      "epoch: 5700, acc: 0.812, loss: 0.589\n",
      "epoch: 5700, acc: 0.656, loss: 0.641\n",
      "epoch: 5700, acc: 0.781, loss: 0.550\n",
      "epoch: 5700, acc: 0.938, loss: 0.274\n",
      "epoch: 5700, acc: 0.812, loss: 0.420\n",
      "epoch: 5700, acc: 0.812, loss: 0.499\n",
      "epoch: 5700, acc: 0.906, loss: 0.314\n",
      "epoch: 5700, acc: 0.812, loss: 0.404\n",
      "epoch: 5700, acc: 0.906, loss: 0.267\n",
      "epoch: 5700, acc: 0.781, loss: 0.423\n",
      "epoch: 5700, acc: 0.969, loss: 0.189\n",
      "epoch: 5700, acc: 0.810, loss: 0.328\n",
      "epoch: 5800, acc: 0.656, loss: 0.589\n",
      "epoch: 5800, acc: 0.750, loss: 0.495\n",
      "epoch: 5800, acc: 0.812, loss: 0.466\n",
      "epoch: 5800, acc: 0.844, loss: 0.339\n",
      "epoch: 5800, acc: 0.812, loss: 0.448\n",
      "epoch: 5800, acc: 0.688, loss: 0.602\n",
      "epoch: 5800, acc: 0.750, loss: 0.403\n",
      "epoch: 5800, acc: 0.906, loss: 0.251\n",
      "epoch: 5800, acc: 0.812, loss: 0.312\n",
      "epoch: 5800, acc: 0.781, loss: 0.435\n",
      "epoch: 5800, acc: 0.719, loss: 0.475\n",
      "epoch: 5800, acc: 0.875, loss: 0.321\n",
      "epoch: 5800, acc: 0.781, loss: 0.485\n",
      "epoch: 5800, acc: 0.875, loss: 0.348\n",
      "epoch: 5800, acc: 0.875, loss: 0.460\n",
      "epoch: 5800, acc: 0.781, loss: 0.467\n",
      "epoch: 5800, acc: 0.938, loss: 0.189\n",
      "epoch: 5800, acc: 0.844, loss: 0.419\n",
      "epoch: 5800, acc: 0.719, loss: 0.564\n",
      "epoch: 5800, acc: 0.969, loss: 0.328\n",
      "epoch: 5800, acc: 0.781, loss: 0.459\n",
      "epoch: 5800, acc: 0.875, loss: 0.362\n",
      "epoch: 5800, acc: 0.844, loss: 0.413\n",
      "epoch: 5800, acc: 0.719, loss: 0.558\n",
      "epoch: 5800, acc: 0.781, loss: 0.490\n",
      "epoch: 5800, acc: 0.812, loss: 0.455\n",
      "epoch: 5800, acc: 0.781, loss: 0.367\n",
      "epoch: 5800, acc: 0.906, loss: 0.272\n",
      "epoch: 5800, acc: 0.812, loss: 0.452\n",
      "epoch: 5800, acc: 0.844, loss: 0.401\n",
      "epoch: 5800, acc: 0.812, loss: 0.543\n",
      "epoch: 5800, acc: 0.875, loss: 0.311\n",
      "epoch: 5800, acc: 0.857, loss: 0.295\n",
      "epoch: 5900, acc: 0.719, loss: 0.585\n",
      "epoch: 5900, acc: 0.719, loss: 0.524\n",
      "epoch: 5900, acc: 0.781, loss: 0.462\n",
      "epoch: 5900, acc: 0.750, loss: 0.620\n",
      "epoch: 5900, acc: 0.688, loss: 0.529\n",
      "epoch: 5900, acc: 0.812, loss: 0.398\n",
      "epoch: 5900, acc: 0.906, loss: 0.264\n",
      "epoch: 5900, acc: 0.844, loss: 0.433\n",
      "epoch: 5900, acc: 0.906, loss: 0.314\n",
      "epoch: 5900, acc: 0.844, loss: 0.369\n",
      "epoch: 5900, acc: 0.812, loss: 0.545\n",
      "epoch: 5900, acc: 0.844, loss: 0.488\n",
      "epoch: 5900, acc: 0.844, loss: 0.343\n",
      "epoch: 5900, acc: 0.812, loss: 0.448\n",
      "epoch: 5900, acc: 0.812, loss: 0.427\n",
      "epoch: 5900, acc: 0.875, loss: 0.330\n",
      "epoch: 5900, acc: 0.969, loss: 0.189\n",
      "epoch: 5900, acc: 0.594, loss: 0.706\n",
      "epoch: 5900, acc: 0.812, loss: 0.527\n",
      "epoch: 5900, acc: 0.719, loss: 0.636\n",
      "epoch: 5900, acc: 0.875, loss: 0.367\n",
      "epoch: 5900, acc: 0.750, loss: 0.405\n",
      "epoch: 5900, acc: 0.938, loss: 0.304\n",
      "epoch: 5900, acc: 0.812, loss: 0.372\n",
      "epoch: 5900, acc: 0.750, loss: 0.413\n",
      "epoch: 5900, acc: 0.812, loss: 0.484\n",
      "epoch: 5900, acc: 0.906, loss: 0.397\n",
      "epoch: 5900, acc: 0.938, loss: 0.235\n",
      "epoch: 5900, acc: 0.844, loss: 0.419\n",
      "epoch: 5900, acc: 0.938, loss: 0.271\n",
      "epoch: 5900, acc: 0.781, loss: 0.412\n",
      "epoch: 5900, acc: 0.844, loss: 0.355\n",
      "epoch: 5900, acc: 0.952, loss: 0.185\n",
      "epoch: 6000, acc: 0.781, loss: 0.505\n",
      "epoch: 6000, acc: 0.812, loss: 0.348\n",
      "epoch: 6000, acc: 0.906, loss: 0.193\n",
      "epoch: 6000, acc: 0.906, loss: 0.276\n",
      "epoch: 6000, acc: 0.844, loss: 0.472\n",
      "epoch: 6000, acc: 0.750, loss: 0.438\n",
      "epoch: 6000, acc: 0.875, loss: 0.450\n",
      "epoch: 6000, acc: 0.906, loss: 0.334\n",
      "epoch: 6000, acc: 0.906, loss: 0.307\n",
      "epoch: 6000, acc: 0.844, loss: 0.299\n",
      "epoch: 6000, acc: 0.812, loss: 0.401\n",
      "epoch: 6000, acc: 0.750, loss: 0.547\n",
      "epoch: 6000, acc: 0.812, loss: 0.531\n",
      "epoch: 6000, acc: 0.812, loss: 0.452\n",
      "epoch: 6000, acc: 0.656, loss: 0.653\n",
      "epoch: 6000, acc: 0.812, loss: 0.393\n",
      "epoch: 6000, acc: 0.875, loss: 0.348\n",
      "epoch: 6000, acc: 0.812, loss: 0.441\n",
      "epoch: 6000, acc: 0.719, loss: 0.541\n",
      "epoch: 6000, acc: 0.812, loss: 0.395\n",
      "epoch: 6000, acc: 0.688, loss: 0.675\n",
      "epoch: 6000, acc: 0.812, loss: 0.504\n",
      "epoch: 6000, acc: 0.844, loss: 0.369\n",
      "epoch: 6000, acc: 0.844, loss: 0.358\n",
      "epoch: 6000, acc: 0.969, loss: 0.272\n",
      "epoch: 6000, acc: 0.719, loss: 0.538\n",
      "epoch: 6000, acc: 0.750, loss: 0.445\n",
      "epoch: 6000, acc: 0.812, loss: 0.433\n",
      "epoch: 6000, acc: 0.875, loss: 0.433\n",
      "epoch: 6000, acc: 0.844, loss: 0.364\n",
      "epoch: 6000, acc: 0.875, loss: 0.272\n",
      "epoch: 6000, acc: 0.750, loss: 0.490\n",
      "epoch: 6000, acc: 0.857, loss: 0.321\n",
      "epoch: 6100, acc: 0.875, loss: 0.323\n",
      "epoch: 6100, acc: 0.750, loss: 0.501\n",
      "epoch: 6100, acc: 0.906, loss: 0.302\n",
      "epoch: 6100, acc: 0.781, loss: 0.417\n",
      "epoch: 6100, acc: 0.812, loss: 0.520\n",
      "epoch: 6100, acc: 0.750, loss: 0.520\n",
      "epoch: 6100, acc: 0.844, loss: 0.385\n",
      "epoch: 6100, acc: 0.812, loss: 0.420\n",
      "epoch: 6100, acc: 0.812, loss: 0.363\n",
      "epoch: 6100, acc: 0.844, loss: 0.494\n",
      "epoch: 6100, acc: 0.906, loss: 0.300\n",
      "epoch: 6100, acc: 0.812, loss: 0.338\n",
      "epoch: 6100, acc: 0.875, loss: 0.369\n",
      "epoch: 6100, acc: 0.812, loss: 0.433\n",
      "epoch: 6100, acc: 0.750, loss: 0.545\n",
      "epoch: 6100, acc: 0.844, loss: 0.344\n",
      "epoch: 6100, acc: 0.719, loss: 0.519\n",
      "epoch: 6100, acc: 0.906, loss: 0.253\n",
      "epoch: 6100, acc: 0.844, loss: 0.329\n",
      "epoch: 6100, acc: 0.875, loss: 0.423\n",
      "epoch: 6100, acc: 0.844, loss: 0.376\n",
      "epoch: 6100, acc: 0.719, loss: 0.495\n",
      "epoch: 6100, acc: 0.875, loss: 0.327\n",
      "epoch: 6100, acc: 0.781, loss: 0.474\n",
      "epoch: 6100, acc: 0.750, loss: 0.592\n",
      "epoch: 6100, acc: 0.906, loss: 0.237\n",
      "epoch: 6100, acc: 0.875, loss: 0.324\n",
      "epoch: 6100, acc: 0.750, loss: 0.568\n",
      "epoch: 6100, acc: 0.750, loss: 0.633\n",
      "epoch: 6100, acc: 0.812, loss: 0.372\n",
      "epoch: 6100, acc: 0.656, loss: 0.589\n",
      "epoch: 6100, acc: 0.844, loss: 0.348\n",
      "epoch: 6100, acc: 0.905, loss: 0.371\n",
      "epoch: 6200, acc: 0.844, loss: 0.345\n",
      "epoch: 6200, acc: 0.844, loss: 0.368\n",
      "epoch: 6200, acc: 0.812, loss: 0.410\n",
      "epoch: 6200, acc: 0.812, loss: 0.441\n",
      "epoch: 6200, acc: 0.688, loss: 0.481\n",
      "epoch: 6200, acc: 0.750, loss: 0.473\n",
      "epoch: 6200, acc: 0.938, loss: 0.372\n",
      "epoch: 6200, acc: 0.844, loss: 0.379\n",
      "epoch: 6200, acc: 0.906, loss: 0.457\n",
      "epoch: 6200, acc: 0.906, loss: 0.345\n",
      "epoch: 6200, acc: 0.906, loss: 0.304\n",
      "epoch: 6200, acc: 0.875, loss: 0.331\n",
      "epoch: 6200, acc: 0.875, loss: 0.328\n",
      "epoch: 6200, acc: 0.844, loss: 0.324\n",
      "epoch: 6200, acc: 0.844, loss: 0.456\n",
      "epoch: 6200, acc: 0.906, loss: 0.301\n",
      "epoch: 6200, acc: 0.906, loss: 0.290\n",
      "epoch: 6200, acc: 0.719, loss: 0.409\n",
      "epoch: 6200, acc: 0.719, loss: 0.518\n",
      "epoch: 6200, acc: 0.812, loss: 0.515\n",
      "epoch: 6200, acc: 0.906, loss: 0.229\n",
      "epoch: 6200, acc: 0.688, loss: 0.583\n",
      "epoch: 6200, acc: 0.750, loss: 0.428\n",
      "epoch: 6200, acc: 0.812, loss: 0.447\n",
      "epoch: 6200, acc: 0.844, loss: 0.356\n",
      "epoch: 6200, acc: 0.844, loss: 0.578\n",
      "epoch: 6200, acc: 0.594, loss: 0.695\n",
      "epoch: 6200, acc: 0.688, loss: 0.749\n",
      "epoch: 6200, acc: 0.844, loss: 0.451\n",
      "epoch: 6200, acc: 0.906, loss: 0.279\n",
      "epoch: 6200, acc: 0.719, loss: 0.454\n",
      "epoch: 6200, acc: 0.812, loss: 0.393\n",
      "epoch: 6200, acc: 0.905, loss: 0.330\n",
      "epoch: 6300, acc: 0.844, loss: 0.334\n",
      "epoch: 6300, acc: 0.875, loss: 0.496\n",
      "epoch: 6300, acc: 0.719, loss: 0.538\n",
      "epoch: 6300, acc: 0.781, loss: 0.486\n",
      "epoch: 6300, acc: 0.875, loss: 0.392\n",
      "epoch: 6300, acc: 0.750, loss: 0.471\n",
      "epoch: 6300, acc: 0.875, loss: 0.354\n",
      "epoch: 6300, acc: 0.750, loss: 0.364\n",
      "epoch: 6300, acc: 0.844, loss: 0.497\n",
      "epoch: 6300, acc: 0.812, loss: 0.443\n",
      "epoch: 6300, acc: 0.875, loss: 0.423\n",
      "epoch: 6300, acc: 0.906, loss: 0.281\n",
      "epoch: 6300, acc: 0.844, loss: 0.349\n",
      "epoch: 6300, acc: 0.812, loss: 0.397\n",
      "epoch: 6300, acc: 0.844, loss: 0.481\n",
      "epoch: 6300, acc: 0.812, loss: 0.406\n",
      "epoch: 6300, acc: 0.906, loss: 0.225\n",
      "epoch: 6300, acc: 0.844, loss: 0.368\n",
      "epoch: 6300, acc: 0.844, loss: 0.414\n",
      "epoch: 6300, acc: 0.750, loss: 0.452\n",
      "epoch: 6300, acc: 0.844, loss: 0.441\n",
      "epoch: 6300, acc: 0.688, loss: 0.568\n",
      "epoch: 6300, acc: 0.656, loss: 0.598\n",
      "epoch: 6300, acc: 0.781, loss: 0.541\n",
      "epoch: 6300, acc: 0.844, loss: 0.392\n",
      "epoch: 6300, acc: 0.906, loss: 0.305\n",
      "epoch: 6300, acc: 0.812, loss: 0.494\n",
      "epoch: 6300, acc: 0.969, loss: 0.242\n",
      "epoch: 6300, acc: 0.812, loss: 0.439\n",
      "epoch: 6300, acc: 0.812, loss: 0.350\n",
      "epoch: 6300, acc: 0.750, loss: 0.487\n",
      "epoch: 6300, acc: 0.875, loss: 0.337\n",
      "epoch: 6300, acc: 0.714, loss: 0.503\n",
      "epoch: 6400, acc: 0.719, loss: 0.444\n",
      "epoch: 6400, acc: 0.844, loss: 0.400\n",
      "epoch: 6400, acc: 0.812, loss: 0.440\n",
      "epoch: 6400, acc: 0.844, loss: 0.387\n",
      "epoch: 6400, acc: 0.812, loss: 0.374\n",
      "epoch: 6400, acc: 0.719, loss: 0.544\n",
      "epoch: 6400, acc: 0.844, loss: 0.318\n",
      "epoch: 6400, acc: 0.906, loss: 0.207\n",
      "epoch: 6400, acc: 0.812, loss: 0.512\n",
      "epoch: 6400, acc: 0.781, loss: 0.492\n",
      "epoch: 6400, acc: 0.781, loss: 0.508\n",
      "epoch: 6400, acc: 0.844, loss: 0.385\n",
      "epoch: 6400, acc: 0.781, loss: 0.447\n",
      "epoch: 6400, acc: 0.812, loss: 0.470\n",
      "epoch: 6400, acc: 0.844, loss: 0.360\n",
      "epoch: 6400, acc: 0.938, loss: 0.268\n",
      "epoch: 6400, acc: 0.812, loss: 0.471\n",
      "epoch: 6400, acc: 0.812, loss: 0.475\n",
      "epoch: 6400, acc: 0.781, loss: 0.477\n",
      "epoch: 6400, acc: 0.844, loss: 0.345\n",
      "epoch: 6400, acc: 0.875, loss: 0.304\n",
      "epoch: 6400, acc: 0.875, loss: 0.348\n",
      "epoch: 6400, acc: 0.844, loss: 0.486\n",
      "epoch: 6400, acc: 0.844, loss: 0.343\n",
      "epoch: 6400, acc: 0.906, loss: 0.343\n",
      "epoch: 6400, acc: 0.875, loss: 0.319\n",
      "epoch: 6400, acc: 0.719, loss: 0.628\n",
      "epoch: 6400, acc: 0.812, loss: 0.505\n",
      "epoch: 6400, acc: 0.656, loss: 0.602\n",
      "epoch: 6400, acc: 0.750, loss: 0.493\n",
      "epoch: 6400, acc: 0.812, loss: 0.360\n",
      "epoch: 6400, acc: 0.781, loss: 0.472\n",
      "epoch: 6400, acc: 0.905, loss: 0.265\n",
      "epoch: 6500, acc: 0.844, loss: 0.458\n",
      "epoch: 6500, acc: 0.844, loss: 0.303\n",
      "epoch: 6500, acc: 0.844, loss: 0.399\n",
      "epoch: 6500, acc: 0.750, loss: 0.575\n",
      "epoch: 6500, acc: 0.781, loss: 0.476\n",
      "epoch: 6500, acc: 0.812, loss: 0.492\n",
      "epoch: 6500, acc: 0.719, loss: 0.442\n",
      "epoch: 6500, acc: 0.875, loss: 0.327\n",
      "epoch: 6500, acc: 0.781, loss: 0.609\n",
      "epoch: 6500, acc: 0.844, loss: 0.408\n",
      "epoch: 6500, acc: 0.875, loss: 0.260\n",
      "epoch: 6500, acc: 0.844, loss: 0.507\n",
      "epoch: 6500, acc: 0.875, loss: 0.392\n",
      "epoch: 6500, acc: 0.812, loss: 0.325\n",
      "epoch: 6500, acc: 0.938, loss: 0.276\n",
      "epoch: 6500, acc: 0.906, loss: 0.345\n",
      "epoch: 6500, acc: 0.875, loss: 0.450\n",
      "epoch: 6500, acc: 0.906, loss: 0.307\n",
      "epoch: 6500, acc: 0.781, loss: 0.394\n",
      "epoch: 6500, acc: 0.781, loss: 0.456\n",
      "epoch: 6500, acc: 0.656, loss: 0.563\n",
      "epoch: 6500, acc: 0.719, loss: 0.627\n",
      "epoch: 6500, acc: 0.844, loss: 0.442\n",
      "epoch: 6500, acc: 0.844, loss: 0.308\n",
      "epoch: 6500, acc: 0.906, loss: 0.327\n",
      "epoch: 6500, acc: 0.812, loss: 0.371\n",
      "epoch: 6500, acc: 0.781, loss: 0.455\n",
      "epoch: 6500, acc: 0.812, loss: 0.374\n",
      "epoch: 6500, acc: 0.844, loss: 0.444\n",
      "epoch: 6500, acc: 0.875, loss: 0.334\n",
      "epoch: 6500, acc: 0.812, loss: 0.363\n",
      "epoch: 6500, acc: 0.719, loss: 0.496\n",
      "epoch: 6500, acc: 0.619, loss: 0.671\n",
      "epoch: 6600, acc: 0.938, loss: 0.303\n",
      "epoch: 6600, acc: 0.750, loss: 0.579\n",
      "epoch: 6600, acc: 0.719, loss: 0.728\n",
      "epoch: 6600, acc: 0.875, loss: 0.323\n",
      "epoch: 6600, acc: 0.906, loss: 0.455\n",
      "epoch: 6600, acc: 0.781, loss: 0.452\n",
      "epoch: 6600, acc: 0.812, loss: 0.456\n",
      "epoch: 6600, acc: 0.719, loss: 0.569\n",
      "epoch: 6600, acc: 0.812, loss: 0.380\n",
      "epoch: 6600, acc: 0.906, loss: 0.306\n",
      "epoch: 6600, acc: 0.906, loss: 0.320\n",
      "epoch: 6600, acc: 0.812, loss: 0.416\n",
      "epoch: 6600, acc: 0.812, loss: 0.365\n",
      "epoch: 6600, acc: 0.875, loss: 0.327\n",
      "epoch: 6600, acc: 0.812, loss: 0.412\n",
      "epoch: 6600, acc: 0.969, loss: 0.251\n",
      "epoch: 6600, acc: 0.844, loss: 0.414\n",
      "epoch: 6600, acc: 0.906, loss: 0.220\n",
      "epoch: 6600, acc: 0.875, loss: 0.322\n",
      "epoch: 6600, acc: 0.688, loss: 0.503\n",
      "epoch: 6600, acc: 0.719, loss: 0.449\n",
      "epoch: 6600, acc: 0.844, loss: 0.344\n",
      "epoch: 6600, acc: 0.750, loss: 0.473\n",
      "epoch: 6600, acc: 0.812, loss: 0.371\n",
      "epoch: 6600, acc: 0.875, loss: 0.401\n",
      "epoch: 6600, acc: 0.781, loss: 0.497\n",
      "epoch: 6600, acc: 0.844, loss: 0.407\n",
      "epoch: 6600, acc: 0.750, loss: 0.535\n",
      "epoch: 6600, acc: 0.781, loss: 0.414\n",
      "epoch: 6600, acc: 0.844, loss: 0.472\n",
      "epoch: 6600, acc: 0.781, loss: 0.508\n",
      "epoch: 6600, acc: 0.812, loss: 0.392\n",
      "epoch: 6600, acc: 0.667, loss: 0.585\n",
      "epoch: 6700, acc: 0.688, loss: 0.575\n",
      "epoch: 6700, acc: 0.906, loss: 0.339\n",
      "epoch: 6700, acc: 0.906, loss: 0.433\n",
      "epoch: 6700, acc: 0.875, loss: 0.352\n",
      "epoch: 6700, acc: 0.875, loss: 0.297\n",
      "epoch: 6700, acc: 0.875, loss: 0.399\n",
      "epoch: 6700, acc: 0.812, loss: 0.441\n",
      "epoch: 6700, acc: 0.719, loss: 0.588\n",
      "epoch: 6700, acc: 0.656, loss: 0.652\n",
      "epoch: 6700, acc: 0.781, loss: 0.428\n",
      "epoch: 6700, acc: 0.781, loss: 0.402\n",
      "epoch: 6700, acc: 0.781, loss: 0.502\n",
      "epoch: 6700, acc: 0.750, loss: 0.383\n",
      "epoch: 6700, acc: 0.719, loss: 0.527\n",
      "epoch: 6700, acc: 0.875, loss: 0.301\n",
      "epoch: 6700, acc: 0.875, loss: 0.272\n",
      "epoch: 6700, acc: 0.812, loss: 0.384\n",
      "epoch: 6700, acc: 0.781, loss: 0.456\n",
      "epoch: 6700, acc: 0.844, loss: 0.403\n",
      "epoch: 6700, acc: 0.938, loss: 0.217\n",
      "epoch: 6700, acc: 0.844, loss: 0.415\n",
      "epoch: 6700, acc: 0.812, loss: 0.406\n",
      "epoch: 6700, acc: 0.844, loss: 0.388\n",
      "epoch: 6700, acc: 0.750, loss: 0.441\n",
      "epoch: 6700, acc: 0.875, loss: 0.399\n",
      "epoch: 6700, acc: 0.906, loss: 0.262\n",
      "epoch: 6700, acc: 0.812, loss: 0.523\n",
      "epoch: 6700, acc: 0.812, loss: 0.444\n",
      "epoch: 6700, acc: 0.750, loss: 0.501\n",
      "epoch: 6700, acc: 0.875, loss: 0.330\n",
      "epoch: 6700, acc: 0.688, loss: 0.611\n",
      "epoch: 6700, acc: 0.812, loss: 0.490\n",
      "epoch: 6700, acc: 0.905, loss: 0.269\n",
      "epoch: 6800, acc: 0.812, loss: 0.418\n",
      "epoch: 6800, acc: 0.781, loss: 0.430\n",
      "epoch: 6800, acc: 0.875, loss: 0.362\n",
      "epoch: 6800, acc: 0.875, loss: 0.335\n",
      "epoch: 6800, acc: 0.781, loss: 0.441\n",
      "epoch: 6800, acc: 0.750, loss: 0.576\n",
      "epoch: 6800, acc: 0.750, loss: 0.383\n",
      "epoch: 6800, acc: 0.812, loss: 0.542\n",
      "epoch: 6800, acc: 0.844, loss: 0.380\n",
      "epoch: 6800, acc: 0.906, loss: 0.281\n",
      "epoch: 6800, acc: 0.812, loss: 0.363\n",
      "epoch: 6800, acc: 0.938, loss: 0.274\n",
      "epoch: 6800, acc: 0.906, loss: 0.281\n",
      "epoch: 6800, acc: 0.781, loss: 0.551\n",
      "epoch: 6800, acc: 0.875, loss: 0.320\n",
      "epoch: 6800, acc: 0.750, loss: 0.458\n",
      "epoch: 6800, acc: 0.844, loss: 0.504\n",
      "epoch: 6800, acc: 0.719, loss: 0.537\n",
      "epoch: 6800, acc: 0.781, loss: 0.509\n",
      "epoch: 6800, acc: 0.625, loss: 0.612\n",
      "epoch: 6800, acc: 0.781, loss: 0.498\n",
      "epoch: 6800, acc: 0.844, loss: 0.487\n",
      "epoch: 6800, acc: 0.844, loss: 0.340\n",
      "epoch: 6800, acc: 0.875, loss: 0.351\n",
      "epoch: 6800, acc: 0.719, loss: 0.630\n",
      "epoch: 6800, acc: 0.812, loss: 0.400\n",
      "epoch: 6800, acc: 0.875, loss: 0.383\n",
      "epoch: 6800, acc: 0.906, loss: 0.330\n",
      "epoch: 6800, acc: 0.781, loss: 0.441\n",
      "epoch: 6800, acc: 0.875, loss: 0.355\n",
      "epoch: 6800, acc: 0.844, loss: 0.324\n",
      "epoch: 6800, acc: 0.906, loss: 0.324\n",
      "epoch: 6800, acc: 0.762, loss: 0.455\n",
      "epoch: 6900, acc: 0.875, loss: 0.350\n",
      "epoch: 6900, acc: 0.844, loss: 0.389\n",
      "epoch: 6900, acc: 0.750, loss: 0.523\n",
      "epoch: 6900, acc: 0.781, loss: 0.411\n",
      "epoch: 6900, acc: 0.688, loss: 0.498\n",
      "epoch: 6900, acc: 0.875, loss: 0.280\n",
      "epoch: 6900, acc: 0.812, loss: 0.410\n",
      "epoch: 6900, acc: 0.875, loss: 0.348\n",
      "epoch: 6900, acc: 0.812, loss: 0.390\n",
      "epoch: 6900, acc: 0.750, loss: 0.619\n",
      "epoch: 6900, acc: 0.750, loss: 0.819\n",
      "epoch: 6900, acc: 0.844, loss: 0.320\n",
      "epoch: 6900, acc: 0.781, loss: 0.508\n",
      "epoch: 6900, acc: 0.938, loss: 0.250\n",
      "epoch: 6900, acc: 0.812, loss: 0.367\n",
      "epoch: 6900, acc: 0.906, loss: 0.450\n",
      "epoch: 6900, acc: 0.750, loss: 0.532\n",
      "epoch: 6900, acc: 0.844, loss: 0.323\n",
      "epoch: 6900, acc: 0.781, loss: 0.488\n",
      "epoch: 6900, acc: 0.938, loss: 0.221\n",
      "epoch: 6900, acc: 0.812, loss: 0.349\n",
      "epoch: 6900, acc: 0.781, loss: 0.532\n",
      "epoch: 6900, acc: 0.719, loss: 0.466\n",
      "epoch: 6900, acc: 0.844, loss: 0.306\n",
      "epoch: 6900, acc: 0.844, loss: 0.402\n",
      "epoch: 6900, acc: 0.875, loss: 0.348\n",
      "epoch: 6900, acc: 0.719, loss: 0.466\n",
      "epoch: 6900, acc: 0.750, loss: 0.542\n",
      "epoch: 6900, acc: 0.906, loss: 0.371\n",
      "epoch: 6900, acc: 0.844, loss: 0.352\n",
      "epoch: 6900, acc: 0.812, loss: 0.445\n",
      "epoch: 6900, acc: 0.812, loss: 0.448\n",
      "epoch: 6900, acc: 0.905, loss: 0.315\n",
      "epoch: 7000, acc: 0.875, loss: 0.276\n",
      "epoch: 7000, acc: 0.781, loss: 0.512\n",
      "epoch: 7000, acc: 0.875, loss: 0.395\n",
      "epoch: 7000, acc: 0.938, loss: 0.276\n",
      "epoch: 7000, acc: 0.875, loss: 0.369\n",
      "epoch: 7000, acc: 0.781, loss: 0.450\n",
      "epoch: 7000, acc: 0.750, loss: 0.436\n",
      "epoch: 7000, acc: 0.719, loss: 0.587\n",
      "epoch: 7000, acc: 0.812, loss: 0.410\n",
      "epoch: 7000, acc: 0.812, loss: 0.414\n",
      "epoch: 7000, acc: 0.750, loss: 0.464\n",
      "epoch: 7000, acc: 0.875, loss: 0.307\n",
      "epoch: 7000, acc: 0.812, loss: 0.476\n",
      "epoch: 7000, acc: 0.781, loss: 0.450\n",
      "epoch: 7000, acc: 0.688, loss: 0.509\n",
      "epoch: 7000, acc: 0.656, loss: 0.755\n",
      "epoch: 7000, acc: 0.812, loss: 0.316\n",
      "epoch: 7000, acc: 0.812, loss: 0.467\n",
      "epoch: 7000, acc: 0.781, loss: 0.453\n",
      "epoch: 7000, acc: 0.781, loss: 0.507\n",
      "epoch: 7000, acc: 0.906, loss: 0.305\n",
      "epoch: 7000, acc: 0.844, loss: 0.483\n",
      "epoch: 7000, acc: 0.875, loss: 0.275\n",
      "epoch: 7000, acc: 0.781, loss: 0.452\n",
      "epoch: 7000, acc: 0.938, loss: 0.242\n",
      "epoch: 7000, acc: 0.750, loss: 0.637\n",
      "epoch: 7000, acc: 0.781, loss: 0.466\n",
      "epoch: 7000, acc: 0.781, loss: 0.395\n",
      "epoch: 7000, acc: 0.906, loss: 0.256\n",
      "epoch: 7000, acc: 0.906, loss: 0.315\n",
      "epoch: 7000, acc: 0.750, loss: 0.494\n",
      "epoch: 7000, acc: 0.906, loss: 0.301\n",
      "epoch: 7000, acc: 0.857, loss: 0.375\n",
      "epoch: 7100, acc: 0.844, loss: 0.316\n",
      "epoch: 7100, acc: 0.719, loss: 0.506\n",
      "epoch: 7100, acc: 0.969, loss: 0.232\n",
      "epoch: 7100, acc: 0.781, loss: 0.474\n",
      "epoch: 7100, acc: 0.875, loss: 0.344\n",
      "epoch: 7100, acc: 0.812, loss: 0.436\n",
      "epoch: 7100, acc: 0.781, loss: 0.588\n",
      "epoch: 7100, acc: 0.938, loss: 0.205\n",
      "epoch: 7100, acc: 0.719, loss: 0.675\n",
      "epoch: 7100, acc: 0.688, loss: 0.526\n",
      "epoch: 7100, acc: 0.781, loss: 0.392\n",
      "epoch: 7100, acc: 0.875, loss: 0.334\n",
      "epoch: 7100, acc: 0.844, loss: 0.367\n",
      "epoch: 7100, acc: 0.844, loss: 0.441\n",
      "epoch: 7100, acc: 0.750, loss: 0.532\n",
      "epoch: 7100, acc: 0.781, loss: 0.495\n",
      "epoch: 7100, acc: 0.875, loss: 0.392\n",
      "epoch: 7100, acc: 0.875, loss: 0.449\n",
      "epoch: 7100, acc: 0.719, loss: 0.432\n",
      "epoch: 7100, acc: 0.906, loss: 0.310\n",
      "epoch: 7100, acc: 0.750, loss: 0.552\n",
      "epoch: 7100, acc: 0.812, loss: 0.292\n",
      "epoch: 7100, acc: 0.844, loss: 0.356\n",
      "epoch: 7100, acc: 0.812, loss: 0.292\n",
      "epoch: 7100, acc: 0.719, loss: 0.491\n",
      "epoch: 7100, acc: 0.750, loss: 0.530\n",
      "epoch: 7100, acc: 0.906, loss: 0.345\n",
      "epoch: 7100, acc: 0.812, loss: 0.339\n",
      "epoch: 7100, acc: 0.844, loss: 0.354\n",
      "epoch: 7100, acc: 0.781, loss: 0.504\n",
      "epoch: 7100, acc: 0.844, loss: 0.307\n",
      "epoch: 7100, acc: 0.844, loss: 0.466\n",
      "epoch: 7100, acc: 0.810, loss: 0.681\n",
      "epoch: 7200, acc: 0.938, loss: 0.340\n",
      "epoch: 7200, acc: 0.875, loss: 0.466\n",
      "epoch: 7200, acc: 0.781, loss: 0.442\n",
      "epoch: 7200, acc: 0.781, loss: 0.554\n",
      "epoch: 7200, acc: 0.781, loss: 0.413\n",
      "epoch: 7200, acc: 0.844, loss: 0.398\n",
      "epoch: 7200, acc: 0.844, loss: 0.456\n",
      "epoch: 7200, acc: 0.812, loss: 0.377\n",
      "epoch: 7200, acc: 0.875, loss: 0.424\n",
      "epoch: 7200, acc: 0.719, loss: 0.504\n",
      "epoch: 7200, acc: 0.719, loss: 0.465\n",
      "epoch: 7200, acc: 0.781, loss: 0.473\n",
      "epoch: 7200, acc: 0.938, loss: 0.300\n",
      "epoch: 7200, acc: 0.875, loss: 0.441\n",
      "epoch: 7200, acc: 0.875, loss: 0.347\n",
      "epoch: 7200, acc: 0.656, loss: 0.672\n",
      "epoch: 7200, acc: 0.781, loss: 0.403\n",
      "epoch: 7200, acc: 0.781, loss: 0.468\n",
      "epoch: 7200, acc: 0.781, loss: 0.411\n",
      "epoch: 7200, acc: 0.906, loss: 0.327\n",
      "epoch: 7200, acc: 0.781, loss: 0.519\n",
      "epoch: 7200, acc: 0.906, loss: 0.281\n",
      "epoch: 7200, acc: 0.844, loss: 0.340\n",
      "epoch: 7200, acc: 0.875, loss: 0.297\n",
      "epoch: 7200, acc: 0.688, loss: 0.503\n",
      "epoch: 7200, acc: 0.875, loss: 0.329\n",
      "epoch: 7200, acc: 0.938, loss: 0.309\n",
      "epoch: 7200, acc: 0.750, loss: 0.528\n",
      "epoch: 7200, acc: 0.875, loss: 0.358\n",
      "epoch: 7200, acc: 0.781, loss: 0.442\n",
      "epoch: 7200, acc: 0.750, loss: 0.450\n",
      "epoch: 7200, acc: 0.812, loss: 0.406\n",
      "epoch: 7200, acc: 0.810, loss: 0.390\n",
      "epoch: 7300, acc: 0.781, loss: 0.511\n",
      "epoch: 7300, acc: 0.781, loss: 0.498\n",
      "epoch: 7300, acc: 0.844, loss: 0.378\n",
      "epoch: 7300, acc: 0.844, loss: 0.371\n",
      "epoch: 7300, acc: 0.875, loss: 0.371\n",
      "epoch: 7300, acc: 0.844, loss: 0.444\n",
      "epoch: 7300, acc: 0.844, loss: 0.418\n",
      "epoch: 7300, acc: 0.938, loss: 0.365\n",
      "epoch: 7300, acc: 0.688, loss: 0.519\n",
      "epoch: 7300, acc: 0.781, loss: 0.485\n",
      "epoch: 7300, acc: 0.844, loss: 0.345\n",
      "epoch: 7300, acc: 0.938, loss: 0.255\n",
      "epoch: 7300, acc: 0.750, loss: 0.562\n",
      "epoch: 7300, acc: 0.844, loss: 0.317\n",
      "epoch: 7300, acc: 0.938, loss: 0.245\n",
      "epoch: 7300, acc: 0.781, loss: 0.523\n",
      "epoch: 7300, acc: 0.750, loss: 0.416\n",
      "epoch: 7300, acc: 0.844, loss: 0.437\n",
      "epoch: 7300, acc: 0.812, loss: 0.542\n",
      "epoch: 7300, acc: 0.875, loss: 0.488\n",
      "epoch: 7300, acc: 0.812, loss: 0.399\n",
      "epoch: 7300, acc: 0.812, loss: 0.432\n",
      "epoch: 7300, acc: 0.812, loss: 0.344\n",
      "epoch: 7300, acc: 0.812, loss: 0.464\n",
      "epoch: 7300, acc: 0.781, loss: 0.359\n",
      "epoch: 7300, acc: 0.906, loss: 0.276\n",
      "epoch: 7300, acc: 0.750, loss: 0.404\n",
      "epoch: 7300, acc: 0.844, loss: 0.362\n",
      "epoch: 7300, acc: 0.875, loss: 0.333\n",
      "epoch: 7300, acc: 0.719, loss: 0.534\n",
      "epoch: 7300, acc: 0.750, loss: 0.583\n",
      "epoch: 7300, acc: 0.844, loss: 0.369\n",
      "epoch: 7300, acc: 0.667, loss: 0.528\n",
      "epoch: 7400, acc: 0.938, loss: 0.216\n",
      "epoch: 7400, acc: 0.906, loss: 0.274\n",
      "epoch: 7400, acc: 0.844, loss: 0.478\n",
      "epoch: 7400, acc: 0.875, loss: 0.317\n",
      "epoch: 7400, acc: 0.781, loss: 0.411\n",
      "epoch: 7400, acc: 0.906, loss: 0.271\n",
      "epoch: 7400, acc: 0.844, loss: 0.393\n",
      "epoch: 7400, acc: 0.812, loss: 0.602\n",
      "epoch: 7400, acc: 0.688, loss: 0.476\n",
      "epoch: 7400, acc: 0.750, loss: 0.416\n",
      "epoch: 7400, acc: 0.844, loss: 0.453\n",
      "epoch: 7400, acc: 0.906, loss: 0.353\n",
      "epoch: 7400, acc: 0.781, loss: 0.523\n",
      "epoch: 7400, acc: 0.812, loss: 0.448\n",
      "epoch: 7400, acc: 0.719, loss: 0.451\n",
      "epoch: 7400, acc: 0.844, loss: 0.406\n",
      "epoch: 7400, acc: 0.844, loss: 0.386\n",
      "epoch: 7400, acc: 0.781, loss: 0.362\n",
      "epoch: 7400, acc: 0.875, loss: 0.394\n",
      "epoch: 7400, acc: 0.906, loss: 0.452\n",
      "epoch: 7400, acc: 0.688, loss: 0.766\n",
      "epoch: 7400, acc: 0.781, loss: 0.392\n",
      "epoch: 7400, acc: 0.781, loss: 0.495\n",
      "epoch: 7400, acc: 0.844, loss: 0.383\n",
      "epoch: 7400, acc: 0.688, loss: 0.593\n",
      "epoch: 7400, acc: 0.844, loss: 0.377\n",
      "epoch: 7400, acc: 0.875, loss: 0.300\n",
      "epoch: 7400, acc: 0.750, loss: 0.518\n",
      "epoch: 7400, acc: 0.719, loss: 0.531\n",
      "epoch: 7400, acc: 0.938, loss: 0.258\n",
      "epoch: 7400, acc: 0.938, loss: 0.259\n",
      "epoch: 7400, acc: 0.812, loss: 0.494\n",
      "epoch: 7400, acc: 0.857, loss: 0.442\n",
      "epoch: 7500, acc: 0.875, loss: 0.410\n",
      "epoch: 7500, acc: 0.906, loss: 0.325\n",
      "epoch: 7500, acc: 0.719, loss: 0.472\n",
      "epoch: 7500, acc: 0.812, loss: 0.447\n",
      "epoch: 7500, acc: 0.844, loss: 0.292\n",
      "epoch: 7500, acc: 0.812, loss: 0.369\n",
      "epoch: 7500, acc: 0.688, loss: 0.533\n",
      "epoch: 7500, acc: 0.844, loss: 0.385\n",
      "epoch: 7500, acc: 0.781, loss: 0.429\n",
      "epoch: 7500, acc: 0.906, loss: 0.265\n",
      "epoch: 7500, acc: 0.844, loss: 0.347\n",
      "epoch: 7500, acc: 0.844, loss: 0.422\n",
      "epoch: 7500, acc: 0.812, loss: 0.315\n",
      "epoch: 7500, acc: 0.844, loss: 0.326\n",
      "epoch: 7500, acc: 0.906, loss: 0.293\n",
      "epoch: 7500, acc: 0.781, loss: 0.620\n",
      "epoch: 7500, acc: 0.750, loss: 0.455\n",
      "epoch: 7500, acc: 0.719, loss: 0.388\n",
      "epoch: 7500, acc: 0.750, loss: 0.509\n",
      "epoch: 7500, acc: 0.719, loss: 0.710\n",
      "epoch: 7500, acc: 0.938, loss: 0.326\n",
      "epoch: 7500, acc: 0.906, loss: 0.360\n",
      "epoch: 7500, acc: 0.781, loss: 0.413\n",
      "epoch: 7500, acc: 0.875, loss: 0.362\n",
      "epoch: 7500, acc: 0.750, loss: 0.480\n",
      "epoch: 7500, acc: 0.812, loss: 0.520\n",
      "epoch: 7500, acc: 0.750, loss: 0.515\n",
      "epoch: 7500, acc: 0.781, loss: 0.424\n",
      "epoch: 7500, acc: 0.812, loss: 0.570\n",
      "epoch: 7500, acc: 0.875, loss: 0.352\n",
      "epoch: 7500, acc: 0.812, loss: 0.412\n",
      "epoch: 7500, acc: 0.844, loss: 0.478\n",
      "epoch: 7500, acc: 0.857, loss: 0.319\n",
      "epoch: 7600, acc: 0.719, loss: 0.562\n",
      "epoch: 7600, acc: 0.844, loss: 0.493\n",
      "epoch: 7600, acc: 0.812, loss: 0.443\n",
      "epoch: 7600, acc: 0.906, loss: 0.253\n",
      "epoch: 7600, acc: 0.719, loss: 0.571\n",
      "epoch: 7600, acc: 0.750, loss: 0.533\n",
      "epoch: 7600, acc: 0.906, loss: 0.258\n",
      "epoch: 7600, acc: 0.719, loss: 0.445\n",
      "epoch: 7600, acc: 0.781, loss: 0.406\n",
      "epoch: 7600, acc: 0.844, loss: 0.467\n",
      "epoch: 7600, acc: 0.906, loss: 0.306\n",
      "epoch: 7600, acc: 0.906, loss: 0.242\n",
      "epoch: 7600, acc: 0.938, loss: 0.390\n",
      "epoch: 7600, acc: 0.875, loss: 0.314\n",
      "epoch: 7600, acc: 0.781, loss: 0.410\n",
      "epoch: 7600, acc: 0.750, loss: 0.575\n",
      "epoch: 7600, acc: 0.969, loss: 0.269\n",
      "epoch: 7600, acc: 0.875, loss: 0.395\n",
      "epoch: 7600, acc: 0.906, loss: 0.266\n",
      "epoch: 7600, acc: 0.812, loss: 0.296\n",
      "epoch: 7600, acc: 0.812, loss: 0.424\n",
      "epoch: 7600, acc: 0.750, loss: 0.513\n",
      "epoch: 7600, acc: 0.969, loss: 0.271\n",
      "epoch: 7600, acc: 0.844, loss: 0.385\n",
      "epoch: 7600, acc: 0.781, loss: 0.616\n",
      "epoch: 7600, acc: 0.781, loss: 0.503\n",
      "epoch: 7600, acc: 0.812, loss: 0.506\n",
      "epoch: 7600, acc: 0.781, loss: 0.412\n",
      "epoch: 7600, acc: 0.719, loss: 0.535\n",
      "epoch: 7600, acc: 0.844, loss: 0.349\n",
      "epoch: 7600, acc: 0.750, loss: 0.471\n",
      "epoch: 7600, acc: 0.875, loss: 0.418\n",
      "epoch: 7600, acc: 0.762, loss: 0.513\n",
      "epoch: 7700, acc: 0.844, loss: 0.336\n",
      "epoch: 7700, acc: 0.719, loss: 0.436\n",
      "epoch: 7700, acc: 0.844, loss: 0.446\n",
      "epoch: 7700, acc: 0.750, loss: 0.586\n",
      "epoch: 7700, acc: 0.844, loss: 0.363\n",
      "epoch: 7700, acc: 0.750, loss: 0.436\n",
      "epoch: 7700, acc: 0.844, loss: 0.381\n",
      "epoch: 7700, acc: 0.812, loss: 0.336\n",
      "epoch: 7700, acc: 0.844, loss: 0.402\n",
      "epoch: 7700, acc: 0.844, loss: 0.363\n",
      "epoch: 7700, acc: 0.781, loss: 0.479\n",
      "epoch: 7700, acc: 0.750, loss: 0.540\n",
      "epoch: 7700, acc: 0.844, loss: 0.378\n",
      "epoch: 7700, acc: 0.844, loss: 0.453\n",
      "epoch: 7700, acc: 0.844, loss: 0.388\n",
      "epoch: 7700, acc: 0.812, loss: 0.390\n",
      "epoch: 7700, acc: 0.812, loss: 0.410\n",
      "epoch: 7700, acc: 0.906, loss: 0.303\n",
      "epoch: 7700, acc: 0.844, loss: 0.446\n",
      "epoch: 7700, acc: 0.906, loss: 0.440\n",
      "epoch: 7700, acc: 0.906, loss: 0.350\n",
      "epoch: 7700, acc: 0.812, loss: 0.429\n",
      "epoch: 7700, acc: 0.781, loss: 0.592\n",
      "epoch: 7700, acc: 0.750, loss: 0.444\n",
      "epoch: 7700, acc: 0.844, loss: 0.372\n",
      "epoch: 7700, acc: 1.000, loss: 0.181\n",
      "epoch: 7700, acc: 0.844, loss: 0.351\n",
      "epoch: 7700, acc: 0.844, loss: 0.378\n",
      "epoch: 7700, acc: 0.594, loss: 0.659\n",
      "epoch: 7700, acc: 0.781, loss: 0.547\n",
      "epoch: 7700, acc: 0.812, loss: 0.343\n",
      "epoch: 7700, acc: 0.844, loss: 0.322\n",
      "epoch: 7700, acc: 0.667, loss: 0.614\n",
      "epoch: 7800, acc: 0.750, loss: 0.604\n",
      "epoch: 7800, acc: 0.906, loss: 0.440\n",
      "epoch: 7800, acc: 0.750, loss: 0.454\n",
      "epoch: 7800, acc: 0.781, loss: 0.393\n",
      "epoch: 7800, acc: 0.781, loss: 0.505\n",
      "epoch: 7800, acc: 0.875, loss: 0.358\n",
      "epoch: 7800, acc: 0.906, loss: 0.291\n",
      "epoch: 7800, acc: 0.844, loss: 0.409\n",
      "epoch: 7800, acc: 0.719, loss: 0.548\n",
      "epoch: 7800, acc: 0.781, loss: 0.556\n",
      "epoch: 7800, acc: 0.750, loss: 0.453\n",
      "epoch: 7800, acc: 0.781, loss: 0.442\n",
      "epoch: 7800, acc: 0.875, loss: 0.330\n",
      "epoch: 7800, acc: 0.812, loss: 0.345\n",
      "epoch: 7800, acc: 0.938, loss: 0.210\n",
      "epoch: 7800, acc: 0.781, loss: 0.445\n",
      "epoch: 7800, acc: 0.750, loss: 0.494\n",
      "epoch: 7800, acc: 0.844, loss: 0.283\n",
      "epoch: 7800, acc: 0.750, loss: 0.519\n",
      "epoch: 7800, acc: 0.844, loss: 0.498\n",
      "epoch: 7800, acc: 0.781, loss: 0.481\n",
      "epoch: 7800, acc: 0.875, loss: 0.350\n",
      "epoch: 7800, acc: 0.750, loss: 0.441\n",
      "epoch: 7800, acc: 0.750, loss: 0.465\n",
      "epoch: 7800, acc: 0.812, loss: 0.516\n",
      "epoch: 7800, acc: 0.844, loss: 0.463\n",
      "epoch: 7800, acc: 0.844, loss: 0.362\n",
      "epoch: 7800, acc: 0.875, loss: 0.385\n",
      "epoch: 7800, acc: 0.812, loss: 0.396\n",
      "epoch: 7800, acc: 0.906, loss: 0.296\n",
      "epoch: 7800, acc: 0.844, loss: 0.422\n",
      "epoch: 7800, acc: 0.844, loss: 0.337\n",
      "epoch: 7800, acc: 0.810, loss: 0.341\n",
      "epoch: 7900, acc: 0.688, loss: 0.629\n",
      "epoch: 7900, acc: 0.875, loss: 0.235\n",
      "epoch: 7900, acc: 0.812, loss: 0.508\n",
      "epoch: 7900, acc: 0.844, loss: 0.358\n",
      "epoch: 7900, acc: 0.750, loss: 0.544\n",
      "epoch: 7900, acc: 0.812, loss: 0.401\n",
      "epoch: 7900, acc: 0.875, loss: 0.303\n",
      "epoch: 7900, acc: 0.938, loss: 0.256\n",
      "epoch: 7900, acc: 0.688, loss: 0.601\n",
      "epoch: 7900, acc: 0.844, loss: 0.416\n",
      "epoch: 7900, acc: 0.781, loss: 0.494\n",
      "epoch: 7900, acc: 0.875, loss: 0.336\n",
      "epoch: 7900, acc: 0.688, loss: 0.578\n",
      "epoch: 7900, acc: 0.750, loss: 0.643\n",
      "epoch: 7900, acc: 0.812, loss: 0.439\n",
      "epoch: 7900, acc: 0.750, loss: 0.472\n",
      "epoch: 7900, acc: 0.875, loss: 0.317\n",
      "epoch: 7900, acc: 0.938, loss: 0.251\n",
      "epoch: 7900, acc: 0.812, loss: 0.378\n",
      "epoch: 7900, acc: 0.750, loss: 0.497\n",
      "epoch: 7900, acc: 0.938, loss: 0.253\n",
      "epoch: 7900, acc: 0.781, loss: 0.422\n",
      "epoch: 7900, acc: 0.844, loss: 0.493\n",
      "epoch: 7900, acc: 0.812, loss: 0.381\n",
      "epoch: 7900, acc: 0.844, loss: 0.539\n",
      "epoch: 7900, acc: 0.875, loss: 0.337\n",
      "epoch: 7900, acc: 0.688, loss: 0.610\n",
      "epoch: 7900, acc: 0.969, loss: 0.243\n",
      "epoch: 7900, acc: 0.844, loss: 0.360\n",
      "epoch: 7900, acc: 0.750, loss: 0.443\n",
      "epoch: 7900, acc: 0.781, loss: 0.439\n",
      "epoch: 7900, acc: 0.875, loss: 0.342\n",
      "epoch: 7900, acc: 0.905, loss: 0.274\n",
      "epoch: 8000, acc: 0.688, loss: 0.623\n",
      "epoch: 8000, acc: 0.875, loss: 0.396\n",
      "epoch: 8000, acc: 0.812, loss: 0.376\n",
      "epoch: 8000, acc: 0.812, loss: 0.428\n",
      "epoch: 8000, acc: 0.812, loss: 0.478\n",
      "epoch: 8000, acc: 0.750, loss: 0.562\n",
      "epoch: 8000, acc: 0.625, loss: 0.760\n",
      "epoch: 8000, acc: 0.688, loss: 0.469\n",
      "epoch: 8000, acc: 0.750, loss: 0.541\n",
      "epoch: 8000, acc: 0.844, loss: 0.368\n",
      "epoch: 8000, acc: 0.938, loss: 0.286\n",
      "epoch: 8000, acc: 0.906, loss: 0.332\n",
      "epoch: 8000, acc: 0.844, loss: 0.376\n",
      "epoch: 8000, acc: 0.875, loss: 0.293\n",
      "epoch: 8000, acc: 0.875, loss: 0.322\n",
      "epoch: 8000, acc: 0.906, loss: 0.356\n",
      "epoch: 8000, acc: 0.812, loss: 0.396\n",
      "epoch: 8000, acc: 0.750, loss: 0.529\n",
      "epoch: 8000, acc: 0.906, loss: 0.288\n",
      "epoch: 8000, acc: 0.812, loss: 0.472\n",
      "epoch: 8000, acc: 0.906, loss: 0.376\n",
      "epoch: 8000, acc: 0.812, loss: 0.395\n",
      "epoch: 8000, acc: 0.875, loss: 0.336\n",
      "epoch: 8000, acc: 0.938, loss: 0.238\n",
      "epoch: 8000, acc: 0.812, loss: 0.322\n",
      "epoch: 8000, acc: 0.719, loss: 0.445\n",
      "epoch: 8000, acc: 0.844, loss: 0.384\n",
      "epoch: 8000, acc: 0.812, loss: 0.593\n",
      "epoch: 8000, acc: 0.844, loss: 0.359\n",
      "epoch: 8000, acc: 0.688, loss: 0.515\n",
      "epoch: 8000, acc: 0.812, loss: 0.418\n",
      "epoch: 8000, acc: 0.844, loss: 0.547\n",
      "epoch: 8000, acc: 1.000, loss: 0.192\n",
      "epoch: 8100, acc: 0.812, loss: 0.394\n",
      "epoch: 8100, acc: 0.844, loss: 0.462\n",
      "epoch: 8100, acc: 0.844, loss: 0.363\n",
      "epoch: 8100, acc: 0.938, loss: 0.308\n",
      "epoch: 8100, acc: 0.719, loss: 0.574\n",
      "epoch: 8100, acc: 0.875, loss: 0.402\n",
      "epoch: 8100, acc: 0.781, loss: 0.482\n",
      "epoch: 8100, acc: 0.812, loss: 0.423\n",
      "epoch: 8100, acc: 0.906, loss: 0.333\n",
      "epoch: 8100, acc: 0.844, loss: 0.305\n",
      "epoch: 8100, acc: 0.938, loss: 0.273\n",
      "epoch: 8100, acc: 0.875, loss: 0.340\n",
      "epoch: 8100, acc: 0.906, loss: 0.314\n",
      "epoch: 8100, acc: 0.719, loss: 0.471\n",
      "epoch: 8100, acc: 0.781, loss: 0.489\n",
      "epoch: 8100, acc: 0.781, loss: 0.522\n",
      "epoch: 8100, acc: 0.750, loss: 0.573\n",
      "epoch: 8100, acc: 0.906, loss: 0.291\n",
      "epoch: 8100, acc: 0.875, loss: 0.352\n",
      "epoch: 8100, acc: 0.812, loss: 0.345\n",
      "epoch: 8100, acc: 0.719, loss: 0.549\n",
      "epoch: 8100, acc: 0.750, loss: 0.452\n",
      "epoch: 8100, acc: 0.719, loss: 0.503\n",
      "epoch: 8100, acc: 0.750, loss: 0.543\n",
      "epoch: 8100, acc: 0.875, loss: 0.344\n",
      "epoch: 8100, acc: 0.844, loss: 0.442\n",
      "epoch: 8100, acc: 0.812, loss: 0.383\n",
      "epoch: 8100, acc: 0.844, loss: 0.352\n",
      "epoch: 8100, acc: 0.844, loss: 0.442\n",
      "epoch: 8100, acc: 0.812, loss: 0.433\n",
      "epoch: 8100, acc: 0.844, loss: 0.585\n",
      "epoch: 8100, acc: 0.875, loss: 0.324\n",
      "epoch: 8100, acc: 0.714, loss: 0.554\n",
      "epoch: 8200, acc: 0.688, loss: 0.505\n",
      "epoch: 8200, acc: 0.812, loss: 0.431\n",
      "epoch: 8200, acc: 0.906, loss: 0.315\n",
      "epoch: 8200, acc: 0.875, loss: 0.295\n",
      "epoch: 8200, acc: 0.781, loss: 0.450\n",
      "epoch: 8200, acc: 0.875, loss: 0.338\n",
      "epoch: 8200, acc: 0.688, loss: 0.484\n",
      "epoch: 8200, acc: 0.875, loss: 0.319\n",
      "epoch: 8200, acc: 0.938, loss: 0.278\n",
      "epoch: 8200, acc: 0.750, loss: 0.507\n",
      "epoch: 8200, acc: 0.906, loss: 0.340\n",
      "epoch: 8200, acc: 0.750, loss: 0.492\n",
      "epoch: 8200, acc: 0.750, loss: 0.517\n",
      "epoch: 8200, acc: 0.906, loss: 0.324\n",
      "epoch: 8200, acc: 0.812, loss: 0.446\n",
      "epoch: 8200, acc: 0.875, loss: 0.342\n",
      "epoch: 8200, acc: 0.812, loss: 0.435\n",
      "epoch: 8200, acc: 0.812, loss: 0.525\n",
      "epoch: 8200, acc: 0.938, loss: 0.237\n",
      "epoch: 8200, acc: 0.906, loss: 0.422\n",
      "epoch: 8200, acc: 0.719, loss: 0.489\n",
      "epoch: 8200, acc: 0.812, loss: 0.426\n",
      "epoch: 8200, acc: 0.719, loss: 0.436\n",
      "epoch: 8200, acc: 0.844, loss: 0.338\n",
      "epoch: 8200, acc: 0.750, loss: 0.468\n",
      "epoch: 8200, acc: 0.844, loss: 0.317\n",
      "epoch: 8200, acc: 0.750, loss: 0.523\n",
      "epoch: 8200, acc: 0.781, loss: 0.405\n",
      "epoch: 8200, acc: 0.781, loss: 0.556\n",
      "epoch: 8200, acc: 0.781, loss: 0.562\n",
      "epoch: 8200, acc: 0.812, loss: 0.508\n",
      "epoch: 8200, acc: 0.875, loss: 0.403\n",
      "epoch: 8200, acc: 0.857, loss: 0.378\n",
      "epoch: 8300, acc: 0.781, loss: 0.432\n",
      "epoch: 8300, acc: 0.812, loss: 0.433\n",
      "epoch: 8300, acc: 0.906, loss: 0.388\n",
      "epoch: 8300, acc: 0.719, loss: 0.633\n",
      "epoch: 8300, acc: 0.844, loss: 0.358\n",
      "epoch: 8300, acc: 0.750, loss: 0.705\n",
      "epoch: 8300, acc: 0.750, loss: 0.475\n",
      "epoch: 8300, acc: 0.875, loss: 0.277\n",
      "epoch: 8300, acc: 0.906, loss: 0.323\n",
      "epoch: 8300, acc: 0.906, loss: 0.288\n",
      "epoch: 8300, acc: 0.750, loss: 0.567\n",
      "epoch: 8300, acc: 0.844, loss: 0.302\n",
      "epoch: 8300, acc: 0.938, loss: 0.235\n",
      "epoch: 8300, acc: 0.812, loss: 0.430\n",
      "epoch: 8300, acc: 0.875, loss: 0.355\n",
      "epoch: 8300, acc: 0.781, loss: 0.647\n",
      "epoch: 8300, acc: 0.906, loss: 0.417\n",
      "epoch: 8300, acc: 0.812, loss: 0.438\n",
      "epoch: 8300, acc: 0.812, loss: 0.357\n",
      "epoch: 8300, acc: 0.781, loss: 0.406\n",
      "epoch: 8300, acc: 0.719, loss: 0.709\n",
      "epoch: 8300, acc: 0.875, loss: 0.348\n",
      "epoch: 8300, acc: 0.938, loss: 0.180\n",
      "epoch: 8300, acc: 0.906, loss: 0.340\n",
      "epoch: 8300, acc: 0.812, loss: 0.363\n",
      "epoch: 8300, acc: 0.750, loss: 0.543\n",
      "epoch: 8300, acc: 0.844, loss: 0.528\n",
      "epoch: 8300, acc: 0.844, loss: 0.325\n",
      "epoch: 8300, acc: 0.781, loss: 0.422\n",
      "epoch: 8300, acc: 0.688, loss: 0.542\n",
      "epoch: 8300, acc: 0.875, loss: 0.285\n",
      "epoch: 8300, acc: 0.719, loss: 0.391\n",
      "epoch: 8300, acc: 0.857, loss: 0.392\n",
      "epoch: 8400, acc: 0.875, loss: 0.416\n",
      "epoch: 8400, acc: 0.781, loss: 0.553\n",
      "epoch: 8400, acc: 0.625, loss: 0.475\n",
      "epoch: 8400, acc: 0.875, loss: 0.284\n",
      "epoch: 8400, acc: 0.812, loss: 0.426\n",
      "epoch: 8400, acc: 0.781, loss: 0.395\n",
      "epoch: 8400, acc: 0.750, loss: 0.512\n",
      "epoch: 8400, acc: 0.844, loss: 0.389\n",
      "epoch: 8400, acc: 0.844, loss: 0.394\n",
      "epoch: 8400, acc: 0.812, loss: 0.521\n",
      "epoch: 8400, acc: 0.875, loss: 0.391\n",
      "epoch: 8400, acc: 0.781, loss: 0.516\n",
      "epoch: 8400, acc: 0.875, loss: 0.327\n",
      "epoch: 8400, acc: 0.844, loss: 0.387\n",
      "epoch: 8400, acc: 0.844, loss: 0.376\n",
      "epoch: 8400, acc: 0.844, loss: 0.362\n",
      "epoch: 8400, acc: 0.812, loss: 0.465\n",
      "epoch: 8400, acc: 0.812, loss: 0.474\n",
      "epoch: 8400, acc: 0.844, loss: 0.450\n",
      "epoch: 8400, acc: 0.812, loss: 0.483\n",
      "epoch: 8400, acc: 0.844, loss: 0.454\n",
      "epoch: 8400, acc: 0.844, loss: 0.464\n",
      "epoch: 8400, acc: 0.938, loss: 0.279\n",
      "epoch: 8400, acc: 0.844, loss: 0.361\n",
      "epoch: 8400, acc: 0.812, loss: 0.399\n",
      "epoch: 8400, acc: 0.812, loss: 0.510\n",
      "epoch: 8400, acc: 0.812, loss: 0.415\n",
      "epoch: 8400, acc: 0.875, loss: 0.318\n",
      "epoch: 8400, acc: 0.812, loss: 0.374\n",
      "epoch: 8400, acc: 0.844, loss: 0.324\n",
      "epoch: 8400, acc: 0.750, loss: 0.546\n",
      "epoch: 8400, acc: 0.750, loss: 0.398\n",
      "epoch: 8400, acc: 0.810, loss: 0.397\n",
      "epoch: 8500, acc: 0.812, loss: 0.410\n",
      "epoch: 8500, acc: 0.844, loss: 0.364\n",
      "epoch: 8500, acc: 0.969, loss: 0.181\n",
      "epoch: 8500, acc: 0.688, loss: 0.668\n",
      "epoch: 8500, acc: 0.812, loss: 0.451\n",
      "epoch: 8500, acc: 0.875, loss: 0.392\n",
      "epoch: 8500, acc: 0.812, loss: 0.327\n",
      "epoch: 8500, acc: 0.750, loss: 0.448\n",
      "epoch: 8500, acc: 0.781, loss: 0.498\n",
      "epoch: 8500, acc: 0.906, loss: 0.357\n",
      "epoch: 8500, acc: 0.812, loss: 0.407\n",
      "epoch: 8500, acc: 0.844, loss: 0.379\n",
      "epoch: 8500, acc: 0.750, loss: 0.553\n",
      "epoch: 8500, acc: 0.906, loss: 0.329\n",
      "epoch: 8500, acc: 0.812, loss: 0.422\n",
      "epoch: 8500, acc: 0.844, loss: 0.432\n",
      "epoch: 8500, acc: 0.750, loss: 0.438\n",
      "epoch: 8500, acc: 0.875, loss: 0.359\n",
      "epoch: 8500, acc: 0.812, loss: 0.368\n",
      "epoch: 8500, acc: 0.750, loss: 0.452\n",
      "epoch: 8500, acc: 0.750, loss: 0.479\n",
      "epoch: 8500, acc: 0.750, loss: 0.408\n",
      "epoch: 8500, acc: 0.750, loss: 0.647\n",
      "epoch: 8500, acc: 0.844, loss: 0.577\n",
      "epoch: 8500, acc: 0.875, loss: 0.293\n",
      "epoch: 8500, acc: 0.875, loss: 0.377\n",
      "epoch: 8500, acc: 0.969, loss: 0.269\n",
      "epoch: 8500, acc: 0.812, loss: 0.386\n",
      "epoch: 8500, acc: 0.781, loss: 0.509\n",
      "epoch: 8500, acc: 0.875, loss: 0.247\n",
      "epoch: 8500, acc: 0.656, loss: 0.556\n",
      "epoch: 8500, acc: 0.812, loss: 0.497\n",
      "epoch: 8500, acc: 0.952, loss: 0.324\n",
      "epoch: 8600, acc: 0.656, loss: 0.630\n",
      "epoch: 8600, acc: 0.938, loss: 0.258\n",
      "epoch: 8600, acc: 0.781, loss: 0.535\n",
      "epoch: 8600, acc: 0.906, loss: 0.223\n",
      "epoch: 8600, acc: 0.781, loss: 0.525\n",
      "epoch: 8600, acc: 0.844, loss: 0.416\n",
      "epoch: 8600, acc: 0.906, loss: 0.281\n",
      "epoch: 8600, acc: 0.719, loss: 0.559\n",
      "epoch: 8600, acc: 0.875, loss: 0.392\n",
      "epoch: 8600, acc: 0.969, loss: 0.217\n",
      "epoch: 8600, acc: 0.844, loss: 0.464\n",
      "epoch: 8600, acc: 0.906, loss: 0.303\n",
      "epoch: 8600, acc: 0.719, loss: 0.476\n",
      "epoch: 8600, acc: 0.781, loss: 0.513\n",
      "epoch: 8600, acc: 0.906, loss: 0.463\n",
      "epoch: 8600, acc: 0.812, loss: 0.411\n",
      "epoch: 8600, acc: 0.812, loss: 0.475\n",
      "epoch: 8600, acc: 0.844, loss: 0.405\n",
      "epoch: 8600, acc: 0.781, loss: 0.396\n",
      "epoch: 8600, acc: 0.875, loss: 0.398\n",
      "epoch: 8600, acc: 0.875, loss: 0.294\n",
      "epoch: 8600, acc: 0.719, loss: 0.428\n",
      "epoch: 8600, acc: 0.906, loss: 0.288\n",
      "epoch: 8600, acc: 0.688, loss: 0.578\n",
      "epoch: 8600, acc: 0.719, loss: 0.459\n",
      "epoch: 8600, acc: 0.875, loss: 0.323\n",
      "epoch: 8600, acc: 0.812, loss: 0.486\n",
      "epoch: 8600, acc: 0.812, loss: 0.440\n",
      "epoch: 8600, acc: 0.688, loss: 0.537\n",
      "epoch: 8600, acc: 0.812, loss: 0.356\n",
      "epoch: 8600, acc: 0.844, loss: 0.399\n",
      "epoch: 8600, acc: 0.750, loss: 0.527\n",
      "epoch: 8600, acc: 0.810, loss: 0.404\n",
      "epoch: 8700, acc: 0.812, loss: 0.449\n",
      "epoch: 8700, acc: 0.906, loss: 0.315\n",
      "epoch: 8700, acc: 0.969, loss: 0.220\n",
      "epoch: 8700, acc: 0.562, loss: 0.784\n",
      "epoch: 8700, acc: 0.844, loss: 0.366\n",
      "epoch: 8700, acc: 0.844, loss: 0.340\n",
      "epoch: 8700, acc: 0.812, loss: 0.299\n",
      "epoch: 8700, acc: 0.750, loss: 0.635\n",
      "epoch: 8700, acc: 0.750, loss: 0.464\n",
      "epoch: 8700, acc: 0.844, loss: 0.540\n",
      "epoch: 8700, acc: 0.906, loss: 0.349\n",
      "epoch: 8700, acc: 0.844, loss: 0.282\n",
      "epoch: 8700, acc: 0.781, loss: 0.477\n",
      "epoch: 8700, acc: 0.750, loss: 0.387\n",
      "epoch: 8700, acc: 0.844, loss: 0.396\n",
      "epoch: 8700, acc: 0.781, loss: 0.510\n",
      "epoch: 8700, acc: 0.750, loss: 0.471\n",
      "epoch: 8700, acc: 0.812, loss: 0.439\n",
      "epoch: 8700, acc: 0.812, loss: 0.367\n",
      "epoch: 8700, acc: 0.750, loss: 0.626\n",
      "epoch: 8700, acc: 0.781, loss: 0.495\n",
      "epoch: 8700, acc: 0.750, loss: 0.424\n",
      "epoch: 8700, acc: 0.812, loss: 0.427\n",
      "epoch: 8700, acc: 0.875, loss: 0.383\n",
      "epoch: 8700, acc: 0.906, loss: 0.343\n",
      "epoch: 8700, acc: 0.781, loss: 0.510\n",
      "epoch: 8700, acc: 0.906, loss: 0.337\n",
      "epoch: 8700, acc: 0.812, loss: 0.426\n",
      "epoch: 8700, acc: 0.844, loss: 0.313\n",
      "epoch: 8700, acc: 0.844, loss: 0.387\n",
      "epoch: 8700, acc: 0.906, loss: 0.290\n",
      "epoch: 8700, acc: 0.875, loss: 0.433\n",
      "epoch: 8700, acc: 0.857, loss: 0.289\n",
      "epoch: 8800, acc: 0.844, loss: 0.307\n",
      "epoch: 8800, acc: 0.812, loss: 0.416\n",
      "epoch: 8800, acc: 0.844, loss: 0.386\n",
      "epoch: 8800, acc: 0.750, loss: 0.538\n",
      "epoch: 8800, acc: 0.844, loss: 0.426\n",
      "epoch: 8800, acc: 0.781, loss: 0.439\n",
      "epoch: 8800, acc: 0.812, loss: 0.446\n",
      "epoch: 8800, acc: 0.812, loss: 0.404\n",
      "epoch: 8800, acc: 0.781, loss: 0.468\n",
      "epoch: 8800, acc: 0.844, loss: 0.361\n",
      "epoch: 8800, acc: 0.812, loss: 0.425\n",
      "epoch: 8800, acc: 0.812, loss: 0.471\n",
      "epoch: 8800, acc: 0.906, loss: 0.263\n",
      "epoch: 8800, acc: 0.844, loss: 0.460\n",
      "epoch: 8800, acc: 0.844, loss: 0.348\n",
      "epoch: 8800, acc: 0.875, loss: 0.334\n",
      "epoch: 8800, acc: 0.875, loss: 0.331\n",
      "epoch: 8800, acc: 0.688, loss: 0.581\n",
      "epoch: 8800, acc: 0.656, loss: 0.599\n",
      "epoch: 8800, acc: 0.812, loss: 0.512\n",
      "epoch: 8800, acc: 0.906, loss: 0.252\n",
      "epoch: 8800, acc: 0.719, loss: 0.462\n",
      "epoch: 8800, acc: 0.781, loss: 0.391\n",
      "epoch: 8800, acc: 0.844, loss: 0.399\n",
      "epoch: 8800, acc: 0.844, loss: 0.417\n",
      "epoch: 8800, acc: 0.781, loss: 0.551\n",
      "epoch: 8800, acc: 0.875, loss: 0.360\n",
      "epoch: 8800, acc: 0.875, loss: 0.311\n",
      "epoch: 8800, acc: 0.844, loss: 0.398\n",
      "epoch: 8800, acc: 0.844, loss: 0.317\n",
      "epoch: 8800, acc: 0.844, loss: 0.397\n",
      "epoch: 8800, acc: 0.781, loss: 0.636\n",
      "epoch: 8800, acc: 0.714, loss: 0.462\n",
      "epoch: 8900, acc: 0.781, loss: 0.496\n",
      "epoch: 8900, acc: 0.906, loss: 0.320\n",
      "epoch: 8900, acc: 0.719, loss: 0.489\n",
      "epoch: 8900, acc: 0.844, loss: 0.372\n",
      "epoch: 8900, acc: 0.719, loss: 0.554\n",
      "epoch: 8900, acc: 0.812, loss: 0.429\n",
      "epoch: 8900, acc: 0.969, loss: 0.258\n",
      "epoch: 8900, acc: 0.844, loss: 0.383\n",
      "epoch: 8900, acc: 0.781, loss: 0.434\n",
      "epoch: 8900, acc: 0.781, loss: 0.423\n",
      "epoch: 8900, acc: 0.938, loss: 0.303\n",
      "epoch: 8900, acc: 0.781, loss: 0.446\n",
      "epoch: 8900, acc: 0.906, loss: 0.304\n",
      "epoch: 8900, acc: 0.719, loss: 0.522\n",
      "epoch: 8900, acc: 0.688, loss: 0.623\n",
      "epoch: 8900, acc: 0.844, loss: 0.369\n",
      "epoch: 8900, acc: 0.719, loss: 0.552\n",
      "epoch: 8900, acc: 0.688, loss: 0.505\n",
      "epoch: 8900, acc: 0.844, loss: 0.429\n",
      "epoch: 8900, acc: 0.875, loss: 0.305\n",
      "epoch: 8900, acc: 0.875, loss: 0.379\n",
      "epoch: 8900, acc: 0.938, loss: 0.371\n",
      "epoch: 8900, acc: 0.812, loss: 0.499\n",
      "epoch: 8900, acc: 0.875, loss: 0.419\n",
      "epoch: 8900, acc: 0.750, loss: 0.589\n",
      "epoch: 8900, acc: 0.969, loss: 0.197\n",
      "epoch: 8900, acc: 0.781, loss: 0.419\n",
      "epoch: 8900, acc: 0.844, loss: 0.453\n",
      "epoch: 8900, acc: 0.875, loss: 0.362\n",
      "epoch: 8900, acc: 0.781, loss: 0.404\n",
      "epoch: 8900, acc: 0.812, loss: 0.360\n",
      "epoch: 8900, acc: 0.719, loss: 0.542\n",
      "epoch: 8900, acc: 0.952, loss: 0.269\n",
      "epoch: 9000, acc: 0.844, loss: 0.381\n",
      "epoch: 9000, acc: 0.781, loss: 0.443\n",
      "epoch: 9000, acc: 0.750, loss: 0.513\n",
      "epoch: 9000, acc: 0.875, loss: 0.419\n",
      "epoch: 9000, acc: 0.781, loss: 0.424\n",
      "epoch: 9000, acc: 0.812, loss: 0.380\n",
      "epoch: 9000, acc: 0.906, loss: 0.373\n",
      "epoch: 9000, acc: 0.781, loss: 0.505\n",
      "epoch: 9000, acc: 0.844, loss: 0.434\n",
      "epoch: 9000, acc: 0.812, loss: 0.425\n",
      "epoch: 9000, acc: 0.938, loss: 0.288\n",
      "epoch: 9000, acc: 0.750, loss: 0.470\n",
      "epoch: 9000, acc: 0.781, loss: 0.404\n",
      "epoch: 9000, acc: 0.812, loss: 0.434\n",
      "epoch: 9000, acc: 0.781, loss: 0.435\n",
      "epoch: 9000, acc: 0.875, loss: 0.485\n",
      "epoch: 9000, acc: 0.844, loss: 0.376\n",
      "epoch: 9000, acc: 0.875, loss: 0.390\n",
      "epoch: 9000, acc: 0.750, loss: 0.484\n",
      "epoch: 9000, acc: 0.812, loss: 0.312\n",
      "epoch: 9000, acc: 0.812, loss: 0.485\n",
      "epoch: 9000, acc: 0.781, loss: 0.454\n",
      "epoch: 9000, acc: 0.906, loss: 0.266\n",
      "epoch: 9000, acc: 0.875, loss: 0.289\n",
      "epoch: 9000, acc: 0.875, loss: 0.430\n",
      "epoch: 9000, acc: 0.750, loss: 0.507\n",
      "epoch: 9000, acc: 0.781, loss: 0.530\n",
      "epoch: 9000, acc: 0.781, loss: 0.408\n",
      "epoch: 9000, acc: 0.844, loss: 0.444\n",
      "epoch: 9000, acc: 0.844, loss: 0.295\n",
      "epoch: 9000, acc: 0.875, loss: 0.455\n",
      "epoch: 9000, acc: 0.719, loss: 0.487\n",
      "epoch: 9000, acc: 0.857, loss: 0.422\n",
      "epoch: 9100, acc: 0.656, loss: 0.553\n",
      "epoch: 9100, acc: 0.719, loss: 0.475\n",
      "epoch: 9100, acc: 0.844, loss: 0.467\n",
      "epoch: 9100, acc: 0.812, loss: 0.403\n",
      "epoch: 9100, acc: 0.625, loss: 0.536\n",
      "epoch: 9100, acc: 0.844, loss: 0.447\n",
      "epoch: 9100, acc: 0.781, loss: 0.490\n",
      "epoch: 9100, acc: 0.906, loss: 0.375\n",
      "epoch: 9100, acc: 0.844, loss: 0.442\n",
      "epoch: 9100, acc: 0.812, loss: 0.450\n",
      "epoch: 9100, acc: 0.844, loss: 0.510\n",
      "epoch: 9100, acc: 0.875, loss: 0.407\n",
      "epoch: 9100, acc: 0.625, loss: 0.649\n",
      "epoch: 9100, acc: 0.812, loss: 0.397\n",
      "epoch: 9100, acc: 0.938, loss: 0.252\n",
      "epoch: 9100, acc: 0.906, loss: 0.328\n",
      "epoch: 9100, acc: 0.750, loss: 0.402\n",
      "epoch: 9100, acc: 0.875, loss: 0.286\n",
      "epoch: 9100, acc: 0.875, loss: 0.304\n",
      "epoch: 9100, acc: 0.750, loss: 0.593\n",
      "epoch: 9100, acc: 0.875, loss: 0.306\n",
      "epoch: 9100, acc: 0.906, loss: 0.374\n",
      "epoch: 9100, acc: 0.844, loss: 0.442\n",
      "epoch: 9100, acc: 0.969, loss: 0.221\n",
      "epoch: 9100, acc: 0.875, loss: 0.344\n",
      "epoch: 9100, acc: 0.906, loss: 0.276\n",
      "epoch: 9100, acc: 0.812, loss: 0.405\n",
      "epoch: 9100, acc: 0.812, loss: 0.332\n",
      "epoch: 9100, acc: 0.688, loss: 0.608\n",
      "epoch: 9100, acc: 0.719, loss: 0.540\n",
      "epoch: 9100, acc: 0.875, loss: 0.293\n",
      "epoch: 9100, acc: 0.875, loss: 0.438\n",
      "epoch: 9100, acc: 0.810, loss: 0.504\n",
      "epoch: 9200, acc: 0.781, loss: 0.435\n",
      "epoch: 9200, acc: 0.812, loss: 0.412\n",
      "epoch: 9200, acc: 0.938, loss: 0.238\n",
      "epoch: 9200, acc: 0.812, loss: 0.491\n",
      "epoch: 9200, acc: 0.844, loss: 0.432\n",
      "epoch: 9200, acc: 0.844, loss: 0.349\n",
      "epoch: 9200, acc: 0.781, loss: 0.470\n",
      "epoch: 9200, acc: 0.781, loss: 0.521\n",
      "epoch: 9200, acc: 0.688, loss: 0.553\n",
      "epoch: 9200, acc: 0.812, loss: 0.578\n",
      "epoch: 9200, acc: 0.906, loss: 0.317\n",
      "epoch: 9200, acc: 0.781, loss: 0.452\n",
      "epoch: 9200, acc: 0.844, loss: 0.354\n",
      "epoch: 9200, acc: 0.719, loss: 0.503\n",
      "epoch: 9200, acc: 0.875, loss: 0.384\n",
      "epoch: 9200, acc: 0.812, loss: 0.372\n",
      "epoch: 9200, acc: 0.938, loss: 0.254\n",
      "epoch: 9200, acc: 0.781, loss: 0.514\n",
      "epoch: 9200, acc: 0.812, loss: 0.390\n",
      "epoch: 9200, acc: 0.812, loss: 0.391\n",
      "epoch: 9200, acc: 0.750, loss: 0.447\n",
      "epoch: 9200, acc: 0.844, loss: 0.379\n",
      "epoch: 9200, acc: 0.875, loss: 0.455\n",
      "epoch: 9200, acc: 0.969, loss: 0.303\n",
      "epoch: 9200, acc: 0.719, loss: 0.629\n",
      "epoch: 9200, acc: 0.750, loss: 0.522\n",
      "epoch: 9200, acc: 0.750, loss: 0.494\n",
      "epoch: 9200, acc: 0.812, loss: 0.406\n",
      "epoch: 9200, acc: 0.875, loss: 0.355\n",
      "epoch: 9200, acc: 0.844, loss: 0.252\n",
      "epoch: 9200, acc: 0.812, loss: 0.531\n",
      "epoch: 9200, acc: 0.938, loss: 0.236\n",
      "epoch: 9200, acc: 0.714, loss: 0.433\n",
      "epoch: 9300, acc: 0.906, loss: 0.256\n",
      "epoch: 9300, acc: 0.906, loss: 0.363\n",
      "epoch: 9300, acc: 0.750, loss: 0.535\n",
      "epoch: 9300, acc: 0.844, loss: 0.277\n",
      "epoch: 9300, acc: 0.719, loss: 0.652\n",
      "epoch: 9300, acc: 0.875, loss: 0.374\n",
      "epoch: 9300, acc: 0.906, loss: 0.356\n",
      "epoch: 9300, acc: 0.656, loss: 0.707\n",
      "epoch: 9300, acc: 0.906, loss: 0.342\n",
      "epoch: 9300, acc: 0.844, loss: 0.490\n",
      "epoch: 9300, acc: 0.812, loss: 0.353\n",
      "epoch: 9300, acc: 0.812, loss: 0.437\n",
      "epoch: 9300, acc: 0.844, loss: 0.390\n",
      "epoch: 9300, acc: 0.750, loss: 0.460\n",
      "epoch: 9300, acc: 0.906, loss: 0.274\n",
      "epoch: 9300, acc: 0.812, loss: 0.417\n",
      "epoch: 9300, acc: 0.812, loss: 0.397\n",
      "epoch: 9300, acc: 0.844, loss: 0.377\n",
      "epoch: 9300, acc: 0.844, loss: 0.334\n",
      "epoch: 9300, acc: 0.875, loss: 0.287\n",
      "epoch: 9300, acc: 0.812, loss: 0.450\n",
      "epoch: 9300, acc: 0.750, loss: 0.458\n",
      "epoch: 9300, acc: 0.688, loss: 0.612\n",
      "epoch: 9300, acc: 0.938, loss: 0.233\n",
      "epoch: 9300, acc: 0.812, loss: 0.396\n",
      "epoch: 9300, acc: 0.844, loss: 0.458\n",
      "epoch: 9300, acc: 0.781, loss: 0.399\n",
      "epoch: 9300, acc: 0.688, loss: 0.556\n",
      "epoch: 9300, acc: 0.812, loss: 0.403\n",
      "epoch: 9300, acc: 0.781, loss: 0.533\n",
      "epoch: 9300, acc: 0.812, loss: 0.518\n",
      "epoch: 9300, acc: 0.781, loss: 0.401\n",
      "epoch: 9300, acc: 0.810, loss: 0.342\n",
      "epoch: 9400, acc: 0.844, loss: 0.445\n",
      "epoch: 9400, acc: 0.875, loss: 0.356\n",
      "epoch: 9400, acc: 0.812, loss: 0.481\n",
      "epoch: 9400, acc: 0.812, loss: 0.522\n",
      "epoch: 9400, acc: 0.844, loss: 0.491\n",
      "epoch: 9400, acc: 0.750, loss: 0.439\n",
      "epoch: 9400, acc: 0.781, loss: 0.423\n",
      "epoch: 9400, acc: 0.875, loss: 0.403\n",
      "epoch: 9400, acc: 0.812, loss: 0.467\n",
      "epoch: 9400, acc: 0.750, loss: 0.544\n",
      "epoch: 9400, acc: 0.875, loss: 0.347\n",
      "epoch: 9400, acc: 0.812, loss: 0.476\n",
      "epoch: 9400, acc: 0.875, loss: 0.333\n",
      "epoch: 9400, acc: 0.781, loss: 0.491\n",
      "epoch: 9400, acc: 0.875, loss: 0.425\n",
      "epoch: 9400, acc: 0.719, loss: 0.525\n",
      "epoch: 9400, acc: 0.875, loss: 0.409\n",
      "epoch: 9400, acc: 0.781, loss: 0.456\n",
      "epoch: 9400, acc: 0.906, loss: 0.390\n",
      "epoch: 9400, acc: 0.688, loss: 0.519\n",
      "epoch: 9400, acc: 0.875, loss: 0.306\n",
      "epoch: 9400, acc: 0.719, loss: 0.518\n",
      "epoch: 9400, acc: 0.781, loss: 0.530\n",
      "epoch: 9400, acc: 0.812, loss: 0.369\n",
      "epoch: 9400, acc: 0.750, loss: 0.504\n",
      "epoch: 9400, acc: 0.938, loss: 0.232\n",
      "epoch: 9400, acc: 0.875, loss: 0.273\n",
      "epoch: 9400, acc: 0.750, loss: 0.493\n",
      "epoch: 9400, acc: 0.844, loss: 0.369\n",
      "epoch: 9400, acc: 0.844, loss: 0.366\n",
      "epoch: 9400, acc: 0.875, loss: 0.323\n",
      "epoch: 9400, acc: 0.844, loss: 0.286\n",
      "epoch: 9400, acc: 0.857, loss: 0.312\n",
      "epoch: 9500, acc: 0.812, loss: 0.585\n",
      "epoch: 9500, acc: 0.938, loss: 0.315\n",
      "epoch: 9500, acc: 0.844, loss: 0.371\n",
      "epoch: 9500, acc: 0.781, loss: 0.535\n",
      "epoch: 9500, acc: 0.906, loss: 0.241\n",
      "epoch: 9500, acc: 0.781, loss: 0.525\n",
      "epoch: 9500, acc: 0.688, loss: 0.507\n",
      "epoch: 9500, acc: 0.906, loss: 0.310\n",
      "epoch: 9500, acc: 0.781, loss: 0.471\n",
      "epoch: 9500, acc: 0.719, loss: 0.486\n",
      "epoch: 9500, acc: 0.781, loss: 0.398\n",
      "epoch: 9500, acc: 0.875, loss: 0.341\n",
      "epoch: 9500, acc: 0.875, loss: 0.388\n",
      "epoch: 9500, acc: 0.812, loss: 0.431\n",
      "epoch: 9500, acc: 0.875, loss: 0.376\n",
      "epoch: 9500, acc: 0.750, loss: 0.530\n",
      "epoch: 9500, acc: 0.906, loss: 0.287\n",
      "epoch: 9500, acc: 0.688, loss: 0.499\n",
      "epoch: 9500, acc: 0.844, loss: 0.267\n",
      "epoch: 9500, acc: 0.719, loss: 0.600\n",
      "epoch: 9500, acc: 0.688, loss: 0.536\n",
      "epoch: 9500, acc: 0.750, loss: 0.468\n",
      "epoch: 9500, acc: 0.844, loss: 0.450\n",
      "epoch: 9500, acc: 0.875, loss: 0.299\n",
      "epoch: 9500, acc: 0.781, loss: 0.391\n",
      "epoch: 9500, acc: 0.844, loss: 0.402\n",
      "epoch: 9500, acc: 0.875, loss: 0.393\n",
      "epoch: 9500, acc: 0.844, loss: 0.529\n",
      "epoch: 9500, acc: 0.906, loss: 0.353\n",
      "epoch: 9500, acc: 0.844, loss: 0.352\n",
      "epoch: 9500, acc: 0.812, loss: 0.351\n",
      "epoch: 9500, acc: 0.875, loss: 0.405\n",
      "epoch: 9500, acc: 0.762, loss: 0.474\n",
      "epoch: 9600, acc: 0.938, loss: 0.207\n",
      "epoch: 9600, acc: 0.938, loss: 0.264\n",
      "epoch: 9600, acc: 0.750, loss: 0.586\n",
      "epoch: 9600, acc: 0.812, loss: 0.343\n",
      "epoch: 9600, acc: 0.812, loss: 0.339\n",
      "epoch: 9600, acc: 0.906, loss: 0.380\n",
      "epoch: 9600, acc: 0.656, loss: 0.632\n",
      "epoch: 9600, acc: 0.875, loss: 0.408\n",
      "epoch: 9600, acc: 0.844, loss: 0.326\n",
      "epoch: 9600, acc: 0.812, loss: 0.560\n",
      "epoch: 9600, acc: 0.781, loss: 0.500\n",
      "epoch: 9600, acc: 0.875, loss: 0.480\n",
      "epoch: 9600, acc: 0.844, loss: 0.429\n",
      "epoch: 9600, acc: 0.844, loss: 0.324\n",
      "epoch: 9600, acc: 0.812, loss: 0.561\n",
      "epoch: 9600, acc: 0.812, loss: 0.388\n",
      "epoch: 9600, acc: 0.719, loss: 0.499\n",
      "epoch: 9600, acc: 0.812, loss: 0.488\n",
      "epoch: 9600, acc: 0.781, loss: 0.432\n",
      "epoch: 9600, acc: 0.969, loss: 0.182\n",
      "epoch: 9600, acc: 0.812, loss: 0.402\n",
      "epoch: 9600, acc: 0.844, loss: 0.342\n",
      "epoch: 9600, acc: 0.812, loss: 0.436\n",
      "epoch: 9600, acc: 0.812, loss: 0.439\n",
      "epoch: 9600, acc: 0.750, loss: 0.541\n",
      "epoch: 9600, acc: 0.875, loss: 0.253\n",
      "epoch: 9600, acc: 0.719, loss: 0.482\n",
      "epoch: 9600, acc: 0.781, loss: 0.551\n",
      "epoch: 9600, acc: 0.812, loss: 0.376\n",
      "epoch: 9600, acc: 0.812, loss: 0.374\n",
      "epoch: 9600, acc: 0.844, loss: 0.423\n",
      "epoch: 9600, acc: 0.781, loss: 0.548\n",
      "epoch: 9600, acc: 0.857, loss: 0.305\n",
      "epoch: 9700, acc: 0.906, loss: 0.301\n",
      "epoch: 9700, acc: 0.812, loss: 0.480\n",
      "epoch: 9700, acc: 0.844, loss: 0.419\n",
      "epoch: 9700, acc: 0.812, loss: 0.439\n",
      "epoch: 9700, acc: 0.750, loss: 0.511\n",
      "epoch: 9700, acc: 0.750, loss: 0.485\n",
      "epoch: 9700, acc: 0.781, loss: 0.429\n",
      "epoch: 9700, acc: 0.750, loss: 0.514\n",
      "epoch: 9700, acc: 0.750, loss: 0.577\n",
      "epoch: 9700, acc: 0.750, loss: 0.467\n",
      "epoch: 9700, acc: 0.938, loss: 0.271\n",
      "epoch: 9700, acc: 0.844, loss: 0.334\n",
      "epoch: 9700, acc: 0.844, loss: 0.293\n",
      "epoch: 9700, acc: 0.781, loss: 0.487\n",
      "epoch: 9700, acc: 0.844, loss: 0.397\n",
      "epoch: 9700, acc: 0.875, loss: 0.385\n",
      "epoch: 9700, acc: 0.844, loss: 0.344\n",
      "epoch: 9700, acc: 0.781, loss: 0.472\n",
      "epoch: 9700, acc: 0.781, loss: 0.460\n",
      "epoch: 9700, acc: 0.812, loss: 0.446\n",
      "epoch: 9700, acc: 0.938, loss: 0.226\n",
      "epoch: 9700, acc: 0.906, loss: 0.331\n",
      "epoch: 9700, acc: 0.906, loss: 0.301\n",
      "epoch: 9700, acc: 0.875, loss: 0.326\n",
      "epoch: 9700, acc: 0.812, loss: 0.425\n",
      "epoch: 9700, acc: 0.719, loss: 0.431\n",
      "epoch: 9700, acc: 0.844, loss: 0.396\n",
      "epoch: 9700, acc: 0.719, loss: 0.684\n",
      "epoch: 9700, acc: 0.844, loss: 0.419\n",
      "epoch: 9700, acc: 0.781, loss: 0.438\n",
      "epoch: 9700, acc: 0.812, loss: 0.506\n",
      "epoch: 9700, acc: 0.812, loss: 0.379\n",
      "epoch: 9700, acc: 0.810, loss: 0.445\n",
      "epoch: 9800, acc: 0.781, loss: 0.443\n",
      "epoch: 9800, acc: 0.719, loss: 0.517\n",
      "epoch: 9800, acc: 0.844, loss: 0.407\n",
      "epoch: 9800, acc: 0.906, loss: 0.265\n",
      "epoch: 9800, acc: 0.969, loss: 0.237\n",
      "epoch: 9800, acc: 0.812, loss: 0.504\n",
      "epoch: 9800, acc: 0.781, loss: 0.487\n",
      "epoch: 9800, acc: 0.844, loss: 0.466\n",
      "epoch: 9800, acc: 0.875, loss: 0.374\n",
      "epoch: 9800, acc: 0.844, loss: 0.404\n",
      "epoch: 9800, acc: 0.906, loss: 0.280\n",
      "epoch: 9800, acc: 0.719, loss: 0.509\n",
      "epoch: 9800, acc: 0.750, loss: 0.484\n",
      "epoch: 9800, acc: 0.781, loss: 0.478\n",
      "epoch: 9800, acc: 0.906, loss: 0.351\n",
      "epoch: 9800, acc: 0.781, loss: 0.619\n",
      "epoch: 9800, acc: 0.875, loss: 0.326\n",
      "epoch: 9800, acc: 0.781, loss: 0.417\n",
      "epoch: 9800, acc: 0.906, loss: 0.278\n",
      "epoch: 9800, acc: 0.844, loss: 0.288\n",
      "epoch: 9800, acc: 0.781, loss: 0.481\n",
      "epoch: 9800, acc: 0.781, loss: 0.384\n",
      "epoch: 9800, acc: 0.906, loss: 0.323\n",
      "epoch: 9800, acc: 0.812, loss: 0.296\n",
      "epoch: 9800, acc: 0.688, loss: 0.572\n",
      "epoch: 9800, acc: 0.781, loss: 0.515\n",
      "epoch: 9800, acc: 0.906, loss: 0.289\n",
      "epoch: 9800, acc: 0.688, loss: 0.461\n",
      "epoch: 9800, acc: 0.875, loss: 0.447\n",
      "epoch: 9800, acc: 0.750, loss: 0.667\n",
      "epoch: 9800, acc: 0.938, loss: 0.257\n",
      "epoch: 9800, acc: 0.781, loss: 0.399\n",
      "epoch: 9800, acc: 0.667, loss: 0.698\n",
      "epoch: 9900, acc: 0.781, loss: 0.483\n",
      "epoch: 9900, acc: 0.875, loss: 0.419\n",
      "epoch: 9900, acc: 0.844, loss: 0.398\n",
      "epoch: 9900, acc: 0.875, loss: 0.490\n",
      "epoch: 9900, acc: 0.875, loss: 0.316\n",
      "epoch: 9900, acc: 0.875, loss: 0.279\n",
      "epoch: 9900, acc: 0.906, loss: 0.269\n",
      "epoch: 9900, acc: 0.844, loss: 0.328\n",
      "epoch: 9900, acc: 0.812, loss: 0.348\n",
      "epoch: 9900, acc: 0.812, loss: 0.453\n",
      "epoch: 9900, acc: 0.812, loss: 0.337\n",
      "epoch: 9900, acc: 0.656, loss: 0.645\n",
      "epoch: 9900, acc: 0.781, loss: 0.415\n",
      "epoch: 9900, acc: 0.906, loss: 0.363\n",
      "epoch: 9900, acc: 0.906, loss: 0.254\n",
      "epoch: 9900, acc: 0.812, loss: 0.368\n",
      "epoch: 9900, acc: 0.719, loss: 0.486\n",
      "epoch: 9900, acc: 0.906, loss: 0.311\n",
      "epoch: 9900, acc: 0.844, loss: 0.362\n",
      "epoch: 9900, acc: 0.844, loss: 0.372\n",
      "epoch: 9900, acc: 0.875, loss: 0.349\n",
      "epoch: 9900, acc: 0.750, loss: 0.577\n",
      "epoch: 9900, acc: 0.844, loss: 0.372\n",
      "epoch: 9900, acc: 0.656, loss: 0.566\n",
      "epoch: 9900, acc: 0.781, loss: 0.409\n",
      "epoch: 9900, acc: 0.750, loss: 0.581\n",
      "epoch: 9900, acc: 0.688, loss: 0.547\n",
      "epoch: 9900, acc: 0.844, loss: 0.405\n",
      "epoch: 9900, acc: 0.875, loss: 0.311\n",
      "epoch: 9900, acc: 0.844, loss: 0.580\n",
      "epoch: 9900, acc: 0.844, loss: 0.397\n",
      "epoch: 9900, acc: 0.844, loss: 0.422\n",
      "epoch: 9900, acc: 0.667, loss: 0.734\n",
      "epoch: 10000, acc: 0.875, loss: 0.323\n",
      "epoch: 10000, acc: 0.812, loss: 0.419\n",
      "epoch: 10000, acc: 0.875, loss: 0.318\n",
      "epoch: 10000, acc: 0.844, loss: 0.335\n",
      "epoch: 10000, acc: 0.812, loss: 0.519\n",
      "epoch: 10000, acc: 0.750, loss: 0.461\n",
      "epoch: 10000, acc: 0.750, loss: 0.459\n",
      "epoch: 10000, acc: 0.719, loss: 0.602\n",
      "epoch: 10000, acc: 0.812, loss: 0.471\n",
      "epoch: 10000, acc: 0.781, loss: 0.376\n",
      "epoch: 10000, acc: 0.875, loss: 0.391\n",
      "epoch: 10000, acc: 0.781, loss: 0.438\n",
      "epoch: 10000, acc: 0.906, loss: 0.343\n",
      "epoch: 10000, acc: 0.750, loss: 0.465\n",
      "epoch: 10000, acc: 0.906, loss: 0.417\n",
      "epoch: 10000, acc: 0.875, loss: 0.396\n",
      "epoch: 10000, acc: 0.844, loss: 0.322\n",
      "epoch: 10000, acc: 0.719, loss: 0.661\n",
      "epoch: 10000, acc: 0.781, loss: 0.438\n",
      "epoch: 10000, acc: 0.781, loss: 0.510\n",
      "epoch: 10000, acc: 0.812, loss: 0.370\n",
      "epoch: 10000, acc: 0.875, loss: 0.297\n",
      "epoch: 10000, acc: 0.875, loss: 0.387\n",
      "epoch: 10000, acc: 0.812, loss: 0.433\n",
      "epoch: 10000, acc: 0.844, loss: 0.445\n",
      "epoch: 10000, acc: 0.906, loss: 0.367\n",
      "epoch: 10000, acc: 0.812, loss: 0.400\n",
      "epoch: 10000, acc: 0.688, loss: 0.549\n",
      "epoch: 10000, acc: 0.750, loss: 0.461\n",
      "epoch: 10000, acc: 0.844, loss: 0.430\n",
      "epoch: 10000, acc: 0.875, loss: 0.307\n",
      "epoch: 10000, acc: 0.875, loss: 0.304\n",
      "epoch: 10000, acc: 0.810, loss: 0.464\n"
     ]
    }
   ],
   "source": [
    "def mini_batch(X, y, batch_size=32):\n",
    "    indices = np.random.permutation(len(X))  # we shuffle data before batching\n",
    "    X_shuffled = X[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    for start_idx in range(0, len(X), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(X))\n",
    "        yield X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]\n",
    "\n",
    "\n",
    "np.random.seed(42)  # for reproducibility\n",
    "dense1 = Layer_Dense(5, 5)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(5, 2)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_GD()\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "for epoch in range(10001):\n",
    "    for batch_X, batch_y in mini_batch(X, training_results_array, batch_size):\n",
    "        dense1.forward(batch_X)\n",
    "        activation1.forward(dense1.output)\n",
    "        dense2.forward(activation1.output)\n",
    "        \n",
    "        loss = loss_activation.forward(dense2.output, batch_y)\n",
    "        predictions = np.argmax(loss_activation.output, axis=1)\n",
    "        y_train = np.argmax(batch_y, axis=1)\n",
    "        \n",
    "        accuracy = np.mean(predictions == y_train)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "        \n",
    "        loss_activation.backward(loss_activation.output, y_train)\n",
    "        dense2.backward(loss_activation.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "        \n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9530f4",
   "metadata": {},
   "source": [
    "as we can see, we got one result per batch. Thus, 32 batches times 10000 epochs, we get 320000 results.\n",
    "Thats noisy and hard to interpret, so let's just print out the averages of each epoch, for both accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3950f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Average Accuracy: 0.8171, Average Loss: 0.4195\n",
      "Epoch 100 - Average Accuracy: 0.8204, Average Loss: 0.4223\n",
      "Epoch 200 - Average Accuracy: 0.8191, Average Loss: 0.4184\n",
      "Epoch 300 - Average Accuracy: 0.8200, Average Loss: 0.4178\n",
      "Epoch 400 - Average Accuracy: 0.8162, Average Loss: 0.4191\n",
      "Epoch 500 - Average Accuracy: 0.8200, Average Loss: 0.4188\n",
      "Epoch 600 - Average Accuracy: 0.8238, Average Loss: 0.4182\n",
      "Epoch 700 - Average Accuracy: 0.8185, Average Loss: 0.4206\n",
      "Epoch 800 - Average Accuracy: 0.8219, Average Loss: 0.4182\n",
      "Epoch 900 - Average Accuracy: 0.8195, Average Loss: 0.4203\n",
      "Epoch 1000 - Average Accuracy: 0.8229, Average Loss: 0.4168\n",
      "Epoch 1100 - Average Accuracy: 0.8224, Average Loss: 0.4191\n",
      "Epoch 1200 - Average Accuracy: 0.8176, Average Loss: 0.4194\n",
      "Epoch 1300 - Average Accuracy: 0.8215, Average Loss: 0.4176\n",
      "Epoch 1400 - Average Accuracy: 0.8229, Average Loss: 0.4191\n",
      "Epoch 1500 - Average Accuracy: 0.8153, Average Loss: 0.4198\n",
      "Epoch 1600 - Average Accuracy: 0.8171, Average Loss: 0.4201\n",
      "Epoch 1700 - Average Accuracy: 0.8166, Average Loss: 0.4217\n",
      "Epoch 1800 - Average Accuracy: 0.8200, Average Loss: 0.4205\n",
      "Epoch 1900 - Average Accuracy: 0.8152, Average Loss: 0.4202\n",
      "Epoch 2000 - Average Accuracy: 0.8181, Average Loss: 0.4202\n",
      "Epoch 2100 - Average Accuracy: 0.8176, Average Loss: 0.4177\n",
      "Epoch 2200 - Average Accuracy: 0.8223, Average Loss: 0.4210\n",
      "Epoch 2300 - Average Accuracy: 0.8167, Average Loss: 0.4198\n",
      "Epoch 2400 - Average Accuracy: 0.8171, Average Loss: 0.4203\n",
      "Epoch 2500 - Average Accuracy: 0.8191, Average Loss: 0.4172\n",
      "Epoch 2600 - Average Accuracy: 0.8181, Average Loss: 0.4192\n",
      "Epoch 2700 - Average Accuracy: 0.8224, Average Loss: 0.4181\n",
      "Epoch 2800 - Average Accuracy: 0.8220, Average Loss: 0.4169\n",
      "Epoch 2900 - Average Accuracy: 0.8205, Average Loss: 0.4189\n",
      "Epoch 3000 - Average Accuracy: 0.8185, Average Loss: 0.4216\n",
      "Epoch 3100 - Average Accuracy: 0.8219, Average Loss: 0.4204\n",
      "Epoch 3200 - Average Accuracy: 0.8234, Average Loss: 0.4173\n",
      "Epoch 3300 - Average Accuracy: 0.8157, Average Loss: 0.4198\n",
      "Epoch 3400 - Average Accuracy: 0.8176, Average Loss: 0.4219\n",
      "Epoch 3500 - Average Accuracy: 0.8186, Average Loss: 0.4176\n",
      "Epoch 3600 - Average Accuracy: 0.8205, Average Loss: 0.4186\n",
      "Epoch 3700 - Average Accuracy: 0.8209, Average Loss: 0.4234\n",
      "Epoch 3800 - Average Accuracy: 0.8166, Average Loss: 0.4206\n",
      "Epoch 3900 - Average Accuracy: 0.8176, Average Loss: 0.4198\n",
      "Epoch 4000 - Average Accuracy: 0.8204, Average Loss: 0.4198\n",
      "Epoch 4100 - Average Accuracy: 0.8218, Average Loss: 0.4215\n",
      "Epoch 4200 - Average Accuracy: 0.8224, Average Loss: 0.4184\n",
      "Epoch 4300 - Average Accuracy: 0.8200, Average Loss: 0.4198\n",
      "Epoch 4400 - Average Accuracy: 0.8152, Average Loss: 0.4213\n",
      "Epoch 4500 - Average Accuracy: 0.8210, Average Loss: 0.4175\n",
      "Epoch 4600 - Average Accuracy: 0.8143, Average Loss: 0.4179\n",
      "Epoch 4700 - Average Accuracy: 0.8224, Average Loss: 0.4179\n",
      "Epoch 4800 - Average Accuracy: 0.8143, Average Loss: 0.4207\n",
      "Epoch 4900 - Average Accuracy: 0.8196, Average Loss: 0.4181\n",
      "Epoch 5000 - Average Accuracy: 0.8209, Average Loss: 0.4209\n",
      "Epoch 5100 - Average Accuracy: 0.8180, Average Loss: 0.4222\n",
      "Epoch 5200 - Average Accuracy: 0.8171, Average Loss: 0.4192\n",
      "Epoch 5300 - Average Accuracy: 0.8185, Average Loss: 0.4206\n",
      "Epoch 5400 - Average Accuracy: 0.8185, Average Loss: 0.4220\n",
      "Epoch 5500 - Average Accuracy: 0.8204, Average Loss: 0.4232\n",
      "Epoch 5600 - Average Accuracy: 0.8210, Average Loss: 0.4178\n",
      "Epoch 5700 - Average Accuracy: 0.8153, Average Loss: 0.4189\n",
      "Epoch 5800 - Average Accuracy: 0.8148, Average Loss: 0.4174\n",
      "Epoch 5900 - Average Accuracy: 0.8243, Average Loss: 0.4169\n",
      "Epoch 6000 - Average Accuracy: 0.8195, Average Loss: 0.4181\n",
      "Epoch 6100 - Average Accuracy: 0.8181, Average Loss: 0.4183\n",
      "Epoch 6200 - Average Accuracy: 0.8200, Average Loss: 0.4187\n",
      "Epoch 6300 - Average Accuracy: 0.8190, Average Loss: 0.4202\n",
      "Epoch 6400 - Average Accuracy: 0.8191, Average Loss: 0.4179\n",
      "Epoch 6500 - Average Accuracy: 0.8161, Average Loss: 0.4234\n",
      "Epoch 6600 - Average Accuracy: 0.8176, Average Loss: 0.4226\n",
      "Epoch 6700 - Average Accuracy: 0.8143, Average Loss: 0.4190\n",
      "Epoch 6800 - Average Accuracy: 0.8195, Average Loss: 0.4205\n",
      "Epoch 6900 - Average Accuracy: 0.8191, Average Loss: 0.4193\n",
      "Epoch 7000 - Average Accuracy: 0.8167, Average Loss: 0.4189\n",
      "Epoch 7100 - Average Accuracy: 0.8153, Average Loss: 0.4228\n",
      "Epoch 7200 - Average Accuracy: 0.8190, Average Loss: 0.4192\n",
      "Epoch 7300 - Average Accuracy: 0.8176, Average Loss: 0.4206\n",
      "Epoch 7400 - Average Accuracy: 0.8233, Average Loss: 0.4209\n",
      "Epoch 7500 - Average Accuracy: 0.8167, Average Loss: 0.4195\n",
      "Epoch 7600 - Average Accuracy: 0.8242, Average Loss: 0.4185\n",
      "Epoch 7700 - Average Accuracy: 0.8157, Average Loss: 0.4210\n",
      "Epoch 7800 - Average Accuracy: 0.8162, Average Loss: 0.4191\n",
      "Epoch 7900 - Average Accuracy: 0.8200, Average Loss: 0.4179\n",
      "Epoch 8000 - Average Accuracy: 0.8239, Average Loss: 0.4173\n",
      "Epoch 8100 - Average Accuracy: 0.8218, Average Loss: 0.4219\n",
      "Epoch 8200 - Average Accuracy: 0.8176, Average Loss: 0.4186\n",
      "Epoch 8300 - Average Accuracy: 0.8233, Average Loss: 0.4192\n",
      "Epoch 8400 - Average Accuracy: 0.8209, Average Loss: 0.4193\n",
      "Epoch 8500 - Average Accuracy: 0.8205, Average Loss: 0.4183\n",
      "Epoch 8600 - Average Accuracy: 0.8171, Average Loss: 0.4200\n",
      "Epoch 8700 - Average Accuracy: 0.8214, Average Loss: 0.4173\n",
      "Epoch 8800 - Average Accuracy: 0.8152, Average Loss: 0.4203\n",
      "Epoch 8900 - Average Accuracy: 0.8224, Average Loss: 0.4175\n",
      "Epoch 9000 - Average Accuracy: 0.8224, Average Loss: 0.4197\n",
      "Epoch 9100 - Average Accuracy: 0.8209, Average Loss: 0.4196\n",
      "Epoch 9200 - Average Accuracy: 0.8190, Average Loss: 0.4197\n",
      "Epoch 9300 - Average Accuracy: 0.8162, Average Loss: 0.4193\n",
      "Epoch 9400 - Average Accuracy: 0.8205, Average Loss: 0.4189\n",
      "Epoch 9500 - Average Accuracy: 0.8176, Average Loss: 0.4202\n",
      "Epoch 9600 - Average Accuracy: 0.8214, Average Loss: 0.4182\n",
      "Epoch 9700 - Average Accuracy: 0.8190, Average Loss: 0.4188\n",
      "Epoch 9800 - Average Accuracy: 0.8185, Average Loss: 0.4219\n",
      "Epoch 9900 - Average Accuracy: 0.8166, Average Loss: 0.4226\n",
      "Epoch 10000 - Average Accuracy: 0.8190, Average Loss: 0.4205\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "epoch_accuracies = []  \n",
    "epoch_losses = []  \n",
    "\n",
    "for epoch in range(10001):\n",
    "    batch_accuracies = []  \n",
    "    batch_losses = []  \n",
    "    \n",
    "    for batch_X, batch_y in mini_batch(X, training_results_array, batch_size):\n",
    "        dense1.forward(batch_X)\n",
    "        activation1.forward(dense1.output)\n",
    "        dense2.forward(activation1.output)\n",
    "        \n",
    "        loss = loss_activation.forward(dense2.output, batch_y)\n",
    "        predictions = np.argmax(loss_activation.output, axis=1)\n",
    "        y_train = np.argmax(batch_y, axis=1)\n",
    "        \n",
    "        accuracy = np.mean(predictions == y_train)  \n",
    "        batch_accuracies.append(accuracy)  \n",
    "        \n",
    "        batch_losses.append(loss)  \n",
    "        \n",
    "        loss_activation.backward(loss_activation.output, y_train)\n",
    "        dense2.backward(loss_activation.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "        \n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "    \n",
    "    epoch_accuracy = np.mean(batch_accuracies)\n",
    "    epoch_loss = np.mean(batch_losses)\n",
    "    \n",
    "    epoch_accuracies.append(epoch_accuracy)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch} - Average Accuracy: {epoch_accuracy:.4f}, Average Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95253fbe",
   "metadata": {},
   "source": [
    "We get very similar accuracy and loss to previous models. However, within the closeness in results to the other models, this learning method gives one of the highest accuracies, better than both the [5,5,2] and [5,10,2] models, and even lower (slightly lower) loss than the previous minimum loss achieved by [5,8,4,2] model.\n",
    "\n",
    "Also, it is good to note that it learnt very quickly too, reaching top accuracy and low loss in the first epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf064e0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1599258",
   "metadata": {},
   "source": [
    "## Recognizing handwritten numbers \n",
    "\n",
    "Adapt the code in Backpropagation 1 to the task of recognizing the digits in MNIST\n",
    "dataset. In the data there are n= 60,000 examples. I suggest to create a network with\n",
    "[784, N, 10] layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b31dadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('mnist.pkl', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8daf3",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0323b0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(50000, 784), dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = train_set\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb78291",
   "metadata": {},
   "source": [
    "true outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5379d821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 8, 4, 8], shape=(50000,))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd1cc8",
   "metadata": {},
   "source": [
    "**a. Why are 784 layers appropriate for the input layer? (2p)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec5034",
   "metadata": {},
   "source": [
    "MNIST, which stands for Modified National Institute of Standards and Technology, is a database thant contains handwritten numbers from 0 to 9, that are frequently used to train image processing algorithms and machine learning models.\n",
    "\n",
    "\n",
    "Apparently, it contains 60,000 training images and 10,000 test images, all in grayscale and sized 28x28 pixels.\n",
    "\n",
    "Is this shape (28x28) that makes 784 layers appropriate for the input layer, as it is the number of pixels that would be entering the neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1a15b5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c682e84",
   "metadata": {},
   "source": [
    "\n",
    "**b. Why are 10 layers appropriate for the output layer? Are there other options? (3p)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e168c8e",
   "metadata": {},
   "source": [
    "There are 10 digit classes (0 to 9, as we said), so we need 10 output neurons, one for each digit.\n",
    "\n",
    "For the neural network, due to its multi-class classification nature, we will use the same approach as with the titanic dataset: Softmax activation + Categorical Cross-Entropy loss.\n",
    "\n",
    "Therefore, each output neuron will represent the probability or confidence score that the input is a specific digit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e1446",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6893b",
   "metadata": {},
   "source": [
    "\n",
    "**c. Run a backpropagation algorithm on a smaller subset of the 60,000 examples and try to find reasonable value on the learning rate as well as the size of the hidden layer. (3p)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f92c6",
   "metadata": {},
   "source": [
    "Let's take 5000 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "41e3b6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 784), (5000,))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subset = X[:5000]\n",
    "y_subset = y[:5000]\n",
    "\n",
    "X_subset.shape, y_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "94fc2166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorized_result(j):\n",
    "    e = np.zeros((10,))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "y_encoded_subset = np.array([vectorized_result(label) for label in y_subset])\n",
    "\n",
    "y_encoded_subset.shape\n",
    "y_encoded_subset[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d14540",
   "metadata": {},
   "source": [
    "Check for standarization, as it looked that data has been already standarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442182c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float32(0.0),\n",
       " np.float32(0.99609375),\n",
       " np.float32(0.1297137),\n",
       " np.float32(0.30633897))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subset.min(), X_subset.max(), X_subset.mean(), X_subset.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db718c1",
   "metadata": {},
   "source": [
    "yes, as we thought, pixels are already standarized to range between 0 and 1. However, originally, pixels could go from 0 to 255 in the grayscale, so we would have to divide X by 255, to standarize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79a928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.104, loss: 2.303\n",
      "epoch: 100, acc: 0.752, loss: 1.044\n",
      "epoch: 200, acc: 0.877, loss: 0.496\n",
      "epoch: 300, acc: 0.902, loss: 0.372\n",
      "epoch: 400, acc: 0.913, loss: 0.317\n",
      "epoch: 500, acc: 0.922, loss: 0.284\n",
      "epoch: 600, acc: 0.929, loss: 0.260\n",
      "epoch: 700, acc: 0.935, loss: 0.241\n",
      "epoch: 800, acc: 0.939, loss: 0.225\n",
      "epoch: 900, acc: 0.944, loss: 0.211\n",
      "epoch: 1000, acc: 0.946, loss: 0.198\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "dense1 = Layer_Dense(784, 64)              \n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 10)               \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_GD(learning_rate=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(1001):\n",
    "    dense1.forward(X_subset)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, y_encoded_subset)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    y_labels = np.argmax(y_encoded_subset, axis=1)\n",
    "    accuracy = np.mean(predictions == y_labels)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y_labels)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f4807",
   "metadata": {},
   "source": [
    "Wow, we get impressive results in just 1000 epochs. The accuracy of almost 95% is very high, and the loss was dramatically reduced from 2.3 to less than 0.2 (ten times lower than at the start).\n",
    "\n",
    "The small learning rate allowed for precise parameter updates, and it didn't take much time to be such a big dataset with 784 inputs every time.\n",
    "\n",
    "We think it is a good enough configuration for the network: lr=0.1 and 64 neurons for the hidden layer.\n",
    "\n",
    "Why 64 neurons?:\n",
    "- To be honest, in neural networks we usually try for 64, 128, 256, etc, neurons for the hidden layer, and compare.\n",
    "    - We tried 64 neurons and it performed so well, that it is very likely the best configuration as it balances for both model complexity and computational efficiency. It isn't too small (which could lead to underfitting) and not too large (which could lead to overfitting and longer training times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9abd2c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18820d58",
   "metadata": {},
   "source": [
    "**d. Run the backpropagation algorithm on the entire data-set, how well is it performing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba50ca",
   "metadata": {},
   "source": [
    "Without creating a subset of rows: use X and y, created at the beginning of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c0697e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float32(0.0),\n",
       " np.float32(0.99609375),\n",
       " np.float32(0.13044983),\n",
       " np.float32(0.3072898))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded = np.array([vectorized_result(label) for label in y])\n",
    "\n",
    "X.min(), X.max(), X.mean(), X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2074343",
   "metadata": {},
   "source": [
    "Keeping same configuration of lr=0.1 and 64 neurons in the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8ddd772c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.108, loss: 2.303\n",
      "epoch: 100, acc: 0.749, loss: 1.100\n",
      "epoch: 200, acc: 0.859, loss: 0.542\n",
      "epoch: 300, acc: 0.884, loss: 0.427\n",
      "epoch: 400, acc: 0.894, loss: 0.378\n",
      "epoch: 500, acc: 0.901, loss: 0.350\n",
      "epoch: 600, acc: 0.906, loss: 0.330\n",
      "epoch: 700, acc: 0.911, loss: 0.315\n",
      "epoch: 800, acc: 0.914, loss: 0.302\n",
      "epoch: 900, acc: 0.917, loss: 0.291\n",
      "epoch: 1000, acc: 0.920, loss: 0.281\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "dense1 = Layer_Dense(784, 64)              \n",
    "activation1 = Activation_ReLU()            \n",
    "dense2 = Layer_Dense(64, 10)              \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    "optimizer = Optimizer_GD(learning_rate=0.1)  \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, y_encoded)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    y_labels = np.argmax(y_encoded, axis=1)\n",
    "    accuracy = np.mean(predictions == y_labels)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y_labels)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81413c",
   "metadata": {},
   "source": [
    "We get very similar results. It underperforms the previous model with a sample of 5000 observations, and it took a much longer time to run, but it was expected as the whole datset is 10 times larger.\n",
    "\n",
    "In conclusion, when trained on a smaller subset of 5000 samples as opposed to the entire MNIST dataset, the model learns more quickly and reaches higher accuracy. Training on the complete dataset improves the model's generalization and makes it more robust to the complexity and diversity of the entire dataset, even though it takes longer and yields a slightly lower accuracy (92%). \n",
    "\n",
    "However, the accuracy difference is just showing that the full dataset improves generalization while the subset allows for faster convergence. We think, both models exhibit impressive performance by the end of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5f2f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
